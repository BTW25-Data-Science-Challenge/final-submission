{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# BTW 2025 Data Science Challenge"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Introduction "
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Forecasting market dynamics, including supply, demand, and pricing, is critical for entities operating within a market environment. Buyers aim to identify favorable moments for purchasing goods at lower prices, while sellers seek to optimize production volumes based on demand and profitability considerations. This is especially relevant in the context of the electricity market, where the stability of the grid depends on maintaining a delicate balance between supply and demand. Disruptions to this equilibrium can significantly affect grid reliability and may lead to power outages, as noted by Horáček (2010).\n",
    "Consequently, the field of electricity price forecasting stands as a subject of considerable relevance for both market participants and academic researchers.\n",
    "Historically, these predictions have relied on statistical methods such as ARIMA, SARIMA, and GARCH models with moving averages to predict next day's prices, as seen in Contreras et al. (2003) or Nunes et al. (2008).\n",
    "In recent years, deep learning has become a prominent feature of these attempts, with research surpassing the accuracy of those statistic approaches using deep learning, for example Lago et al. (2018).\n",
    "Explainable AI (xAI) approaches have also enabled a greater understanding of the electricity market, with research such as Tschora et al. (2022) being able to identify the importance and weight of individual influences on the prediction of electricity prices.\n",
    "\n",
    "This report outlines the methodology employed by the Dresden Database Research Group to forecast day-ahead electricity prices as part of the BTW 2025 Data Science Challenge, with a focus on comparing and enhancing several established time series forecasting methods. Initially, general domain knowledge related to energy markets and pricing was researched and presented (<ins>&#8594; Gathering Domain Knowledge</ins>).\n",
    "Following this, multiple data sources influencing day-ahead prices were identified, including finance, meteorology, and societal data. Various preprocessing techniques are also discussed. The data underwent thorough analysis using methods such as explainable machine learning models, including Temporal Fusion Transformers. Based on this analysis, data preparation was carried out iteratively. Additionally, the aspect of feature importance was compared across models (<ins>&#8594; Data Sources</ins>).\n",
    "In the subsequent section, the models utilized are described in detail, and intermediate results are presented (<ins>&#8594; Visualization & Story Telling</ins>). \n",
    "The models are then compared in a final benchmark to identify the most optimal model from the available methods (<ins>&#8594; Predictive Modelling</ins>).\n",
    "Finally, the report summarizes the results, reflects on the experience gained, and outlines potential features for future work (<ins>&#8594; Summary & Future Work</ins>)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utils\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Installs for real-time execution\n",
    "!pip install git+https://github.com/amazon-science/chronos-forecasting.git\n",
    "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install autogluon\n",
    "!pip install statsmodels\n",
    "!pip install python-dotenv\n",
    "!pip install entsoe-py\n",
    "!pip install newsapi-python\n",
    "!pip install sentence-transformers\n",
    "!pip install tensorflow\n",
    "!pip install seaborn\n",
    "!pip install pytorch_forecasting"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import final\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Autogluon imports\n",
    "import sys\n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "from autogluon.timeseries import TimeSeriesPredictor  \n",
    "from autogluon.common import space\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from matplotlib import pyplot as plt  \n",
    "import seaborn as sns  \n",
    "import time\n",
    "import tensorflow as tf  \n",
    "import csv\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "Base model structure.\n",
    "Every Forecasting model should inherit from this.\n",
    "\n",
    "Note: override all abstract methods and keep the final methods unchanged\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import final\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "class BaseModel(ABC):\n",
    "\n",
    "    def __init__(self, model_name: str, model_type: str):\n",
    "        \"\"\"Init and create model.\n",
    "\n",
    "        :param model_name: Name of your model\n",
    "        :param model_type: Type of your model e.g. LSTM\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model_type = model_type\n",
    "        self.model = None  # this is a placeholder for your model\n",
    "        self.create_model()\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_model(self):\n",
    "        \"\"\"Define your own model under self.model.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, X_train: pd.DataFrame, y_train: pd.DataFrame,\n",
    "              X_val: pd.DataFrame = None, y_val: pd.DataFrame = None,\n",
    "              X_test: pd.DataFrame = None, y_test: pd.DataFrame = None) -> pd.DataFrame | None:\n",
    "        \"\"\"train the model on the training data.\n",
    "        test and validation data can be used only for evaluation (if available).\n",
    "\n",
    "        :param X_train: training features dataset\n",
    "        :param y_train: training target values\n",
    "        :param X_val: validation features' dataset\n",
    "        :param y_val: validation target values\n",
    "        :param X_test: testing features' dataset\n",
    "        :param y_test: testing target values\n",
    "        :return: training history (losses while training, if available else None) [epoch | train_loss | test_loss]\n",
    "        \"\"\"\n",
    "        # call the training loop/function of your model\n",
    "        # and return a history (if available, otherwise None)\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def run_prediction(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"run prediction on your defined model\n",
    "\n",
    "        :param X: features dataset\n",
    "        :return: prediction output, [timestamp | value]\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @final\n",
    "    def predict(self, X: pd.DataFrame, exp_dir: str = None) -> pd.DataFrame:\n",
    "        \"\"\"call this to run prediction\n",
    "\n",
    "        :param X: features dataset\n",
    "        :param exp_dir: dir to store prediction result\n",
    "        :return: prediction output, [timestamp | value]\n",
    "        \"\"\"\n",
    "        # run your custom prediction\n",
    "        prediction_results = self.run_prediction(X)\n",
    "\n",
    "        # store if dir is provided\n",
    "        if exp_dir is not None:\n",
    "            prediction_results.to_csv(f'{exp_dir}\\\\{self.model_type}_{self.model_name}_prediction.csv')\n",
    "        return prediction_results\n",
    "\n",
    "    @abstractmethod\n",
    "    def custom_save(self, model: object, filename: str):\n",
    "        \"\"\"Use your own dataformat to save your model here\n",
    "\n",
    "        :param filename: filename or path\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def custom_load(self, filename: str) -> object:\n",
    "        \"\"\"Use your own dataformat to load your model here\n",
    "\n",
    "        :param filename: filename or path\n",
    "        :return: your loaded model\n",
    "        \"\"\"\n",
    "        # return model\n",
    "        ...\n",
    "\n",
    "    @final\n",
    "    def save(self, exp_dir: str):\n",
    "        \"\"\"call this to save self.model.\n",
    "\n",
    "        :param exp_dir: dir name or path to dir\n",
    "        \"\"\"\n",
    "        self.__custom_save(model=self.model, filename=f'{exp_dir}\\\\{self.model_type}_{self.model_name}')\n",
    "\n",
    "    @final\n",
    "    def load(self, exp_dir: str):\n",
    "        \"\"\"call this to load a retrained model\n",
    "\n",
    "        :param exp_dir: dir name or path to dir\n",
    "        \"\"\"\n",
    "        self.model = self.__custom_load(filename=f'{exp_dir}\\\\{self.model_type}_{self.model_name}')\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from chronos import ChronosPipeline\n",
    "\n",
    "class ChronosModel(BaseModel):\n",
    "    def __init__(self, model_name: str, model_type: str):\n",
    "        \"\"\"Call the BaseModel constructor with the required arguments.\"\"\"\n",
    "        \n",
    "        # Define the missing method to prevent AttributeError\n",
    "        self._BaseModel__create_model = self.create_model  # Redirect it to the correct method\n",
    "        \n",
    "        super().__init__(model_name, model_type)\n",
    "\n",
    "    def create_model(self):\n",
    "        self.model = None\n",
    "\n",
    "\n",
    "    \n",
    "    def run_prediction(self, X):\n",
    "\n",
    "        #define which column to be forecasted and forecast legth\n",
    "        target_column = \"day_ahead_prices_EURO\"\n",
    "        prediction_length = 24\n",
    "\n",
    "        \n",
    "        context = torch.tensor(X[target_column].values)[-512:]  # Limit context to last 512 samples\n",
    "        forecast = self.model.predict(context, prediction_length)\n",
    "        low, median, high = np.quantile(forecast[0].numpy(), [0.1, 0.5, 0.9], axis=0)\n",
    "\n",
    "        context_dates = X.index[-512:]\n",
    "        last_date = context_dates[-1]\n",
    "        forecast_index = pd.date_range(last_date + pd.Timedelta(hours=1), periods=prediction_length, freq=\"h\")\n",
    "\n",
    "        prediction_results = pd.DataFrame({\n",
    "            \"timestamp\": forecast_index,\n",
    "            \"forecasted_values\": median\n",
    "        })\n",
    "        \n",
    "        return prediction_results\n",
    "\n",
    "    \n",
    "    def custom_load(self, model_dir):\n",
    "        \n",
    "        # Load the model pipeline\n",
    "        pipeline = ChronosPipeline.from_pretrained(\n",
    "            model_dir,\n",
    "            device_map=(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "        self.model = pipeline  # Set the model to the loaded pipeline\n",
    "\n",
    "        return pipeline\n",
    "    \n",
    "    \n",
    "                \n",
    "    def custom_save(self, model = None, filename = None):\n",
    "        return\n",
    "\n",
    "    def train(self, X_train = None, y_train = None, X_val = None, y_val = None, X_test = None, y_test = None):\n",
    "        return None"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "class ETSModel(BaseModel):\n",
    "    def create_model(self):\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, X_train, y_train, X_val = None, y_val = None, X_test = None, y_test = None):\n",
    "        #time_series = X_train\n",
    "        if y_train.isnull().any():\n",
    "            print(\"Warning: Missing values in y_train are being automatically filled by the ETSModel.\")\n",
    "            y_train = y_train.ffill()\n",
    "        model = ExponentialSmoothing(y_train, seasonal_periods=24, trend=\"add\", seasonal=\"add\")\n",
    "        self.model = model.fit()\n",
    "        return None\n",
    "    \n",
    "    def run_prediction(self, X):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"The model is not trained. Call `train` before prediction.\")\n",
    "        forecast = self.model.forecast(len(X))\n",
    "        #print(forecast)\n",
    "        #print(X)\n",
    "        prediction_results = pd.DataFrame({\n",
    "            'timestamp': X['Date'],  # Use the 'Date' column in X for timestamps\n",
    "            'value': forecast.values\n",
    "        })\n",
    "        return prediction_results\n",
    "\n",
    "    def custom_save(self, model, filename):\n",
    "         with open(f\"{filename}.pkl\", \"wb\") as file:\n",
    "            pickle.dump(model, file)\n",
    "    \n",
    "    def custom_load(self, filename):\n",
    "        with open(f\"{filename}.pkl\", \"rb\") as file:\n",
    "            loaded_model = pickle.load(file)\n",
    "        return loaded_model\n",
    "    \n",
    "\n",
    "class MultivarLSTM(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers  # number of recurrent layers in the lstm\n",
    "        self.input_size = input_size  # number of input features\n",
    "        self.output_size = output_size  # output sequence length\n",
    "        self.hidden_size = hidden_size  # neurons in each lstm layer\n",
    "        # LSTM model\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=0.2,\n",
    "                            bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_size * 24 * 2, output_size)\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        # hidden state init\n",
    "        h_0 = Variable(torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size, device=device))\n",
    "        # cell state init\n",
    "        c_0 = Variable(torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size, device=device))\n",
    "        # propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0))  # (input, hidden, and internal state)\n",
    "        # reduce dimension to the required output sequence length\n",
    "        predictions = self.linear(output.reshape(output.size(0), output.size(1) * output.size(2)))\n",
    "        pred_out = self.activation(predictions)\n",
    "\n",
    "        return pred_out\n",
    "\n",
    "\n",
    "class MultivariateBiLSTM(BaseModel):\n",
    "    def __init__(self, features, target):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.input_length = 24\n",
    "        self.hidden_layer_size = 256\n",
    "        self.num_layers = 12\n",
    "        self.output_length = 24\n",
    "        # Check For GPU -> If available send model and data to it\n",
    "        self.device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        super(MultivariateBiLSTM, self).__init__(model_name='MultivarLSTM',\n",
    "                                                 model_type='MultivariateBidirectionalLSTM')\n",
    "\n",
    "    def create_model(self):\n",
    "        self.model = MultivarLSTM(num_layers=self.num_layers, hidden_size=self.hidden_layer_size,\n",
    "                                  input_size=len(self.features), output_size=self.output_length)\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def train(self, X_train: pd.DataFrame, y_train: pd.DataFrame,\n",
    "              X_val: pd.DataFrame = None, y_val: pd.DataFrame = None,\n",
    "              X_test: pd.DataFrame = None, y_test: pd.DataFrame = None,\n",
    "              n_epochs=500, batch_size=1024, learning_rate=0.001) -> pd.DataFrame | None:\n",
    "        self.target_scaler = MinMaxScaler()\n",
    "        self.feature_scaler = StandardScaler()\n",
    "\n",
    "        # select features and target columns\n",
    "        if len(self.features) > 0:\n",
    "            # use only selected features (of self.features not defined: use all columns as features)\n",
    "            X_train = X_train[self.features]\n",
    "            X_test = X_test[self.features]\n",
    "            X_val = X_val[self.features]\n",
    "        y_train = y_train[self.target].values\n",
    "        y_test = y_test[self.target].values\n",
    "        y_val = y_val[self.target].values\n",
    "\n",
    "        # fit scalar on training data\n",
    "        self.feature_scaler.fit(X_train)\n",
    "        self.target_scaler.fit(y_train.reshape(-1, 1))\n",
    "\n",
    "        X_train = self.feature_scaler.transform(X_train)\n",
    "        y_train = self.target_scaler.transform(y_train.reshape(-1, 1))\n",
    "        X_test = self.feature_scaler.transform(X_test)\n",
    "        y_test = self.target_scaler.transform(y_test.reshape(-1, 1))\n",
    "        X_val = self.feature_scaler.transform(X_val)\n",
    "        y_val = self.target_scaler.transform(y_val.reshape(-1, 1))\n",
    "\n",
    "        # convert dataset to tensors suitable for training the model\n",
    "        X_train_tensors = self.__prepare_feature_dataset(X_train)\n",
    "        y_train_tensors = self.__prepare_target_dataset(y_train)\n",
    "        X_test_tensors = self.__prepare_feature_dataset(X_test)\n",
    "        y_test_tensors = self.__prepare_target_dataset(y_test)\n",
    "        X_val_tensors = self.__prepare_feature_dataset(X_val)\n",
    "        y_val_tensors = self.__prepare_target_dataset(y_val)\n",
    "\n",
    "        history = self.__training_loop(n_epochs=n_epochs,\n",
    "                                       X_train=X_train_tensors,\n",
    "                                       y_train=y_train_tensors,\n",
    "                                       X_test=X_test_tensors,\n",
    "                                       y_test=y_test_tensors,\n",
    "                                       batch_size=batch_size,\n",
    "                                       learning_rate=learning_rate)\n",
    "\n",
    "    def __training_loop(self, n_epochs, X_train, y_train, X_test, y_test, batch_size, learning_rate):\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        loss_fn = torch.nn.MSELoss()  # mean-squared error for regression\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        train_history = {'epoch': [], 'train loss': [], 'test loss': []}\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            train_losses = []\n",
    "            self.model.train()\n",
    "            for seq, labels in train_loader:\n",
    "                outputs = self.model(seq)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_losses.append(loss.to('cpu').item())\n",
    "            # test loss\n",
    "            test_losses = []\n",
    "            self.model.eval()\n",
    "            for seq, labels in test_loader:\n",
    "                test_preds = self.model(seq)\n",
    "                test_loss = loss_fn(test_preds, labels)\n",
    "                test_losses.append(test_loss.to('cpu').item())\n",
    "            if epoch % 1 == 0:\n",
    "                print(\n",
    "                    f'Epoch {epoch + 1}:\\t|\\tTrain_Loss: {round(sum(train_losses) / len(train_losses), 5)}\\t|\\t'\n",
    "                    f'Test_Loss: {round(sum(test_losses) / len(test_losses), 5)}')\n",
    "                train_history['epoch'].append(epoch + 1)\n",
    "                train_history['train loss'].append((sum(train_losses) / len(train_losses)))\n",
    "                train_history['test loss'].append((sum(test_losses) / len(test_losses)))\n",
    "        train_history = pd.DataFrame(train_history).set_index('epoch')\n",
    "\n",
    "        return train_history\n",
    "\n",
    "    def run_prediction(self, X: pd.DataFrame, batch_size=1024) -> pd.DataFrame:\n",
    "        \"\"\"run prediction on your defined model.\n",
    "\n",
    "        :param X: features dataset\n",
    "        :return: prediction output, [timestamp | value]\n",
    "        \"\"\"\n",
    "        X = X.reset_index(names='timestamp')\n",
    "        timestamps = X['timestamp']\n",
    "        X = X.drop(['timestamp'], axis=1)\n",
    "\n",
    "        # scale features using the training scaler\n",
    "        X = self.feature_scaler.transform(X[self.features])\n",
    "        # prepare dataset\n",
    "        X_pred_tensors = self.__prepare_feature_dataset(X)\n",
    "        dataset = TensorDataset(X_pred_tensors)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        pred_list = []\n",
    "        # predict each batch\n",
    "        for seq in data_loader:\n",
    "            X_batch = seq[0]\n",
    "            predictions = self.model.forward(X_batch)\n",
    "            pred_list.append(predictions.to('cpu').detach().numpy())\n",
    "\n",
    "        sequences = np.concatenate(pred_list)\n",
    "\n",
    "        total_length = len(sequences) + self.target_length - 1  # total number of positions covered\n",
    "        sum_values = np.zeros(total_length)\n",
    "        count_values = np.zeros(total_length)\n",
    "        min_values = np.full(total_length, np.inf)  # init with infinity\n",
    "        max_values = np.full(total_length, -np.inf)  # init with negative infinity\n",
    "\n",
    "        for i, seq in enumerate(sequences):\n",
    "            for j in range(24):\n",
    "                index = i + j  # global index in the expanded array\n",
    "                value = seq[j]\n",
    "\n",
    "                sum_values[index] += value\n",
    "                count_values[index] += 1\n",
    "                min_values[index] = min(min_values[index], value)\n",
    "                max_values[index] = max(max_values[index], value)\n",
    "\n",
    "        mean_values = sum_values / count_values\n",
    "\n",
    "        pred_sequence = self.target_scaler.inverse_transform(mean_values.reshape(-1, 1)).reshape(-1)\n",
    "        mins = self.target_scaler.inverse_transform(min_values.reshape(-1, 1)).reshape(-1)\n",
    "        maxs = self.target_scaler.inverse_transform(max_values.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "        df_result = pd.DataFrame({'timestamp': timestamps[:pred_sequence.shape[0]],\n",
    "                                  'day_ahead_price_predicted': pred_sequence,\n",
    "                                  'pred_min': mins,\n",
    "                                  'pred_max': maxs})\n",
    "        return df_result\n",
    "\n",
    "    def __prepare_feature_dataset(self, X):\n",
    "        X_seq = self.__split_feature_sequences(features_seq=X)\n",
    "\n",
    "        X_tensor = Variable(torch.Tensor(X_seq))\n",
    "        X_tensor_format = torch.reshape(X_tensor, (X_tensor.shape[0], self.input_length, X_tensor.shape[2]))\n",
    "        X_tensor_format = X_tensor_format.to(self.device)\n",
    "\n",
    "        return X_tensor_format\n",
    "\n",
    "    def __prepare_target_dataset(self, y):\n",
    "        y_seq = self.__split_target_sequences(y)\n",
    "\n",
    "        y_tensor = Variable(torch.Tensor(y_seq))\n",
    "        y_tensor = y_tensor.to(self.device)\n",
    "\n",
    "        return y_tensor\n",
    "\n",
    "    def __split_feature_sequences(self, features_seq):\n",
    "        X = []  # instantiate X and y\n",
    "        for i in range(len(features_seq)):\n",
    "            # find the end of the sequence\n",
    "            end_ix = i + self.input_length\n",
    "            # check if we are beyond the dataset\n",
    "            if end_ix > len(features_seq):\n",
    "                break\n",
    "            # gather input and output of the pattern\n",
    "            seq_x = features_seq[i:end_ix]\n",
    "            X.append(seq_x)\n",
    "        return np.array(X)\n",
    "\n",
    "    def __split_target_sequences(self, target_seq):\n",
    "        y = []  # instantiate y\n",
    "        for i in range(len(target_seq)):\n",
    "            # find the end of the sequence\n",
    "            end_ix = i + self.input_length\n",
    "            # check if we are beyond the dataset\n",
    "            if end_ix > len(target_seq):\n",
    "                break\n",
    "            # gather input and output of the pattern\n",
    "            seq_y = target_seq[i:end_ix, -1]\n",
    "            y.append(seq_y)\n",
    "        return np.array(y)\n",
    "\n",
    "    def create_scalers(self, X_train, y_train):\n",
    "        self.target_scaler = MinMaxScaler()\n",
    "        self.feature_scaler = StandardScaler()\n",
    "\n",
    "        # select features and target columns\n",
    "        if len(self.features) > 0:\n",
    "            # use only selected features (of self.features not defined: use all columns as features)\n",
    "            X_train = X_train[self.features]\n",
    "        y_train = y_train[self.target].values\n",
    "\n",
    "        # fit scalar on training data\n",
    "        self.feature_scaler.fit(X_train)\n",
    "        self.target_scaler.fit(y_train.reshape(-1, 1))\n",
    "\n",
    "    def custom_load(self, filename: str) -> object:\n",
    "        \"\"\"Use your own dataformat to load your model here\n",
    "\n",
    "        :param filename: filename or path\n",
    "        :return: your loaded model\n",
    "        \"\"\"\n",
    "        self.model.load_state_dict(torch.load(filename))\n",
    "        self.model.eval()\n",
    "        return self.model\n",
    "\n",
    "    def custom_save(self, model: object, filename: str):\n",
    "        \"\"\"Use your own dataformat to save your model here\n",
    "\n",
    "        :param filename: filename or path\n",
    "        \"\"\"\n",
    "        torch.save(self.model.state_dict(), filename)\n",
    "        \n",
    "        \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 3, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"forward calculating the attention context and weights\n",
    "        :param hidden: (batch_size, 1, hidden_dim)\n",
    "        :param encoder_outputs: (batch_size, seq_len, hidden_dim)\n",
    "        :return: attention context and weights\n",
    "        \"\"\"\n",
    "        # Repeat decoder hidden state across sequence length\n",
    "        hidden = hidden.repeat(1, encoder_outputs.size(1), 1)  # (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # concatenate hidden and encoder outputs\n",
    "        combined = torch.cat((hidden, encoder_outputs), dim=2)  # (batch_size, seq_len, hidden_dim * 2)\n",
    "\n",
    "        # calculate attention scores\n",
    "        energy = torch.tanh(self.attn(combined))  # (batch_size, seq_len, hidden_dim)\n",
    "        scores = self.v(energy).squeeze(2)  # (batch_size, seq_len)\n",
    "\n",
    "        # apply softmax to get attention weights\n",
    "        attn_weights = torch.softmax(scores, dim=1)  # (batch_size, seq_len)\n",
    "\n",
    "        # Compute context vector as weighted sum of encoder outputs\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)  # (batch_size, 1, hidden_dim)\n",
    "\n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1, bidir=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=bidir)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(x.device)    # if bidir\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # output: (batch_size, seq_len, num_layer*hidden_dim)\n",
    "        # hidden: (num_layers*2, batch_size, hidden_dim)        -> *2 for num_layers if bidirectional=True\n",
    "        # cell: (num_layers*2, batch_size, hidden_dim)\n",
    "        output, (hidden, cell) = self.lstm(x, (h0, c0))\n",
    "        return output, hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim=16, num_layers=1, num_heads=4, bidir=False, use_attention=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.use_attention = use_attention\n",
    "        self.lstm = nn.LSTM(10 + (2 * hidden_dim), hidden_dim, self.num_layers, batch_first=True, bidirectional=bidir)\n",
    "        self.fc = nn.Linear(hidden_dim*2, 1)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "\n",
    "    def forward(self, x, hidden, cell, encoder_out):\n",
    "        \"\"\"Calculate decoder output using attention.\n",
    "\n",
    "        :param x: decoder input, shape: (batch_size, 1, encoder_out_dim)\n",
    "        :param hidden: encoder hidden state, shape: (num_layers*num_directions, batch_size, hidden_dim)\n",
    "        :param cell: encoder cell state, shape: (num_layers*num_directions, batch_size, hidden_dim)\n",
    "        :param encoder_out: encoder output, shape: (batch_size, seq_len, hidden_dim*num_directions)\n",
    "        :return: decoder output, shape: (batch_size, 1, 1)\n",
    "        \"\"\"\n",
    "        if self.use_attention:\n",
    "            # Compute attention\n",
    "            # context: (batch_size, 1, hidden_dim*num_directions)\n",
    "            # att_weights: (batch_size, seq_len)\n",
    "            context, att_weights = self.attention(hidden[-1].unsqueeze(1), encoder_out)\n",
    "        else:\n",
    "            # use equal attention weights\n",
    "            seq_len = encoder_out.shape[1]\n",
    "            batch_size = x.shape[0]\n",
    "            value = 1 / seq_len\n",
    "            att_weights = torch.full((batch_size, seq_len), value, device=x.device)\n",
    "            context = torch.bmm(att_weights.unsqueeze(1), encoder_out)\n",
    "\n",
    "        # contatenate context with output from last timestamp\n",
    "        x = x.repeat(1, 1, 10)\n",
    "        # x: (batch_size, 1, hidden_dim*num_directions + enc_output_dim*10)\n",
    "        x = torch.cat((x, context), dim=2)\n",
    "            \n",
    "        # output: (batch_size, seq_len, num_layer*hidden_dim)\n",
    "        # hidden: (num_layers*2, batch_size, hidden_dim)        -> *2 for num_layers if bidirectional=True\n",
    "        # cell: (num_layers*2, batch_size, hidden_dim)\n",
    "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden, cell, att_weights\n",
    "\n",
    "\n",
    "class EncDecLSTM(nn.Module):\n",
    "    def __init__(self, encoder, decoder, target_length):\n",
    "        super(EncDecLSTM, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.target_length = target_length\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, dec_init=None, teacher_forcing_ratio=0.5, target_values=None):\n",
    "        \"\"\"Forward pass the input sequences, containing feature space through the\n",
    "        Encoder-Attention-Decoder model and output the predicted sequence\n",
    "\n",
    "        :param x: input sequences tensor\n",
    "        :param dec_init: target value for previous timestamp\n",
    "        :param teacher_forcing_ratio: probibility if teacher forcing is used in each batch\n",
    "        :param target_values: actual target values used for teacher forcing while training (when available)\n",
    "        :return: predicted target sequence\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length, input_dim = x.size()\n",
    "        encoder_output, hidden, cell = self.encoder(x)\n",
    "        if dec_init is not None:\n",
    "            decoder_input = dec_init\n",
    "        else:\n",
    "            decoder_input = torch.zeros(batch_size, 1, 1).to(x.device)\n",
    "\n",
    "        # init empty target sequence\n",
    "        out = torch.zeros(batch_size, self.target_length, 1).to(x.device)\n",
    "\n",
    "        # step by step decoding\n",
    "        for t in range(self.target_length):\n",
    "            decoder_output, hidden, cell, attn_weights = self.decoder(decoder_input, hidden, cell, encoder_output)\n",
    "            out[:, t, :] = decoder_output.squeeze(1)\n",
    "\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            if teacher_force and target_values is not None:\n",
    "                decoder_input = target_values[:, t].unsqueeze(1).unsqueeze(1)\n",
    "            else:\n",
    "                decoder_input = decoder_output\n",
    "\n",
    "        out = torch.reshape(out, (out.size(0), out.size(1)))\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EncoderDecoderAttentionLSTM(BaseModel):\n",
    "    def __init__(self, target_length, features, target, hidden_size=64, num_layers=3, use_attention=True):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.target = target\n",
    "        self.features = features\n",
    "        self.target_length = target_length\n",
    "        self.use_attention = use_attention\n",
    "        # Check For GPU -> If available send model and data to it\n",
    "        self.device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        super(EncoderDecoderAttentionLSTM, self).__init__(model_name='EncDecAttLSTM',\n",
    "                                                          model_type='EncoderDecoderAttentionLSTM')\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"define model\n",
    "        \"\"\"\n",
    "        num_heads = 1\n",
    "        input_size = len(self.features)\n",
    "        self.input_length = 24\n",
    "        # case seq2seq decoder: use output_size = self.output_length\n",
    "        # case autoregressive decoder: use output_size = 1\n",
    "        output_size = self.target_length\n",
    "\n",
    "        Enc = Encoder(input_dim=input_size, hidden_dim=self.hidden_size, num_layers=self.num_layers, bidir=True)\n",
    "        Dec = Decoder(hidden_dim=self.hidden_size, num_layers=self.num_layers,\n",
    "                      num_heads=num_heads, bidir=True, use_attention=self.use_attention)\n",
    "        self.model = EncDecLSTM(encoder=Enc, decoder=Dec, target_length=self.target_length)\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def train(self, X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame = None,\n",
    "              y_val: pd.DataFrame = None, X_test: pd.DataFrame = None,\n",
    "              y_test: pd.DataFrame = None, n_epochs: int = 500, batch_size: int = 1024,\n",
    "              learning_rate: float = 0.001) -> pd.DataFrame | None:\n",
    "        \"\"\"train the model on the training data.\n",
    "        test and validation data can be used only for evaluation (if available).\n",
    "\n",
    "        :param X_train: training features dataset\n",
    "        :param y_train: training target values\n",
    "        :param X_val: validation features' dataset\n",
    "        :param y_val: validation target values\n",
    "        :param X_test: testing features' dataset\n",
    "        :param y_test: testing target values\n",
    "        :param n_epochs: number of training iterations\n",
    "        :param batch_size: size of each processed chunk of data in trainings loop\n",
    "        :param learning_rate: learning rate\n",
    "        :return: training history (losses while training, if available else None) [epoch | train_loss | test_loss]\n",
    "        \"\"\"\n",
    "\n",
    "        self.target_scaler = MinMaxScaler()\n",
    "        self.feature_scaler = StandardScaler()\n",
    "\n",
    "        # select features and target columns\n",
    "        if len(self.features) > 0:\n",
    "            # use only selected features (of self.features not defined: use all columns as features)\n",
    "            X_train = X_train[self.features]\n",
    "            X_test = X_test[self.features]\n",
    "            X_val = X_val[self.features]\n",
    "        y_train = y_train[self.target].values\n",
    "        y_test = y_test[self.target].values\n",
    "        y_val = y_val[self.target].values\n",
    "\n",
    "        # fit scalar on training data\n",
    "        self.feature_scaler.fit(X_train)\n",
    "        self.target_scaler.fit(y_train.reshape(-1, 1))\n",
    "\n",
    "        X_train = self.feature_scaler.transform(X_train)\n",
    "        y_train = self.target_scaler.transform(y_train.reshape(-1, 1))\n",
    "        X_test = self.feature_scaler.transform(X_test)\n",
    "        y_test = self.target_scaler.transform(y_test.reshape(-1, 1))\n",
    "        X_val = self.feature_scaler.transform(X_val)\n",
    "        y_val = self.target_scaler.transform(y_val.reshape(-1, 1))\n",
    "\n",
    "        # convert dataset to tensors suitable for training the model\n",
    "        X_train_tensors = self.__prepare_feature_dataset(X_train)\n",
    "        y_train_tensors = self.__prepare_target_dataset(y_train)\n",
    "        X_test_tensors = self.__prepare_feature_dataset(X_test)\n",
    "        y_test_tensors = self.__prepare_target_dataset(y_test)\n",
    "        X_val_tensors = self.__prepare_feature_dataset(X_val)\n",
    "        y_val_tensors = self.__prepare_target_dataset(y_val)\n",
    "\n",
    "        loss_fn = torch.nn.L1Loss()  # mean absolute error loss for regression\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        history = self.__training_loop(n_epochs=n_epochs,\n",
    "                                       optimiser=optimizer,\n",
    "                                       loss_fn=loss_fn,\n",
    "                                       X_train=X_train_tensors,\n",
    "                                       y_train=y_train_tensors,\n",
    "                                       X_test=X_test_tensors,\n",
    "                                       y_test=y_test_tensors,\n",
    "                                       batch_size=batch_size)\n",
    "        return history\n",
    "\n",
    "    def __training_loop(self, n_epochs, optimiser, loss_fn, X_train, y_train,\n",
    "                        X_test, y_test, batch_size):\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        train_history = {'epoch': [], 'train loss': [], 'test loss': []}\n",
    "\n",
    "        # set teacher forcing probability to 0.5\n",
    "        tr = 0.5\n",
    "        \n",
    "        min_loss = 1000\n",
    "        model_state = self.model.state_dict()\n",
    "        for epoch in range(n_epochs):\n",
    "            train_losses = []\n",
    "            self.model.train()\n",
    "            timestep = 0\n",
    "            for seq, labels in train_loader:\n",
    "                decoder_input = seq[:, -1:, -1:]\n",
    "                outputs = self.model.foreward(seq, decoder_input, \n",
    "                                              teacher_forcing_ratio=tr, target_values=labels)\n",
    "                optimiser.zero_grad()\n",
    "\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "                train_losses.append(loss.to('cpu').item())\n",
    "                timestep += 1\n",
    "            # test loss\n",
    "            test_losses = []\n",
    "            self.model.eval()\n",
    "            # for seq, labels in tqdm(test_loader):\n",
    "            for seq, labels in test_loader:\n",
    "                decoder_input = seq[:, -1:, -1:]\n",
    "                test_preds = self.model(seq, decoder_input)\n",
    "\n",
    "                test_loss = loss_fn(test_preds, labels)\n",
    "                test_losses.append(test_loss.to('cpu').item())\n",
    "            if epoch % 1 == 0:\n",
    "                print(\n",
    "                    f'Epoch {epoch + 1}:\\t|\\tTrain_Loss: {round(sum(train_losses) / len(train_losses), 5)}\\t|\\t'\n",
    "                    f'Test_Loss: {round(sum(test_losses) / len(test_losses), 5)}')\n",
    "                train_history['epoch'].append(epoch + 1)\n",
    "                train_history['train loss'].append((sum(train_losses) / len(train_losses)))\n",
    "                train_history['test loss'].append((sum(test_losses) / len(test_losses)))\n",
    "            if min_loss > (sum(test_losses) / len(test_losses)):\n",
    "                model_state = self.model.state_dict()\n",
    "                min_loss = (sum(test_losses) / len(test_losses))\n",
    "\n",
    "        train_history = pd.DataFrame(train_history).set_index('epoch')\n",
    "\n",
    "        return train_history\n",
    "\n",
    "    def create_scalers(self, X_train, y_train):\n",
    "        self.target_scaler = MinMaxScaler()\n",
    "        self.feature_scaler = StandardScaler()\n",
    "\n",
    "        # select features and target columns\n",
    "        if len(self.features) > 0:\n",
    "            # use only selected features (of self.features not defined: use all columns as features)\n",
    "            X_train = X_train[self.features]\n",
    "        y_train = y_train[self.target].values\n",
    "\n",
    "        # fit scalar on training data\n",
    "        self.feature_scaler.fit(X_train)\n",
    "        self.target_scaler.fit(y_train.reshape(-1, 1))\n",
    "\n",
    "    def run_prediction(self, X: pd.DataFrame, batch_size=1024) -> pd.DataFrame:\n",
    "        \"\"\"run prediction on your defined model.\n",
    "\n",
    "        :param X: features dataset\n",
    "        :return: prediction output, [timestamp | value]\n",
    "        \"\"\"\n",
    "        pred_length = X.shape[0]\n",
    "        X = X.reset_index(names='timestamp')\n",
    "        timestamps = X['timestamp']\n",
    "        X = X.drop(['timestamp'], axis=1)\n",
    "        start_timestamp = timestamps[0]\n",
    "\n",
    "        # scale features using the training scaler\n",
    "        X = self.feature_scaler.transform(X[self.features])\n",
    "        X_pred_tensors = self.__prepare_feature_dataset(X)\n",
    "        dataset = TensorDataset(X_pred_tensors)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        pred_list = []\n",
    "        for seq in data_loader:\n",
    "            X_batch = seq[0]\n",
    "            decoder_input = X_batch[:, -1:, -1:]\n",
    "            predictions = self.model.forward(X_batch, decoder_input)\n",
    "            pred_list.append(predictions.to('cpu').detach().numpy())\n",
    "        sequences = np.concatenate(pred_list)\n",
    "\n",
    "        total_length = len(sequences) + self.target_length - 1  # total number of positions covered\n",
    "        sum_values = np.zeros(total_length)\n",
    "        count_values = np.zeros(total_length)\n",
    "        min_values = np.full(total_length, np.inf)  # init with infinity\n",
    "        max_values = np.full(total_length, -np.inf)  # init with negative infinity\n",
    "\n",
    "        for i, seq in enumerate(sequences):\n",
    "            for j in range(self.target_length):\n",
    "                index = i + j  # global index in the expanded array\n",
    "                value = seq[j]\n",
    "\n",
    "                sum_values[index] += value\n",
    "                count_values[index] += 1\n",
    "                min_values[index] = min(min_values[index], value)\n",
    "                max_values[index] = max(max_values[index], value)\n",
    "\n",
    "        mean_values = sum_values / count_values\n",
    "\n",
    "        end_timestamp = start_timestamp + pd.Timedelta(hours=total_length - 1)\n",
    "        timestamps = pd.date_range(start_timestamp, end_timestamp, freq='1H')\n",
    "\n",
    "        pred_sequence = self.target_scaler.inverse_transform(mean_values.reshape(-1, 1)).reshape(-1)\n",
    "        mins = self.target_scaler.inverse_transform(min_values.reshape(-1, 1)).reshape(-1)\n",
    "        maxs = self.target_scaler.inverse_transform(max_values.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "        df_result = pd.DataFrame({'timestamp': timestamps[:pred_sequence.shape[0]],\n",
    "                                  'day_ahead_price_predicted': pred_sequence,\n",
    "                                  'pred_min': mins,\n",
    "                                  'pred_max': maxs})\n",
    "        return df_result\n",
    "\n",
    "    def __prepare_feature_dataset(self, X):\n",
    "\n",
    "        X_seq = self.__split_feature_sequences(features_seq=X)\n",
    "\n",
    "        X_tensor = Variable(torch.Tensor(X_seq))\n",
    "        X_tensor_format = torch.reshape(X_tensor, (X_tensor.shape[0], self.input_length, X_tensor.shape[2]))\n",
    "        X_tensor_format = X_tensor_format.to(self.device)\n",
    "\n",
    "        return X_tensor_format\n",
    "\n",
    "    def __prepare_target_dataset(self, y):\n",
    "\n",
    "        y_seq = self.__split_target_sequences(y)\n",
    "\n",
    "        y_tensor = Variable(torch.Tensor(y_seq))\n",
    "        y_tensor = y_tensor.to(self.device)\n",
    "\n",
    "        return y_tensor\n",
    "\n",
    "    def __split_feature_sequences(self, features_seq):\n",
    "        X = []  # instantiate X\n",
    "        for i in range(len(features_seq)):\n",
    "            # find the end of the input, output sequence\n",
    "            end_ix = i + self.input_length\n",
    "            if end_ix > len(features_seq):\n",
    "                break\n",
    "            # gather input and output of the pattern\n",
    "            seq_x = features_seq[i:end_ix]\n",
    "            X.append(seq_x)\n",
    "        return np.asarray(X)\n",
    "\n",
    "    def __split_target_sequences(self, target_seq):\n",
    "        y = []  # instantiate y\n",
    "        for i in range(len(target_seq)):\n",
    "            # find the end of the input, output sequence\n",
    "            end_ix = i + self.input_length\n",
    "            if end_ix > len(target_seq):\n",
    "                break\n",
    "            # gather input and output of the pattern\n",
    "            seq_y = target_seq[i:end_ix, -1]\n",
    "            y.append(seq_y)\n",
    "        return np.asarray(y)\n",
    "\n",
    "    def custom_save(self, model: object, filename: str):\n",
    "        \"\"\"Use your own dataformat to save your model here\n",
    "\n",
    "        :param filename: filename or path\n",
    "        \"\"\"\n",
    "        torch.save(self.model.state_dict(), filename)\n",
    "        pass\n",
    "\n",
    "    def custom_load(self, filename: str) -> object:\n",
    "        \"\"\"Use your own dataformat to load your model here\n",
    "\n",
    "        :param filename: filename or path\n",
    "        :return: your loaded model\n",
    "        \"\"\"\n",
    "        self.model.load_state_dict(torch.load(filename))\n",
    "        self.model.eval()\n",
    "        return self.model\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#TFT implementation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet, NaNLabelEncoder\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def calc_metric(y_pred, y_actuals, metric, allow_zeros=True):\n",
    "    epsilon = 1e-10\n",
    "    if metric == 'mape':\n",
    "        if not allow_zeros:\n",
    "            idx_zeros = np.where((y_actuals <= -2) | (y_actuals >= 2))\n",
    "            y_actuals = y_actuals[idx_zeros]\n",
    "            y_pred = y_pred[idx_zeros]\n",
    "        res = np.mean(np.abs((y_pred - y_actuals) / (y_actuals + epsilon))) * 100\n",
    "    elif metric == 'mae':\n",
    "        res = np.mean(np.abs(y_pred - y_actuals))\n",
    "    elif metric == 'rmse':\n",
    "        res = np.sqrt(np.mean(np.power(y_pred - y_actuals, 2)))\n",
    "    elif metric == 'smape':\n",
    "        if not allow_zeros:\n",
    "            idx_zeros = np.where((y_actuals <= -2) | (y_actuals >= 2))\n",
    "            y_actuals = y_actuals[idx_zeros]\n",
    "            y_pred = y_pred[idx_zeros]\n",
    "        res = np.mean(np.abs(y_pred - y_actuals) / ((np.abs(y_pred) + np.abs(y_actuals)) / 2)) * 100\n",
    "    return res\n",
    "\n",
    "\n",
    "class TftForecaster(BaseModel):\n",
    "    def __init__(self, model_name: str, model_type: str):\n",
    "        self.model = None\n",
    "        self.data = None\n",
    "        self.data_index = None\n",
    "        self.data_predict = None\n",
    "        self.y_pred = None\n",
    "        self.y_actuals = None\n",
    "        self._BaseModel__create_model = self.create_model\n",
    "        super().__init__(model_name, model_type)\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        raw_prediction = self.model.predict(X, return_y=True)\n",
    "        self.y_pred = raw_prediction.output.cpu().numpy()\n",
    "        self.y_actuals = raw_prediction.y[0].cpu().numpy()\n",
    "\n",
    "    def get_current_data(self, data_path) -> pd.DataFrame:\n",
    "        data = pd.read_csv(data_path, index_col=0)\n",
    "        data.index = pd.to_datetime(data.index, utc=True)\n",
    "        data.index = data.index.tz_convert('Europe/Brussels')\n",
    "\n",
    "        data_index = pd.DataFrame(index=range(1, len(data) + 1), data=data.index)\n",
    "        data_index.rename(columns={0: 'timestamp'}, inplace=True)\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "        data['country'] = 'DE_LU'\n",
    "        data['time_idx'] = range(1, len(data) + 1)\n",
    "        self.data = data\n",
    "        self.data_index = data_index\n",
    "        return data\n",
    "\n",
    "    def get_data_predict(self, predict_date):\n",
    "        max_prediction_length = 24\n",
    "        max_encoder_length = 24 * 7\n",
    "        date = pd.Timestamp(predict_date, tz='Europe/Brussels')\n",
    "        last_encoder = date - pd.Timedelta(hours=1)\n",
    "        first_encoder = last_encoder - pd.Timedelta(hours=max_encoder_length)\n",
    "        last_decoder = last_encoder + pd.Timedelta(hours=max_prediction_length)\n",
    "\n",
    "        last_encoder_idx = self.data_index[self.data_index['timestamp'] == last_encoder].index[0]\n",
    "        first_encoder_idx = self.data_index[self.data_index['timestamp'] == first_encoder].index[0]\n",
    "        last_decoder_idx = self.data_index[self.data_index['timestamp'] == last_decoder].index[0]\n",
    "\n",
    "        encoder_data = self.data[lambda x: (x.time_idx > first_encoder_idx) & (x.time_idx <= last_encoder_idx)]\n",
    "        decoder_data = self.data[lambda x: (x.time_idx > last_encoder_idx) & (x.time_idx <= last_decoder_idx)]\n",
    "\n",
    "        self.data_predict = pd.concat([encoder_data, decoder_data], ignore_index=True)\n",
    "\n",
    "    def custom_load(self, filename: str):\n",
    "\n",
    "        self.model = TemporalFusionTransformer.load_from_checkpoint(filename)\n",
    "\n",
    "    def custom_save(self, model: object, filename: str):\n",
    "\n",
    "        torch.save(model.state_dict(), filename)\n",
    "\n",
    "    def train(self, X_train: pd.DataFrame, y_train: pd.DataFrame,\n",
    "              X_val: pd.DataFrame = None, y_val: pd.DataFrame = None,\n",
    "              X_test: pd.DataFrame = None, y_test: pd.DataFrame = None) -> pd.DataFrame | None:\n",
    "        pass\n",
    "\n",
    "    def run_prediction(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "    def create_model(self):\n",
    "        pass\n",
    "\n",
    "    def plot(self, date):\n",
    "        text_mape = f\"MAPE = {calc_metric(self.y_pred, self.y_actuals, metric='mape'):.2f} %\"\n",
    "        text_smape = f\"SMAPE = {calc_metric(self.y_pred, self.y_actuals, metric='smape'):.2f} %\"\n",
    "        text_mae = f\"MAE = {calc_metric(self.y_pred, self.y_actuals, metric='mae'):.2f} EUR/MWh\"\n",
    "        text_rmse = f\"RMSE = {calc_metric(self.y_pred, self.y_actuals, metric='rmse'):.2f} EUR/MWh\"\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(self.y_pred[0], label='Forecast')\n",
    "        plt.plot(self.y_actuals[0], label='Actuals')\n",
    "\n",
    "        plt.title(f'forecast for {date.date()}')\n",
    "        plt.xlabel('hour index')\n",
    "        plt.ylabel('electricity price [EUR/MWh]')\n",
    "        plt.xticks(range(0, 24))\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.legend()\n",
    "\n",
    "        plt.text(0.75, 0.15, text_mape, transform=plt.gca().transAxes,\n",
    "                 verticalalignment='bottom')\n",
    "        plt.text(0.75, 0.1, text_mae, transform=plt.gca().transAxes,\n",
    "                 verticalalignment='bottom')\n",
    "        plt.text(0.75, 0.05, text_rmse, transform=plt.gca().transAxes,\n",
    "                 verticalalignment='bottom')\n",
    "        plt.text(0.75, 0.0, text_smape, transform=plt.gca().transAxes,\n",
    "                 verticalalignment='bottom')\n",
    "\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "#TFT running script\n",
    "'''\n",
    "model_path_gpu = r\"./models/tft_forecast/trained_models/GPU_model_epoch=33-step=3672.ckpt\"\n",
    "model_path_cpu = r\"./models/tft_forecast/trained_models/CPU_model_epoch=14-step=6510.ckpt\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model_path = model_path_gpu\n",
    "else:\n",
    "    model_path = model_path_cpu\n",
    "\n",
    "\n",
    "date = pd.to_datetime('2025-02-04 00:00')\n",
    "model_name = 'Temporal Fusion Transformer',\n",
    "model_type = 'TemporalFusionTransformer'\n",
    "data_path = r\"./models/tft_forecast/dataset/multivar_dataset_040225.csv\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tft = TftForecaster(model_name=model_name, model_type=model_type)\n",
    "    tft.custom_load(filename = model_path)\n",
    "    tft.get_current_data(data_path = data_path)\n",
    "    tft.get_data_predict(predict_date=date)\n",
    "    tft.predict(X = tft.data_predict)\n",
    "    tft.plot(date=date)\n",
    "    print(f'Predicted day-ahead prices: {tft.y_pred}')\n",
    "'''"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BenchmarkMaker:\n",
    "    def __init__(self, export_dir):\n",
    "        self.export_dir = export_dir  # output for plots and benchmark .csv files\n",
    "\n",
    "        if not os.path.exists(self.export_dir):\n",
    "            # create the directory in case it does not already exist\n",
    "            os.makedirs(self.export_dir)\n",
    "\n",
    "        # load input data\n",
    "        self.data = None\n",
    "        self.model_names = []\n",
    "\n",
    "    def load_dataframes(self, predictions: dict, prices: pd.DataFrame):\n",
    "        \"\"\"loading datasets of predictions from different models\n",
    "\n",
    "        :param predictions: dict form: {model_name1: df1, model_name2: df2, ...}\n",
    "        :param prices: ground truth prices dataframe\n",
    "        \"\"\"\n",
    "        for k in predictions.keys():\n",
    "            predictions[k] = predictions[k].set_axis([str(k)], axis='columns')\n",
    "            self.model_names.append(str(k))\n",
    "        prices = prices.set_axis(['day_ahead_prices'], axis='columns')\n",
    "        self.align_dataframes(list(predictions.values()) + [prices])\n",
    "        self.data.index = pd.date_range(self.data.index[0], self.data.index[-1], freq='1H')\n",
    "\n",
    "    def align_dataframes(self, dataframes):\n",
    "        \"\"\"Align multiple DataFrames with timestamp indices by merging them on their index.\n",
    "        \"\"\"\n",
    "        result_df = dataframes[0]\n",
    "        for df in dataframes[1:]:\n",
    "            result_df = result_df.join(df, how='outer')\n",
    "        self.data = result_df\n",
    "\n",
    "    def calc_errors(self):\n",
    "        no_nan = self.data.copy(deep=True).dropna(how='any')\n",
    "        gt_values = no_nan['day_ahead_prices'].values\n",
    "        for pred_model in self.model_names:\n",
    "            pred_values = no_nan[pred_model].values\n",
    "            self.data[str(pred_model) + '_RMSE'] = self.calc_rmse(gt_values, pred_values)\n",
    "            self.data[str(pred_model) + '_MAE'] = self.calc_mae(gt_values, pred_values)\n",
    "            self.data[str(pred_model) + '_MAPE'] = self.calc_mape(gt_values, pred_values)\n",
    "            self.data[str(pred_model) + '_SE'] = self.calc_squared_error(self.data['day_ahead_prices'].values,\n",
    "                                                                         self.data[pred_model].values)\n",
    "            self.data[str(pred_model) + '_AE'] = self.calc_absolute_error(self.data['day_ahead_prices'].values,\n",
    "                                                                          self.data[pred_model].values)\n",
    "\n",
    "    def calc_squared_error(self, actual_values, predicted_values):\n",
    "        se = (actual_values - predicted_values)**2\n",
    "        return se\n",
    "\n",
    "    def calc_absolute_error(self, actual_values, predicted_values):\n",
    "        ae = abs(actual_values - predicted_values)\n",
    "        return ae\n",
    "\n",
    "    def calc_rmse(self, actual_values, predicted_values):\n",
    "        mse = mean_squared_error(actual_values, predicted_values)\n",
    "        rmse = np.sqrt(mse)\n",
    "        return rmse\n",
    "\n",
    "    def calc_mae(self, actual_values, predicted_values):\n",
    "        mae = mean_absolute_error(actual_values, predicted_values)\n",
    "        return mae\n",
    "\n",
    "    def calc_mape(self, actual_values: np.ndarray, predicted_values: np.ndarray) -> float:\n",
    "        \"\"\"calculate mean average percentage error.\n",
    "        close to 0: good\n",
    "        close to 1: bad\n",
    "\n",
    "        :param actual_values: correct underlying values\n",
    "        :param predicted_values: forecasted values\n",
    "        :return: mape\"\"\"\n",
    "        mape = mean_absolute_percentage_error(actual_values, predicted_values)\n",
    "        return mape\n",
    "\n",
    "    def plot_rmse_per_hour(self):\n",
    "        for model in self.model_names:\n",
    "            plt.plot(self.data.index.values, np.sqrt(self.data[model + '_SE'].values), label=model)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title('RMSE per Hour')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        if self.export_dir is not None:\n",
    "            plt.savefig(self.export_dir + '\\\\' + 'hourly_rmse.png')\n",
    "        plt.show(block=True)\n",
    "\n",
    "    def plot_mae_per_hour(self):\n",
    "        for model in self.model_names:\n",
    "            plt.plot(self.data.index.values, self.data[model + '_AE'].values, label=model)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title('Absolute Error per Hour')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('Absolute Error')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        if self.export_dir is not None:\n",
    "            plt.savefig(self.export_dir + '\\\\' + 'hourly_mae.png')\n",
    "        plt.show(block=True)\n",
    "\n",
    "    def plot_rmse_per_day(self):\n",
    "        copy_df = self.data.reset_index(names='timestamp')\n",
    "        for model in self.model_names:\n",
    "            days_df = copy_df.groupby(copy_df.timestamp.dt.date).mean().reset_index(names='date')\n",
    "            plt.plot(days_df['timestamp'], np.sqrt(days_df[model + '_SE']), label=model)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title('RMSE per Day')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        if self.export_dir is not None:\n",
    "            plt.savefig(self.export_dir + '\\\\' + 'daily_rmse.png')\n",
    "        plt.show(block=True)\n",
    "\n",
    "    def plot_mae_per_day(self):\n",
    "        copy_df = self.data.reset_index(names='timestamp')\n",
    "        for model in self.model_names:\n",
    "            days_df = copy_df.groupby(copy_df.timestamp.dt.date).mean().reset_index(names='date')\n",
    "            plt.plot(days_df['timestamp'], np.sqrt(days_df[model + '_AE']), label=model)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title('MAE per Day')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        if self.export_dir is not None:\n",
    "            plt.savefig(self.export_dir + '\\\\' + 'daily_mae.png')\n",
    "        plt.show(block=True)\n",
    "\n",
    "    def plot_compare_predictions_hourly(self, start_date=None, end_date=None):\n",
    "        plot_data = self.data[start_date:end_date].copy(deep=True)\n",
    "\n",
    "        gt_values = plot_data['day_ahead_prices'].values\n",
    "        timestamps = plot_data.index.values\n",
    "\n",
    "        for model in self.model_names:\n",
    "            pred_values = plot_data[model].values\n",
    "            plt.plot(timestamps, pred_values, label=model)\n",
    "\n",
    "        plt.plot(timestamps, gt_values, label='Actual Values')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title('Model Predictions per Hour')\n",
    "        plt.ylabel('Day Ahead Price in €')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        if self.export_dir is not None:\n",
    "            plt.savefig(self.export_dir + '\\\\' + 'hourly_compare_predictions.png')\n",
    "        plt.show(block=True)\n",
    "\n",
    "    def plot_compare_predictions_daily(self):\n",
    "        copy_df = self.data.reset_index(names='timestamp')\n",
    "        days_df = copy_df.groupby(copy_df.timestamp.dt.date).mean().reset_index(\n",
    "            names='date')\n",
    "        timestamps = days_df['timestamp'].values\n",
    "\n",
    "        gt_values = days_df['day_ahead_prices'].values\n",
    "        plt.plot(timestamps, gt_values, label='Actual Values')\n",
    "\n",
    "        for model in self.model_names:\n",
    "            pred_values = days_df[model].values\n",
    "            plt.plot(timestamps, pred_values, label=model)\n",
    "\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title('Model Predictions per Day')\n",
    "        plt.ylabel('Day Ahead Price in €')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        if self.export_dir is not None:\n",
    "            plt.savefig(self.export_dir + '\\\\' + 'daily_compare_predictions.png')\n",
    "        plt.show(block=True)\n",
    "\n",
    "    def plot_compare_mae(self):\n",
    "        no_nan = self.data.copy(deep=True).dropna(how='any')\n",
    "        gt_values = no_nan['day_ahead_prices'].values\n",
    "\n",
    "        mae_values = []\n",
    "        for model in self.model_names:\n",
    "            pred_values = no_nan[model].values\n",
    "            mae = self.calc_mae(gt_values, pred_values)\n",
    "            mae_values.append(mae)\n",
    "        x = np.arange(len(self.model_names))\n",
    "\n",
    "        ax = plt.subplot(111)\n",
    "        my_cmap = plt.get_cmap(\"jet\")\n",
    "        colors = my_cmap(np.linspace(0, 1, len(self.model_names)))\n",
    "        bar = ax.bar(x, mae_values, width=0.4, align='center', label='MAE', color=colors)\n",
    "\n",
    "        def digit_label(rects):\n",
    "            for rect in rects:\n",
    "                h = rect.get_height()\n",
    "                ax.text(rect.get_x() + rect.get_width() / 2., 1.05 * h, f'{int(h)}',\n",
    "                        ha='center', va='bottom', color=rect.get_facecolor())\n",
    "        digit_label(bar)\n",
    "        plt.title('MAE per Model')\n",
    "        ax.set_ylabel('MAE')\n",
    "        ax.set_xlabel('Model')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(self.model_names)\n",
    "        ax.set_ylim([0, max(mae_values) + 5])\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        if self.export_dir is not None:\n",
    "            plt.savefig(self.export_dir + '\\\\' + 'compare_mae.png')\n",
    "        plt.show(block=True)\n",
    "\n",
    "    def plot_compare_rmse(self):\n",
    "        no_nan = self.data.copy(deep=True).dropna(how='any')\n",
    "        gt_values = no_nan['day_ahead_prices'].values\n",
    "\n",
    "        rmse_values = []\n",
    "        for model in self.model_names:\n",
    "            pred_values = no_nan[model].values\n",
    "            rmse = self.calc_rmse(gt_values, pred_values)\n",
    "            rmse_values.append(rmse)\n",
    "        x = np.arange(len(self.model_names))\n",
    "\n",
    "        ax = plt.subplot(111)\n",
    "        my_cmap = plt.get_cmap(\"jet\")\n",
    "        colors = my_cmap(np.linspace(0, 1, len(self.model_names)))\n",
    "        bar = ax.bar(x, rmse_values, width=0.4, align='center', label='RMSE', color=colors)\n",
    "\n",
    "        def digit_label(rects):\n",
    "            for rect in rects:\n",
    "                h = rect.get_height()\n",
    "                ax.text(rect.get_x() + rect.get_width() / 2., 1.05 * h, f'{int(h)}',\n",
    "                        ha='center', va='bottom', color=rect.get_facecolor())\n",
    "        digit_label(bar)\n",
    "        plt.title('RMSE per Model')\n",
    "        ax.set_ylabel('RMSE')\n",
    "        ax.set_xlabel('Model')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(self.model_names)\n",
    "        ax.set_ylim([0, max(rmse_values) + 5])\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        if self.export_dir is not None:\n",
    "            plt.savefig(self.export_dir + '\\\\' + 'compare_rmse.png')\n",
    "        plt.show(block=True)\n",
    "\n",
    "    def plot_compare_mape(self):\n",
    "        no_nan = self.data.copy(deep=True).dropna(how='any')\n",
    "        gt_values = no_nan['day_ahead_prices'].values\n",
    "\n",
    "        mape_values = []\n",
    "        for model in self.model_names:\n",
    "            pred_values = no_nan[model].values\n",
    "            mape = self.calc_mae(gt_values, pred_values)\n",
    "            mape_values.append(mape / abs(no_nan[model].max() - no_nan[model].min()) * 100)\n",
    "        x = np.arange(len(self.model_names))\n",
    "\n",
    "        ax = plt.subplot(111)\n",
    "        my_cmap = plt.get_cmap(\"jet\")\n",
    "        colors = my_cmap(np.linspace(0, 1, len(self.model_names)))\n",
    "        bar = ax.bar(x, mape_values, width=0.4, align='center', label='MAPE', color=colors)\n",
    "\n",
    "        def digit_label(rects):\n",
    "            for rect in rects:\n",
    "                h = rect.get_height()\n",
    "                ax.text(rect.get_x() + rect.get_width() / 2., 1.05 * h, f'{int(h)}',\n",
    "                        ha='center', va='bottom', color=rect.get_facecolor())\n",
    "        digit_label(bar)\n",
    "        plt.title('MAPE per Model')\n",
    "        ax.set_ylabel('MAPE in %')\n",
    "        ax.set_xlabel('Model')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(self.model_names)\n",
    "        ax.set_ylim([0, 100])\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        if self.export_dir is not None:\n",
    "            plt.savefig(self.export_dir + '\\\\' + 'compare_mape.png')\n",
    "        plt.show(block=True)\n",
    "\n",
    "    def plot_single_model(self, model_name: str = ''):\n",
    "        x = self.data.index.values\n",
    "        y1 = self.data[model_name].values\n",
    "        y2 = self.data[model_name + '_AE'].values\n",
    "        y3 = self.data[model_name + '_SE'].values\n",
    "\n",
    "        fig, ax1 = plt.subplots()\n",
    "\n",
    "        # plot prediction and absolute error with € scale\n",
    "        ax1.plot(x, y1, 'b', label='y1 (sin(x)')\n",
    "        ax1.set_xlabel('X-axis')\n",
    "        ax1.set_ylabel('y1', color='b')\n",
    "        ax1.tick_params('y', colors='b')\n",
    "\n",
    "        # plot RMSE with additional scale\n",
    "        ax2 = ax1.twinx()\n",
    "\n",
    "        ax2.plot(x, y2, 'g', label='y2 (exp(-x))')\n",
    "        ax2.set_ylabel('y2', color='g')\n",
    "        ax2.tick_params('y', colors='g')\n",
    "\n",
    "        ax3 = ax1.twinx()\n",
    "\n",
    "        ax3.plot(x, y3, 'r', label='y3 (100*cos(x))')\n",
    "        ax3.spines['right'].set_position(('outward', 60))\n",
    "        ax3.set_ylabel('y3', color='r')\n",
    "        ax3.tick_params('y', colors='r')\n",
    "\n",
    "        lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        lines3, labels3 = ax3.get_legend_handles_labels()\n",
    "        lines = lines1 + lines2 + lines3\n",
    "        labels = labels1 + labels2 + labels3\n",
    "        plt.legend(lines, labels, loc='upper right')\n",
    "\n",
    "        plt.title('Multiple Y-axis Scales')\n",
    "        plt.show(block=True)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Functions\n",
    "\n",
    "def chronosForecast(model_dir, start_time, end_time):\n",
    "    model_type = \"Chronos\"\n",
    "    model_name = model_dir\n",
    "\n",
    "\n",
    "    model = ChronosModel(model_name=model_name, model_type=model_type)\n",
    "    model.model = model.custom_load(model_dir)\n",
    "\n",
    "    # Load the data\n",
    "    file_path = 'merged_data/data_collection/day_ahead_prices.csv'\n",
    "\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    data.set_index('timestamp', inplace=True)\n",
    "    data.index = data.index.tz_localize(None)\n",
    "\n",
    "    # Define forecast date range\n",
    "    start_forecast_date = pd.Timestamp(start_time).tz_localize(None)\n",
    "    end_forecast_date = pd.Timestamp(end_time).tz_localize(None)\n",
    "    forecast_timestamps = pd.date_range(start=start_forecast_date, end=end_forecast_date, freq=\"h\")\n",
    "\n",
    "    # Slice the actual data for the forecast period\n",
    "    actual_data = data.loc[forecast_timestamps].reset_index()\n",
    "    actual_data.rename(columns={'index': 'timestamp', 'day_ahead_prices_EURO': 'actual_value'}, inplace=True)\n",
    "\n",
    "    # Collect forecasts\n",
    "    forecast_results = []\n",
    "\n",
    "    for forecast_date in pd.date_range(start_forecast_date, end_forecast_date, freq=\"D\"):\n",
    "        # Determine the context period (up to 1 hour before the forecast start date)\n",
    "        context_end_date = forecast_date - pd.Timedelta(hours=1)\n",
    "\n",
    "        # Slice the data to get the context window\n",
    "        context_data = data.loc[:context_end_date].iloc[-512:]  # Limit to the last 512 entries\n",
    "\n",
    "        # Run the forecast for the current day (24 hourly values)\n",
    "        forecast_result = model.run_prediction(context_data)\n",
    "\n",
    "        # Add timestamps to forecast results\n",
    "        forecast_result['timestamp'] = pd.date_range(\n",
    "            start=forecast_date,\n",
    "            periods=24,\n",
    "            freq=\"h\"\n",
    "        )\n",
    "        forecast_results.append(forecast_result)\n",
    "\n",
    "    # Combine all forecast results into a single DataFrame\n",
    "    forecast_df = pd.concat(forecast_results, ignore_index=True)\n",
    "\n",
    "    # Merge forecasts with actual data to compute absolute error\n",
    "    combined_df = forecast_df.merge(actual_data, left_on='timestamp', right_on='timestamp', how='left')\n",
    "    combined_df['absolute_error'] = abs(combined_df['forecasted_values'] - combined_df['actual_value'])\n",
    "\n",
    "    # Drop unnecessary columns and return the result\n",
    "    return combined_df\n",
    "\n",
    "def finalBenchmarkChronos(model_dir, start_time, end_time):\n",
    "    combined_df = chronosForecast(model_dir, start_time, end_time)\n",
    "\n",
    "    # Select only the required columns\n",
    "    forecast_df = combined_df[['timestamp', 'forecasted_values']]\n",
    "\n",
    "    # Set timestamp as index\n",
    "    forecast_df.set_index('timestamp', inplace=True)\n",
    "\n",
    "    return forecast_df\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from newsapi import NewsApiClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import logging\n",
    "from typing import List, Dict, Union\n",
    "import pytz\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "class NewsEmbeddingPipeline:\n",
    "    def __init__(self, api_key: str, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline with NewsAPI credentials and embedding model.\n",
    "        \"\"\"\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API key cannot be empty\")\n",
    "            \n",
    "        self.newsapi = NewsApiClient(api_key=api_key)\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        logger.info(\"Pipeline initialized successfully\")\n",
    "\n",
    "    def fetch_news(\n",
    "        self,\n",
    "        query: str = \"energy OR electricity OR power market OR renewable\",\n",
    "        from_date: str = None,\n",
    "        to_date: str = None,\n",
    "        language: str = \"en\",\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Fetch news articles from NewsAPI with error handling and logging.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Fetching news with query: {query} from {from_date} to {to_date}\")\n",
    "            \n",
    "            # Ensure dates are properly formatted\n",
    "            if from_date:\n",
    "                datetime.strptime(from_date, '%Y-%m-%d')\n",
    "            if to_date:\n",
    "                datetime.strptime(to_date, '%Y-%m-%d')\n",
    "            \n",
    "            response = self.newsapi.get_everything(\n",
    "                q=query,\n",
    "                from_param=from_date,\n",
    "                to=to_date,\n",
    "                language=language,\n",
    "                sort_by=\"publishedAt\",\n",
    "                page_size=100\n",
    "            )\n",
    "            \n",
    "            articles = response.get(\"articles\", [])\n",
    "            logger.info(f\"Retrieved {len(articles)} articles\")\n",
    "            return articles\n",
    "\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()   \n",
    "            logger.error(f\"Error fetching news: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create embeddings for a list of texts.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Creating embeddings for {len(texts)} texts\")\n",
    "            embeddings = self.model.encode(texts)\n",
    "            # print number of dimensions in the embeddings\n",
    "            logger.info(f\"Embedding dimensions: {embeddings.shape[1]}\")\n",
    "            logger.info(\"Embeddings created successfully\")\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            logger.error(f\"Error creating embeddings: {str(e)}\")\n",
    "            return np.array([])\n",
    "\n",
    "    def process_articles(self, articles: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process articles and create embeddings.\n",
    "        \"\"\"\n",
    "        if not articles:\n",
    "            logger.warning(\"No articles to process\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            # Extract relevant information\n",
    "            processed_articles = []\n",
    "            for article in articles:\n",
    "                # Convert timestamp to UTC\n",
    "                timestamp = pd.to_datetime(article[\"publishedAt\"]).tz_convert('UTC')\n",
    "                # Round to nearest hour\n",
    "                timestamp_hourly = timestamp.round('H')\n",
    "                \n",
    "                processed_articles.append({\n",
    "                    \"timestamp\": timestamp_hourly,\n",
    "                    \"title\": article[\"title\"],\n",
    "                    \"description\": article[\"description\"],\n",
    "                    \"content\": article[\"content\"],\n",
    "                    \"source\": article[\"source\"][\"name\"],\n",
    "                    \"url\": article[\"url\"],\n",
    "                })\n",
    "\n",
    "            df = pd.DataFrame(processed_articles)\n",
    "            logger.info(f\"Processed {len(df)} articles into DataFrame\")\n",
    "\n",
    "            # Combine title and description for embedding\n",
    "            texts = [\n",
    "                f\"{row['title']} {row['description']}\"\n",
    "                if pd.notna(row[\"description\"])\n",
    "                else row[\"title\"]\n",
    "                for _, row in df.iterrows()\n",
    "            ]\n",
    "\n",
    "            # Create embeddings\n",
    "            embeddings = self.create_embeddings(texts)\n",
    "            \n",
    "            if len(embeddings) > 0:\n",
    "                # Store embeddings as numpy arrays for easier manipulation\n",
    "                df[\"embedding\"] = list(embeddings)\n",
    "                logger.info(\"Added embeddings to DataFrame\")\n",
    "            else:\n",
    "                logger.warning(\"No embeddings were created\")\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            logger.error(f\"Error processing articles: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def aggregate_embeddings(self, df: pd.DataFrame, aggregation_method: str = 'mean') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Aggregate embeddings for articles with the same timestamp.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with articles and embeddings\n",
    "            aggregation_method: 'mean' or 'weighted_mean'\n",
    "        \"\"\"\n",
    "        if df.empty:\n",
    "            return df\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Aggregating embeddings by timestamp\")\n",
    "            \n",
    "            # Group by timestamp\n",
    "            grouped = df.groupby('timestamp')\n",
    "            \n",
    "            aggregated_data = []\n",
    "            \n",
    "            for timestamp, group in grouped:\n",
    "                embeddings = np.stack(group['embedding'].values)\n",
    "                \n",
    "                if aggregation_method == 'mean':\n",
    "                    # Simple mean of embeddings\n",
    "                    combined_embedding = np.mean(embeddings, axis=0)\n",
    "                elif aggregation_method == 'weighted_mean':\n",
    "                    # You could implement different weighting schemes here\n",
    "                    # Example: weight by article length\n",
    "                    weights = [len(text) for text in group['content']]\n",
    "                    weights = np.array(weights) / np.sum(weights)\n",
    "                    combined_embedding = np.average(embeddings, axis=0, weights=weights)\n",
    "                \n",
    "                # Collect metadata\n",
    "                sources = list(group['source'].unique())\n",
    "                urls = list(group['url'])\n",
    "                \n",
    "                aggregated_data.append({\n",
    "                    'timestamp': timestamp,\n",
    "                    'embedding': combined_embedding,\n",
    "                    'num_articles': len(group),\n",
    "                    'sources': sources,\n",
    "                    'urls': urls,\n",
    "                })\n",
    "            \n",
    "            aggregated_df = pd.DataFrame(aggregated_data)\n",
    "            logger.info(f\"Aggregated {len(df)} articles into {len(aggregated_df)} timestamps\")\n",
    "            \n",
    "            return aggregated_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            logger.error(f\"Error aggregating embeddings: {str(e)}\")\n",
    "            return df\n",
    "\n",
    "    def collect_and_save_historical_data(\n",
    "        self, \n",
    "        days_back: int = 30, \n",
    "        save_path: str = \"news_embeddings_hourly.csv\",\n",
    "        aggregate: bool = True,\n",
    "        aggregation_method: str = 'mean'\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Collect historical news data day by day and save with continuous hourly timestamps.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Collecting historical data for past {days_back} days\")\n",
    "            \n",
    "            # End date should be yesterday (as today's data isn't available in free tier)\n",
    "            end_date = datetime.now(pytz.UTC).replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=1)\n",
    "            start_date = end_date - timedelta(days=days_back)\n",
    "            \n",
    "            logger.info(f\"Collecting data from {start_date} to {end_date}\")\n",
    "            all_data = []\n",
    "\n",
    "            # Process each day individually\n",
    "            for day_offset in range(days_back + 1):\n",
    "                current_date = start_date + timedelta(days=day_offset)\n",
    "                current_date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "                \n",
    "                logger.info(f\"Processing data for {current_date_str}\")\n",
    "                \n",
    "                # Fetch news for single day\n",
    "                articles = self.fetch_news(\n",
    "                    from_date=current_date_str,\n",
    "                    to_date=current_date_str\n",
    "                )\n",
    "                \n",
    "                if articles:\n",
    "                    # Process articles for this day\n",
    "                    daily_df = self.process_articles(articles)\n",
    "                    \n",
    "                    if not daily_df.empty:\n",
    "                        if aggregate:\n",
    "                            daily_df = self.aggregate_embeddings(daily_df, aggregation_method)\n",
    "                        all_data.append(daily_df)\n",
    "                \n",
    "                # Add a small delay to avoid rate limiting\n",
    "                time.sleep(1)\n",
    "            \n",
    "            # Combine all daily data if we have any\n",
    "            if all_data:\n",
    "                print(\"all_data\", all_data)\n",
    "                # Combine all data\n",
    "                combined_df = pd.concat(all_data, ignore_index=True)\n",
    "                \n",
    "                # Create continuous hourly index\n",
    "                full_hourly_index = pd.date_range(\n",
    "                    start=start_date,\n",
    "                    end=end_date,\n",
    "                    freq='h'  # Using 'h' instead of deprecated 'H'\n",
    "                )\n",
    "                \n",
    "                # Ensure timestamp is datetime index\n",
    "                combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp'])\n",
    "                combined_df.set_index('timestamp', inplace=True)\n",
    "                \n",
    "                # Reindex with full hourly range and forward fill\n",
    "                combined_df = combined_df.reindex(full_hourly_index)\n",
    "                \n",
    "                # Forward fill embeddings and other columns\n",
    "                combined_df['embedding'] = combined_df['embedding'].ffill()\n",
    "                combined_df['num_articles'] = combined_df['num_articles'].ffill().fillna(0).astype(int)\n",
    "                \n",
    "                # Handle list columns safely\n",
    "                combined_df['sources'] = combined_df['sources'].fillna(str([])).apply(lambda x: x if isinstance(x, list) else eval(x))\n",
    "                combined_df['urls'] = combined_df['urls'].fillna(str([])).apply(lambda x: x if isinstance(x, list) else eval(x))\n",
    "                \n",
    "                # Reset index to make timestamp a column again\n",
    "                combined_df.reset_index(names=['timestamp'], inplace=True)\n",
    "                \n",
    "                # Convert embeddings to lists for storage\n",
    "                combined_df['embedding'] = combined_df['embedding'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\n",
    "                \n",
    "                print(\"save_path\", save_path)\n",
    "                save_path = os.path.join(save_path, \"newsapi_embeddings.csv\")\n",
    "                # # Save combined data\n",
    "                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                combined_df.to_csv(save_path, index=False)\n",
    "                logger.info(f\"Saved data with continuous hourly timestamps to {save_path}\")\n",
    "                \n",
    "                # Log some statistics\n",
    "                logger.info(f\"Total hours covered: {len(combined_df)}\")\n",
    "                logger.info(f\"Hours with actual news: {combined_df['num_articles'].astype(bool).sum()}\")\n",
    "                \n",
    "                return combined_df\n",
    "            else:\n",
    "                # Create empty DataFrame with correct structure\n",
    "                full_hourly_index = pd.date_range(\n",
    "                    start=start_date,\n",
    "                    end=end_date,\n",
    "                    freq='h'\n",
    "                )\n",
    "                \n",
    "                empty_df = pd.DataFrame({\n",
    "                    'timestamp': full_hourly_index,\n",
    "                    'embedding': [[] for _ in range(len(full_hourly_index))],\n",
    "                    'num_articles': [0 for _ in range(len(full_hourly_index))],\n",
    "                    'sources': [[] for _ in range(len(full_hourly_index))],\n",
    "                    'urls': [[] for _ in range(len(full_hourly_index))]\n",
    "                })\n",
    "                \n",
    "                print(\"save_path\", save_path)\n",
    "                # Save empty DataFrame\n",
    "                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                empty_df.to_csv(save_path, index=False)\n",
    "                logger.info(f\"Saved empty DataFrame with continuous hourly timestamps to {save_path}\")\n",
    "                \n",
    "                return empty_df\n",
    "\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()   \n",
    "            logger.error(f\"Error collecting historical data: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "# guardian pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import logging\n",
    "import pytz\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class GuardianEmbeddingPipeline:\n",
    "    def __init__(self, api_key: str, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API key cannot be empty\")\n",
    "            \n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://content.guardianapis.com/search\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        logger.info(\"Pipeline initialized successfully\")\n",
    "\n",
    "    def fetch_news(\n",
    "        self,\n",
    "        query: str = \"energy OR electricity OR renewable OR weather\",\n",
    "        from_date: str = None,\n",
    "        to_date: str = None,\n",
    "        page_size: int = 50,\n",
    "        max_pages: int = 20\n",
    "    ) -> List[Dict]:\n",
    "        try:\n",
    "            logger.info(f\"Fetching news with query: {query} from {from_date} to {to_date}\")\n",
    "            \n",
    "            all_articles = []\n",
    "            current_page = 1\n",
    "            \n",
    "            while current_page <= max_pages:\n",
    "                params = {\n",
    "                    'q': query,\n",
    "                    'section': 'business|environment|technology|money',\n",
    "                    'from-date': from_date,\n",
    "                    'to-date': to_date,\n",
    "                    'page-size': page_size,\n",
    "                    'page': current_page,\n",
    "                    'api-key': self.api_key,\n",
    "                    'show-fields': 'all',\n",
    "                    'order-by': 'newest'\n",
    "                }\n",
    "                \n",
    "                response = requests.get(self.base_url, params=params)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                data = response.json()['response']\n",
    "                articles = data.get('results', [])\n",
    "                \n",
    "                if not articles:\n",
    "                    break\n",
    "                    \n",
    "                all_articles.extend(articles)\n",
    "                \n",
    "                if current_page >= data['pages']:\n",
    "                    break\n",
    "                    \n",
    "                current_page += 1\n",
    "                \n",
    "            logger.info(f\"Retrieved {len(all_articles)} articles\")\n",
    "            return all_articles\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Request error: {str(e)}\")\n",
    "            return []\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"JSON decode error: {str(e)}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching news: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Create embeddings for a list of texts.\"\"\"\n",
    "        if not texts:\n",
    "            logger.warning(\"No texts provided for embedding\")\n",
    "            return np.array([])\n",
    "            \n",
    "        try:\n",
    "            logger.info(f\"Creating embeddings for {len(texts)} texts\")\n",
    "            embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "            logger.info(f\"Embedding dimensions: {embeddings.shape[1]}\")\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating embeddings: {str(e)}\")\n",
    "            return np.array([])\n",
    "\n",
    "    def process_articles(self, articles: List[Dict]) -> pd.DataFrame:\n",
    "        if not articles:\n",
    "            logger.warning(\"No articles to process\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            processed_articles = []\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    timestamp = pd.to_datetime(article['webPublicationDate']).tz_convert('UTC')\n",
    "                    timestamp_hourly = timestamp.round('h')\n",
    "                    \n",
    "                    fields = article.get('fields', {})\n",
    "                    processed_articles.append({\n",
    "                        \"timestamp\": timestamp_hourly,\n",
    "                        \"title\": article.get('webTitle', ''),\n",
    "                        \"description\": fields.get('trailText', ''),\n",
    "                        \"content\": fields.get('bodyText', ''),\n",
    "                        \"source\": \"The Guardian\",\n",
    "                        \"url\": article.get('webUrl', ''),\n",
    "                        \"section\": article.get('sectionName', ''),\n",
    "                    })\n",
    "                except (KeyError, ValueError) as e:\n",
    "                    logger.warning(f\"Skipping malformed article: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            if not processed_articles:\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            df = pd.DataFrame(processed_articles)\n",
    "            \n",
    "            df['text_for_embedding'] = df.apply(\n",
    "                lambda row: f\"{row['title']} {row['description']}\" if pd.notna(row[\"description\"]) else row[\"title\"],\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            embeddings = self.create_embeddings(df['text_for_embedding'].tolist())\n",
    "            \n",
    "            if len(embeddings) > 0:\n",
    "                df[\"embedding\"] = list(embeddings)\n",
    "                df.drop('text_for_embedding', axis=1, inplace=True)\n",
    "                logger.info(\"Added embeddings to DataFrame\")\n",
    "            else:\n",
    "                logger.warning(\"No embeddings were created\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing articles: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def aggregate_embeddings(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if df.empty:\n",
    "            return df\n",
    "\n",
    "        try:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.sort_values('timestamp')\n",
    "            \n",
    "            aggregated = df.groupby('timestamp').agg({\n",
    "                'embedding': lambda x: np.mean([emb for emb in x if isinstance(emb, (np.ndarray, list))], axis=0),\n",
    "                'source': lambda x: list(set(x)),\n",
    "                'url': list,\n",
    "                'section': lambda x: list(set(x)),\n",
    "                'title': 'count'\n",
    "            }).reset_index()\n",
    "            \n",
    "            aggregated.rename(columns={'title': 'num_articles'}, inplace=True)\n",
    "            \n",
    "            logger.info(f\"Aggregated {len(df)} articles into {len(aggregated)} timestamps\")\n",
    "            return aggregated\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error aggregating embeddings: {str(e)}\")\n",
    "            return df\n",
    "\n",
    "    def collect_and_save_historical_data(\n",
    "        self, \n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        save_path: str = \"data/guardian_embeddings\",\n",
    "        query: str = \"energy OR electricity OR renewable OR weather\"\n",
    "    ) -> pd.DataFrame:\n",
    "        try:\n",
    "            start_dt = pd.to_datetime(start_date).tz_localize(pytz.UTC).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "            end_dt = pd.to_datetime(end_date).tz_localize(pytz.UTC).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "            \n",
    "            if start_dt > end_dt:\n",
    "                raise ValueError(\"Start date must be before end date\")\n",
    "                \n",
    "            logger.info(f\"Collecting data from {start_dt} to {end_dt}\")\n",
    "            \n",
    "            from_date = start_dt.strftime(\"%Y-%m-%d\")\n",
    "            to_date = end_dt.strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "            articles = self.fetch_news(from_date=from_date, to_date=to_date, query=query)\n",
    "            if articles:\n",
    "                df = self.process_articles(articles)\n",
    "                if not df.empty:\n",
    "                    aggregated_df = self.aggregate_embeddings(df)\n",
    "                else:\n",
    "                    logger.warning(\"No articles processed\")\n",
    "                    return pd.DataFrame()\n",
    "            else:\n",
    "                logger.warning(\"No articles fetched\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            full_hourly_index = pd.date_range(\n",
    "                start=start_dt,\n",
    "                end=end_dt,\n",
    "                freq='h',\n",
    "                tz='UTC'\n",
    "            )\n",
    "            \n",
    "            template_df = pd.DataFrame({\n",
    "                'timestamp': full_hourly_index,\n",
    "                'embedding': [None for _ in range(len(full_hourly_index))],\n",
    "                'source': [None for _ in range(len(full_hourly_index))],\n",
    "                'url': [None for _ in range(len(full_hourly_index))],\n",
    "                'section': [None for _ in range(len(full_hourly_index))],\n",
    "                'num_articles': [None for _ in range(len(full_hourly_index))]\n",
    "            })\n",
    "            \n",
    "            final_df = pd.merge(\n",
    "                template_df,\n",
    "                aggregated_df,\n",
    "                on='timestamp',\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            final_df['embedding'] = final_df['embedding_y'].fillna(final_df['embedding_x'])\n",
    "            final_df['source'] = final_df['source_y'].fillna(final_df['source_x'])\n",
    "            final_df['url'] = final_df['url_y'].fillna(final_df['url_x'])\n",
    "            final_df['section'] = final_df['section_y'].fillna(final_df['section_x'])\n",
    "            final_df['num_articles'] = final_df['num_articles_y'].fillna(final_df['num_articles_x'])\n",
    "            \n",
    "            final_df['embedding'] = final_df['embedding'].ffill()\n",
    "            final_df['source'] = final_df['source'].ffill().apply(lambda x: x if isinstance(x, list) else [])\n",
    "            final_df['url'] = final_df['url'].ffill().apply(lambda x: x if isinstance(x, list) else [])\n",
    "            final_df['section'] = final_df['section'].ffill().apply(lambda x: x if isinstance(x, list) else [])\n",
    "            final_df['num_articles'] = final_df['num_articles'].fillna(0).astype(int)\n",
    "            \n",
    "            final_df = final_df[['timestamp', 'embedding', 'source', 'url', 'section', 'num_articles']]\n",
    "            \n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            output_file = os.path.join(save_path, f\"guardian_embeddings.csv\")\n",
    "            \n",
    "            final_df['embedding'] = final_df['embedding'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\n",
    "            final_df.to_csv(output_file, index=False)\n",
    "            \n",
    "            logger.info(f\"Total hours: {len(final_df)}\")\n",
    "            logger.info(f\"Hours with articles: {(final_df['num_articles'] > 0).sum()}\")\n",
    "            \n",
    "            return final_df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error collecting historical data: {str(e)}\")\n",
    "            return pd.DataFrame()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def dataset_Setup(dataset):\n",
    "    #CSV-Datei laden\n",
    "    file_path = \"../final-submission/merged_data/allData.csv\"\n",
    "    if dataset=='all':        \n",
    "        loaded_dataframe = \"AllData.csv\"\n",
    "        data = pd.read_csv(file_path)\n",
    "    elif dataset=='less':       \n",
    "        columns_to_load = [\n",
    "            \"Date\", \"Forecasted Load\", \"day_ahead_prices_EURO\", \"Actual Aggregated\", \"Oil WTI\", \n",
    "            \"Natural Gas\", \"Coal\", \"Uran\", \"actual_E_Total_Gridload_MWh\", \"actual_E_Residual_Load_MWh\", \n",
    "            \"actual_generation_E_Biomass_MWh\", \"actual_generation_E_Hydropower_MWh\", \"actual_generation_E_Windoffshore_MWh\", \n",
    "            \"actual_generation_E_Windonshore_MWh\", \"actual_generation_E_Photovoltaics_MWh\", \"actual_generation_E_OtherRenewable_MWh\", \n",
    "            \"actual_generation_E_Nuclear_MWh\", \"actual_generation_E_Lignite_MWh\", \"actual_generation_E_HardCoal_MWh\", \n",
    "            \"actual_generation_E_FossilGas_MWh\", \"actual_generation_E_HydroPumpedStorage_MWh\", \"actual_generation_E_OtherConventional_MWh\", \n",
    "            \"carbon_price_EURO\", \"wind_speed_ms_Hamburg_review\", \"T_temperature_C_Muenchen_review\", \n",
    "            \"sunshine_min_Muenchen_review\", \"month\", \"weekday\", \"week_of_year\", \"is_weekend\", \"is_holiday\"\n",
    "        ]\n",
    "        loaded_dataframe = \"lessData.csv\"        \n",
    "        data = pd.read_csv(file_path, usecols=columns_to_load)\n",
    "        print(data.head())\n",
    "    elif dataset=='dayhead':\n",
    "        loaded_dataframe = \"Dayahead only\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        data = df[[\"Date\", \"day_ahead_prices_EURO\"]]\n",
    "    else:\n",
    "        print(f\"Error: No valid 'dataset': {dataset}\")\n",
    "        sys.exit(1)\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.rename(columns={'Date': 'timestamp'}, inplace=True)\n",
    "    return loaded_dataframe, file_path, data\n",
    "\n",
    "def correlation_calculation(data, output_folder_autogluon):\n",
    "    os.makedirs(output_folder_autogluon, exist_ok=True)\n",
    "    data_copy = data.copy()\n",
    "    data_copy = data_copy.select_dtypes(include=[np.number])\n",
    "    correlation_matrix = data_copy.corr()\n",
    "    print(correlation_matrix)\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title(\"Correlation Matrix of Extra Features\")\n",
    "    plt.savefig(f\"{output_folder_autogluon}/correlation.png\")\n",
    "\n",
    "    corr_matrix = correlation_matrix \n",
    "    threshold = 0.95\n",
    "    high_corr_pairs = []\n",
    "    for col in corr_matrix.columns: # Loop through the correlation matrix to find pairs with correlation above the threshold\n",
    "        high_corr = corr_matrix.index[corr_matrix[col] > threshold].tolist()\n",
    "        high_corr = [x for x in high_corr if x != col]\n",
    "        for pair in high_corr:\n",
    "            high_corr_pairs.append((col, pair, corr_matrix[col][pair]))\n",
    "\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Column 1', 'Column 2', 'Correlation'])\n",
    "\n",
    "    high_corr_df.to_csv(f\"{output_folder_autogluon}/high_correlation_pairs.csv\", index=False)\n",
    "    print(\"High correlation pairs saved to 'high_correlation_pairs.csv'\")\n",
    "\n",
    "    missing_percentage = data.isnull().mean() * 100\n",
    "    print(\"Missing values percentage per column:\")\n",
    "    print(missing_percentage)\n",
    "\n",
    "    missing_df = pd.DataFrame(missing_percentage)\n",
    "    missing_df.to_csv(f\"{output_folder_autogluon}/missing.csv\", index=True)\n",
    "\n",
    "    high_missing_cols = missing_percentage[missing_percentage > 50].index #more than 50% are missing\n",
    "    print(\"Columns with more than 50% missing values:\", high_missing_cols) \n",
    "\n",
    "def set_known_covariates(data):    \n",
    "    known_covariates_columns = ['item_id','month','weekday','week_of_year','is_weekend','is_holiday','superbowl_bool','oktoberfest_bool','berlinale_bool','precipitationTotal_mm_KoelnBonn_review', 'sunshine_min_KoelnBonn_review','stationPressure_hPa_KoelnBonn_review', 'surfacePressure_hPa_KoelnBonn_review','T_temperature_C_KoelnBonn_review', 'humidity_Percent_KoelnBonn_review','wind_speed_ms_KoelnBonn_review', 'wind_direction_degree_KoelnBonn_review', 'clouds_KoelnBonn_review','T_temperature_C_Hamburg_review', 'humidity_Percent_Hamburg_review', 'stationPressure_hPa_Hamburg_review','surfacePressure_hPa_Hamburg_review', 'wind_speed_ms_Hamburg_review', 'wind_direction_degree_Hamburg_review','clouds_Hamburg_review', 'precipitationTotal_mm_Hamburg_review', 'sunshine_min_Hamburg_review', 'precipitationTotal_mm_Muenchen_review', 'sunshine_min_Muenchen_review', 'stationPressure_hPa_Muenchen_review','surfacePressure_hPa_Muenchen_review', 'T_temperature_C_Muenchen_review', 'humidity_Percent_Muenchen_review','clouds_Muenchen_review', 'wind_speed_ms_Muenchen_review', 'wind_direction_degree_Muenchen_review', 'Covid factor']\n",
    "\n",
    "    existing_columns = []\n",
    "    for col in known_covariates_columns:\n",
    "        if col in data.columns:\n",
    "            existing_columns.append(col)  \n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' is missing and will be ignored.\")\n",
    "\n",
    "    known_covariates = data[['timestamp'] + existing_columns]\n",
    "    return known_covariates\n",
    "\n",
    "def train_autogluon(set_preset,set_time_limit,dataset,provide_known_covariables,output_folder_autogluon,loaded_dataframe, file_path, data):\n",
    "    print(f\"Zeitlimit:{set_time_limit}\")\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    print(f\"Preset: {set_preset}\")\n",
    "    #Zeitstempel beim Start des Skripts erstellen\n",
    "    start_time_script = time.time()\n",
    "    script_start_time = datetime.now().strftime('%Y.%m.%d-%H.%M')\n",
    "    add_path=f\"{script_start_time}_{set_preset}_{dataset}\"\n",
    "    output_folder_train_ag=os.path.join(output_folder_autogluon, add_path)\n",
    "    #output_folder = f\"/data/horse/ws/fewa833b-time-series-forecast/AutoGluon/Felix/AG-Preset/{script_start_time}_{set_preset}_{dataset}\"\n",
    "    os.makedirs(output_folder_train_ag, exist_ok=True)  #Ordner erstellen, falls er nicht existiert\n",
    "    \n",
    "        \n",
    "    data['item_id'] = 'item_1'    \n",
    "    required_columns = ['timestamp', 'day_ahead_prices_EURO', 'item_id']\n",
    "    for col in required_columns:\n",
    "        if col not in data.columns:\n",
    "            raise ValueError(f\"Spalte '{col}' fehlt in der Datei {file_path}.\")\n",
    "        \n",
    "    if provide_known_covariables:\n",
    "        known_covariates=set_known_covariates(data)  \n",
    "\n",
    "    \n",
    "    train_data =data[(data['timestamp'] > '2015-01-04') & (data['timestamp'] < '2023-12-01')]\n",
    "    val_data = data[(data['timestamp'] >= '2023-12-01') & (data['timestamp'] < '2024-06-01')]\n",
    "    test_data = data[(data['timestamp'] >= '2024-06-01') & (data['timestamp'] < '2024-12-01')]\n",
    "\n",
    "    print(f\"Trainingsdaten bis 01.12.2023: {train_data.shape[0]} Zeilen\")\n",
    "    print(f\"Validierungsdaten bis 01.12.2023: {val_data.shape[0]} Zeilen\")\n",
    "    print(f\"Testdaten ab 01.06.2024: {test_data.shape[0]} Zeilen\")\n",
    "\n",
    "    \n",
    "    if train_data.empty or val_data.empty or test_data.empty:\n",
    "        raise ValueError(\"Eine der Datenaufteilungen (Train, Validation, Test) ist leer.\")\n",
    "\n",
    "    \n",
    "    train_data.to_csv(f\"{output_folder_train_ag}/train_data.csv\", index=False)\n",
    "    val_data.to_csv(f\"{output_folder_train_ag}/val_data.csv\", index=False)\n",
    "    test_data.to_csv(f\"{output_folder_train_ag}/test_data.csv\", index=False)\n",
    "\n",
    "    autogluon_path = os.path.join(output_folder_train_ag, \"AutogluonModels\")\n",
    "    predictor = TimeSeriesPredictor(\n",
    "        target=\"day_ahead_prices_EURO\",  \n",
    "        prediction_length=24,\n",
    "        path=autogluon_path,                   \n",
    "        eval_metric=\"MAE\"             \n",
    "    )\n",
    "    if set_preset=='best_quality' or set_preset=='fast_training' or set_preset=='high_quality' or set_preset=='medium_quality':\n",
    "        predictor.fit(train_data=train_data, \n",
    "                    tuning_data=val_data,            \n",
    "                    presets=set_preset,\n",
    "                    time_limit=set_time_limit\n",
    "                    )\n",
    "    elif set_preset=='hp1':\n",
    "        predictor.fit(train_data=train_data, \n",
    "                        tuning_data=val_data,\n",
    "                        time_limit=set_time_limit,            \n",
    "                        hyperparameters = {\n",
    "                            'ADIDA':{},\n",
    "                            'AutoARIMA':{}, \n",
    "                            'AutoCES':{}, \n",
    "                            'AutoETS':{}, \n",
    "                            'Croston':{}, \n",
    "                            'IMAPA':{}, \n",
    "                            'NPTS':{},                  \n",
    "                            'Chronos': {\n",
    "                                'num_epochs': 2000,        \n",
    "                                'batch_size': 64,          \n",
    "                            },\n",
    "                            'DeepAR': {\n",
    "                                'num_lstm_layers': 4,      \n",
    "                                'num_lstm_units': 256,     \n",
    "                                'dropout_rate': 0.3,       \n",
    "                                'learning_rate': 1e-4,     \n",
    "                                'num_epochs': 2000,        \n",
    "                                'batch_size': 64,          \n",
    "                            },\n",
    "                            'DirectTabular': {\n",
    "                                'num_layers': 10,           \n",
    "                                'hidden_size': 512,        \n",
    "                                'dropout_rate': 0.3,       \n",
    "                            },\n",
    "                            'DLinear': {\n",
    "                                'num_layers': 10,          \n",
    "                                'hidden_size': 512,       \n",
    "                                'dropout_rate': 0.3,       \n",
    "                            },\n",
    "                            'PatchTST': {\n",
    "                                'num_layers': 10,           \n",
    "                                'hidden_size': 512,        \n",
    "                                'dropout_rate': 0.3,       \n",
    "                            },\n",
    "                            'RecursiveTabular': {\n",
    "                                'num_layers': 10,           \n",
    "                                'hidden_size': 512,        \n",
    "                                'dropout_rate': 0.3,       \n",
    "                            },\n",
    "                            'SimpleFeedForward': {\n",
    "                                'num_layers': 10,           \n",
    "                                'hidden_size': 512,        \n",
    "                                'dropout_rate': 0.3,       \n",
    "                            },\n",
    "                            'TemporalFusionTransformer': {\n",
    "                                'num_encoder_layers': 6,  \n",
    "                                'num_decoder_layers': 6,\n",
    "                                'attention_heads': 16,\n",
    "                                'hidden_size': 512,        \n",
    "                                'dropout_rate': 0.3,       \n",
    "                            },\n",
    "                            'TiDE': {\n",
    "                                'num_layers': 10,           \n",
    "                                'hidden_size': 512,        \n",
    "                                'dropout_rate': 0.3,       \n",
    "                            },\n",
    "                            'WaveNet': {\n",
    "                                'num_layers': 8,           \n",
    "                                'hidden_size': 512,        \n",
    "                                'dropout_rate': 0.3,       \n",
    "                            }\n",
    "                        }\n",
    "            )\n",
    "    \n",
    "    else: \n",
    "        print('No preset found')\n",
    "    \n",
    "    #Check Feature Importance\n",
    "    try:\n",
    "        importance = predictor.feature_importance(data=val_data)\n",
    "        importance_df = pd.DataFrame(importance)\n",
    "        importance_df.to_csv(f\"{output_folder_train_ag}/feature_importance.csv\", index=True)\n",
    "        print(\"Feature Importance saved.\")\n",
    "    except ValueError as e:\n",
    "        print(\"Feature importance could not be computed:\", e)\n",
    "     \n",
    "    start_time = test_data[\"timestamp\"].min()\n",
    "    end_time = test_data[\"timestamp\"].max()\n",
    "    print(f\"Starttime: {start_time}, endtime: {end_time}\")\n",
    "\n",
    "    predictions = []\n",
    "    current_time = start_time\n",
    "\n",
    "    while current_time <= end_time:\n",
    "        current_day_data = data[            \n",
    "            (data[\"timestamp\"] < current_time)\n",
    "        ]\n",
    "\n",
    "        if not current_day_data.empty:\n",
    "            if provide_known_covariables:\n",
    "                predicted_values = predictor.predict(current_day_data,known_covariates=known_covariates)\n",
    "            else:\n",
    "                predicted_values = predictor.predict(current_day_data)\n",
    "            print(f\"Predicted Values: {predicted_values}\")            \n",
    "            if \"mean\" in predicted_values:\n",
    "                mean_predicted_values = predicted_values[\"mean\"].values\n",
    "            else:\n",
    "                raise ValueError(\"'mean' column not found in predictions\")\n",
    "            next_24_hours = [current_time + timedelta(hours=i) for i in range(24)]\n",
    "            print(f\"Next 24 timestamps: {next_24_hours}\")            \n",
    "            if len(mean_predicted_values) != len(next_24_hours):\n",
    "                raise ValueError(\"Mismatch between predicted values and generated timestamps\")        \n",
    "            predictions.extend([\n",
    "                {\"timestamp\": ts, \"predicted\": pred}\n",
    "                for ts, pred in zip(next_24_hours, mean_predicted_values)\n",
    "            ])        \n",
    "        current_time += timedelta(days=1)\n",
    "\n",
    "    if predictions:\n",
    "        final_predictions = pd.DataFrame(predictions)\n",
    "        final_predictions[\"timestamp\"] = pd.to_datetime(final_predictions[\"timestamp\"])  # Sicherstellen, dass der Timestamp korrekt ist\n",
    "        final_predictions.to_csv(f\"{output_folder_train_ag}/results_data.csv\", index=False)\n",
    "    else:\n",
    "        print(\"Keine Vorhersagen konnten gemacht werden.\")\n",
    "        final_predictions = pd.DataFrame(columns=[\"timestamp\", \"predicted\"])\n",
    "\n",
    "    test_data_comparison = test_data[['timestamp', 'day_ahead_prices_EURO']].rename(columns={'day_ahead_prices_EURO': 'actual_price'})\n",
    "    test_data_comparison['timestamp'] = pd.to_datetime(test_data_comparison['timestamp'])\n",
    "\n",
    "    comparison = pd.merge(test_data_comparison, final_predictions, on='timestamp', how='outer')\n",
    "    comparison.to_csv(f\"{output_folder_train_ag}/comparison.csv\", index=False)\n",
    "\n",
    "\n",
    "    y_true = comparison[\"actual_price\"]\n",
    "    y_pred = comparison[\"predicted\"]\n",
    "    mae_sklearn= mae_sklearn = mean_absolute_error(y_true, y_pred)\n",
    "    rmse_sklearn = root_mean_squared_error(y_true, y_pred, squared=False)    \n",
    "\n",
    "    print(f\"Alle Ergebnisse wurden im Ordner '{output_folder_train_ag}' gespeichert.\")\n",
    "    duration_skript=  time.time() - start_time_script\n",
    "    print(f\"Dauer: {duration_skript}\")\n",
    "    filename_mape = f\"MAE{mae_sklearn:.2f}.txt\"\n",
    "    file_path_mape = os.path.join(output_folder_train_ag, filename_mape)\n",
    "    with open(file_path_mape, \"w\") as file:\n",
    "        file.write(f\"MAE SKlearn: {mae_sklearn}\\n\")\n",
    "        file.write(f\"RMSE SKlearn: {rmse_sklearn}\\n\")\n",
    "        file.write(f\"Modelpreset: {set_preset}\\n\")\n",
    "        file.write(f\"Dataset: {dataset}\\n\")\n",
    "        file.write(f\"Duration: {duration_skript}\")\n",
    "        file.write(f\"Knowncoavariables: {provide_known_covariables}\")\n",
    "    print(f\"Ergebnisse wurden in die Datei '{file_path_mape}' gespeichert.\")\n",
    "\n",
    "    data = {\n",
    "        \"Dataset\": dataset,\n",
    "        \"Modelpreset\": set_preset,\n",
    "        \"Knowncoavariables\": provide_known_covariables,\n",
    "        \"MAE SKlearn\": mae_sklearn,\n",
    "        \"RMSE SKlearn\": rmse_sklearn,\n",
    "        \"Duration\": duration_skript,        \n",
    "        \"Date\":script_start_time,\n",
    "    }\n",
    "\n",
    "    #write to csv\n",
    "    file_path_csv =os.path.join(output_folder_autogluon, \"preset-comparison.csv\")\n",
    "    #file_path_csv ='/data/horse/ws/fewa833b-time-series-forecast/AutoGluon/Felix/AG-Preset/preset-comparison.csv'\n",
    "    file_exists = os.path.isfile(file_path_csv)\n",
    "    with open(file_path_csv, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=data.keys())\n",
    "        if not file_exists:\n",
    "            #set header if new file\n",
    "            writer.writeheader()\n",
    "        #write data\n",
    "        writer.writerow(data)\n",
    "    print(f\"Ergebnisse wurden in die Datei '{file_path_csv}' gespeichert.\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def expand_embeddings(df):\n",
    "    \"\"\"\n",
    "    Convert string embeddings to separate numeric columns\n",
    "    \"\"\"\n",
    "    if 'embedding' not in df.columns:\n",
    "        return df\n",
    "        \n",
    "    print(\"Converting string embeddings to numeric arrays...\")\n",
    "    # Convert string representation of list to actual list of floats\n",
    "    embeddings = df['embedding'].apply(eval)  # converts string representation to list\n",
    "    # Convert to numpy array for easier handling\n",
    "    embedding_array = np.vstack(embeddings)\n",
    "    # Create separate columns for each embedding dimension\n",
    "    embedding_cols = [f'embedding_{i}' for i in range(embedding_array.shape[1])]\n",
    "    embedding_df = pd.DataFrame(embedding_array, columns=embedding_cols, index=df.index)\n",
    "    \n",
    "    # Drop original embedding column and add expanded columns\n",
    "    df = df.drop(columns=['embedding'])\n",
    "    df = pd.concat([df, embedding_df], axis=1)\n",
    "    \n",
    "    print(f\"Expanded embeddings into {len(embedding_cols)} dimensions\")\n",
    "    return df\n",
    "\n",
    "def reduce_embedding_dimensionality(df, n_components=50):\n",
    "    \"\"\"\n",
    "    Reduce dimensionality of embeddings using PCA\n",
    "    \"\"\"\n",
    "    # Extract embedding columns\n",
    "    embedding_cols = [col for col in df.columns if col.startswith('embedding_')]\n",
    "    if not embedding_cols:\n",
    "        return df, None\n",
    "    \n",
    "    # Standardize the embeddings\n",
    "    scaler = StandardScaler()\n",
    "    embeddings_scaled = scaler.fit_transform(df[embedding_cols])\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    embeddings_reduced = pca.fit_transform(embeddings_scaled)\n",
    "    \n",
    "    # Create new dataframe with reduced embeddings\n",
    "    reduced_cols = [f'pca_embedding_{i}' for i in range(n_components)]\n",
    "    embeddings_df = pd.DataFrame(embeddings_reduced, columns=reduced_cols, index=df.index)\n",
    "    \n",
    "    # Drop original embeddings and add reduced ones\n",
    "    df_reduced = df.drop(columns=embedding_cols)\n",
    "    df_reduced = pd.concat([df_reduced, embeddings_df], axis=1)\n",
    "    \n",
    "    # Print variance explained\n",
    "    variance_explained = np.sum(pca.explained_variance_ratio_)\n",
    "    print(f\"Variance explained by {n_components} components: {variance_explained:.3f}\")\n",
    "    \n",
    "    return df_reduced, pca\n",
    "\n",
    "def prepare_data_simple(df, use_pca=False, n_pca_components=50):\n",
    "    \"\"\"\n",
    "    Simple data preparation focusing only on price and embeddings.\n",
    "    Option to use PCA or keep full dimensionality.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Drop metadata columns if they exist\n",
    "    metadata_cols = ['source', 'url', 'section', 'num_articles']\n",
    "    df = df.drop(columns=[col for col in metadata_cols if col in df.columns])\n",
    "    \n",
    "    # Create target variable (next day's price)\n",
    "    df['target'] = df['day_ahead_prices_EURO']\n",
    "    \n",
    "    # Expand embeddings from string to numeric columns\n",
    "    \n",
    "    # Optionally reduce dimensionality\n",
    "    pca = None\n",
    "    if use_pca:\n",
    "        df = expand_embeddings(df)\n",
    "        df, pca = reduce_embedding_dimensionality(df, n_components=n_pca_components)\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Define features\n",
    "    base_features = []\n",
    "    if use_pca:\n",
    "        embedding_features = [col for col in df.columns if col.startswith('pca_embedding_')]\n",
    "    else:\n",
    "        embedding_features = [col for col in df.columns if col.startswith('embedding_')]\n",
    "    features = base_features + embedding_features\n",
    "    \n",
    "    df['item_id'] = 'price_series'\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']).dt.tz_localize(None)\n",
    "    \n",
    "    return df, features, pca\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prepare_embedding_data(guardian_embeddings, newsapi_embeddings, day_ahead_prices):\n",
    "    guardian_embeddings_day_ahead = pd.merge(guardian_embeddings, day_ahead_prices, on='timestamp', how='inner')\n",
    "    newsapi_embeddings_day_ahead = pd.merge(newsapi_embeddings, day_ahead_prices, on='timestamp', how='inner')\n",
    "\n",
    "    # cut day ahead prices to the same time frame as the embeddings\n",
    "    guardian_embeddings_start = guardian_embeddings_day_ahead['timestamp'].min()\n",
    "    guardian_embeddings_end = guardian_embeddings_day_ahead['timestamp'].max()\n",
    "    \n",
    "    \n",
    "    newsapi_embeddings_day_ahead = newsapi_embeddings_day_ahead[(newsapi_embeddings_day_ahead['timestamp'] >= guardian_embeddings_start) & (newsapi_embeddings_day_ahead['timestamp'] <= guardian_embeddings_end)]\n",
    "    \n",
    "    news_embeddings_start = newsapi_embeddings_day_ahead['timestamp'].min()\n",
    "    news_embeddings_end = newsapi_embeddings_day_ahead['timestamp'].max()\n",
    "\n",
    "    guardian_embeddings_day_ahead = guardian_embeddings_day_ahead[(guardian_embeddings_day_ahead['timestamp'] >= news_embeddings_start) & (guardian_embeddings_day_ahead['timestamp'] <= news_embeddings_end)]\n",
    "    \n",
    "    day_ahead_prices = day_ahead_prices[(day_ahead_prices['timestamp'] >= news_embeddings_start) & (day_ahead_prices['timestamp'] <= news_embeddings_end)]\n",
    "\n",
    "    # remove the last 24 rows\n",
    "    guardian_embeddings_day_ahead = guardian_embeddings_day_ahead[:-24]\n",
    "    newsapi_embeddings_day_ahead = newsapi_embeddings_day_ahead[:-24]\n",
    "    day_ahead_prices_for_prediction = day_ahead_prices.copy()[:-24]\n",
    "    ground_truth = day_ahead_prices.copy()[-24:]\n",
    "    \n",
    "    return guardian_embeddings_day_ahead, newsapi_embeddings_day_ahead, day_ahead_prices_for_prediction, ground_truth\n",
    "\n",
    "def predict_embedding_data(predictor_path, df):\n",
    "    predictor = TimeSeriesPredictor.load(predictor_path)\n",
    "    \n",
    "    df_embeddings, _, _ = prepare_data_simple(df)\n",
    "    \n",
    "    predict_data = TimeSeriesDataFrame(df_embeddings)\n",
    "    \n",
    "    predictions = predictor.predict(predict_data)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def visualize_embedding_model_results(predictions, ground_truth, label: str):\n",
    "    \n",
    "    predictions = predictions.reset_index()\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(predictions['mean'], ground_truth['day_ahead_prices_EURO']))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(ground_truth['timestamp'], ground_truth['day_ahead_prices_EURO'], label='Actual Price')\n",
    "    plt.plot(predictions['timestamp'], predictions['mean'], label='Predicted Price')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title(f'Actual vs. Predicted Prices {label} (RMSE: {rmse:.2f})')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Download All Of The Data."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from entsoe import EntsoeRawClient, EntsoePandasClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv \n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "\n",
    "## Entsoe\n",
    "\n",
    "ENTSOE_API_KEY = \"562a20c4-03b0-4ee6-a692-19d534b4393a\"\n",
    "client = EntsoePandasClient(api_key=ENTSOE_API_KEY)\n",
    "\n",
    "start = pd.Timestamp('20150101', tz='UTC')\n",
    "change_date = pd.Timestamp('20181001', tz='UTC')\n",
    "# end = pd.Timestamp(datetime.datetime.now(), tz='UTC')\n",
    "end = pd.Timestamp(datetime.datetime.now() + pd.DateOffset(days=2), tz='UTC')\n",
    "\n",
    "\n",
    "print(os.getcwd())\n",
    "out_dir = 'merged_data/data_collection'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "country_code_old = 'DE_AT_LU'\n",
    "country_code_new = 'DE_LU'\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10), retry=retry_if_exception_type(Exception))\n",
    "def query_entsoe_data(query_func, country_code, start, end):\n",
    "    try:\n",
    "        df = query_func(country_code, start=start, end=end)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying data: {e}\")\n",
    "        raise\n",
    "    return df\n",
    "\n",
    "def merge_data(query_func):\n",
    "    data_old = query_entsoe_data(query_func, country_code_old, start, change_date)\n",
    "    \n",
    "    data_new = query_entsoe_data(query_func, country_code_new, change_date, end)\n",
    "\n",
    "    if not isinstance(data_old, pd.DataFrame):\n",
    "        data_old = data_old.to_frame()\n",
    "    if not isinstance(data_new, pd.DataFrame):\n",
    "        data_new = data_new.to_frame()\n",
    "    \n",
    "    if not data_old.empty and not data_new.empty:\n",
    "        if len(data_old.columns) != len(data_new.columns):\n",
    "            same_columns = list(set(data_old.columns) & set(data_new.columns))\n",
    "            data_old = data_old[same_columns]\n",
    "            data_new = data_new[same_columns]\n",
    "        else:\n",
    "            data_new.columns = data_old.columns\n",
    "    df_combined = pd.concat([data_old, data_new])\n",
    "    df_combined.index = df_combined.index.tz_convert('UTC')\n",
    "    return df_combined\n",
    "\n",
    "def save_df_with_timestamp(df, filename):\n",
    "    df_copy = df.copy()\n",
    "    df_copy.index.name = 'timestamp'\n",
    "    df_copy.to_csv(filename)\n",
    "\n",
    "# Day-ahead prices (EUR/MWh)\n",
    "day_ahead_prices = merge_data(client.query_day_ahead_prices)\n",
    "day_ahead_prices = day_ahead_prices.rename(columns={day_ahead_prices.columns[0]: 'day_ahead_prices_EURO'})\n",
    "save_df_with_timestamp(day_ahead_prices, f'{out_dir}/day_ahead_prices.csv')\n",
    "print('Day-ahead prices done')\n",
    "\n",
    "# Load forecast (MWh)\n",
    "load_forecast = merge_data(client.query_load_forecast)\n",
    "load_forecast = load_forecast.rename(columns={load_forecast.columns[0]: 'E_load_forecast_MWh'})\n",
    "save_df_with_timestamp(load_forecast, f'{out_dir}/load_forecast.csv')\n",
    "print('Load forecast done')\n",
    "\n",
    "# Generation forecast (MWh)\n",
    "generation_forecast = merge_data(client.query_generation_forecast)\n",
    "generation_forecast = generation_forecast.rename(columns={generation_forecast.columns[0]: 'E_generation_forecast_MWh'})\n",
    "save_df_with_timestamp(generation_forecast, f'{out_dir}/generation_forecast.csv')\n",
    "print('Generation forecast done')\n",
    "\n",
    "# Wind and solar forecasts (MWh)\n",
    "intraday_wind_solar_forecast = merge_data(client.query_intraday_wind_and_solar_forecast)\n",
    "for col in intraday_wind_solar_forecast.columns:\n",
    "    if 'Wind' in col:\n",
    "        intraday_wind_solar_forecast = intraday_wind_solar_forecast.rename(columns={col: 'E_wind_forecast_MWh'})\n",
    "    elif 'Solar' in col:\n",
    "        intraday_wind_solar_forecast = intraday_wind_solar_forecast.rename(columns={col: 'E_solar_forecast_MWh'})\n",
    "save_df_with_timestamp(intraday_wind_solar_forecast, f'{out_dir}/intraday_wind_solar_forecast.csv')\n",
    "print('Intraday wind and solar forecast done')\n",
    "\n",
    "# Day ahead wind and solar forecast (MWh)\n",
    "day_ahead_wind_solar_forecast = merge_data(client.query_wind_and_solar_forecast)\n",
    "for col in day_ahead_wind_solar_forecast.columns:\n",
    "    if 'Wind' in col:\n",
    "        day_ahead_wind_solar_forecast = day_ahead_wind_solar_forecast.rename(columns={col: 'E_wind_forecast_MWh'})\n",
    "    elif 'Solar' in col:\n",
    "        day_ahead_wind_solar_forecast = day_ahead_wind_solar_forecast.rename(columns={col: 'E_solar_forecast_MWh'})\n",
    "save_df_with_timestamp(day_ahead_wind_solar_forecast, f'{out_dir}/day_ahead_wind_solar_forecast.csv')\n",
    "print('Day ahead wind and solar forecast done')\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "##News Embeddings\n",
    "NEWS_API_KEY=\"9b546c7456c147f5b45e9cfb00b0b445\"\n",
    "GUARDIAN_API_KEY=\"39250d58-e880-4584-b0b5-c7b2f1fe0317\"\n",
    "\n",
    "\n",
    "# First get the datetime object\n",
    "end_date_dt = datetime.now()\n",
    "# Calculate start date using datetime object\n",
    "start_date_dt = end_date_dt - timedelta(days=30)\n",
    "\n",
    "# Convert to strings only when needed\n",
    "end_date = end_date_dt.strftime(\"%Y-%m-%d\")\n",
    "start_date = start_date_dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "save_path = '../final-submission/merged_data/data_collection'\n",
    "\n",
    "guardian_pipeline = GuardianEmbeddingPipeline(GUARDIAN_API_KEY)\n",
    "guardian_df = guardian_pipeline.collect_and_save_historical_data(\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    save_path=save_path,\n",
    "    query=\"energy OR electricity OR renewable OR weather\"\n",
    ")\n",
    "\n",
    "news_pipeline = NewsEmbeddingPipeline(NEWS_API_KEY)\n",
    "news_df = news_pipeline.collect_and_save_historical_data(\n",
    "    days_back=30,\n",
    "    save_path=save_path,\n",
    "    aggregate=True,\n",
    "    aggregation_method='mean'\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import requests\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "\n",
    "def download_and_unzip_github_file(local_path, url):\n",
    "    # GitHub raw content URL for the zip file\n",
    "    # Convert the normal GitHub URL to raw content URL\n",
    "    raw_url = url\n",
    "    \n",
    "    print(\"Downloading zip file...\")\n",
    "    response = requests.get(raw_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Create local directory if it doesn't exist\n",
    "        local_path = local_path\n",
    "        os.makedirs(local_path, exist_ok=True)\n",
    "        \n",
    "        print(\"Download successful. Unzipping...\")\n",
    "        # Create a BytesIO object from the downloaded content\n",
    "        zip_file = BytesIO(response.content)\n",
    "        \n",
    "        # Extract the contents\n",
    "        with ZipFile(zip_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(local_path)\n",
    "            \n",
    "        print(f\"Successfully extracted to {local_path}\")\n",
    "    else:\n",
    "        print(\"Failed to download file. Please check the URL.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_and_unzip_github_file(\"models/models/guardian_embeddings\", \"https://raw.githubusercontent.com/BTW25-Data-Science-Challenge/final-submission/refs/heads/main/models/models/guardian_embedding_model.zip\")\n",
    "    download_and_unzip_github_file(\"models/models/newsapi_embeddings\", \"https://raw.githubusercontent.com/BTW25-Data-Science-Challenge/final-submission/refs/heads/main/models/models/newsapi_embedding_model.zip\")\n",
    "    download_and_unzip_github_file(\"models/models/no_embeddings\", \"https://raw.githubusercontent.com/BTW25-Data-Science-Challenge/final-submission/refs/heads/main/models/models/no_embedding_model.zip)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime as dt\n",
    "from functools import partial\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import datetime\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "import holidays\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from entsoe import EntsoePandasClient, EntsoeRawClient\n",
    "from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_exponential\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "#from newsapi import NewsApiClient\n",
    "#from sentence_transformers import SentenceTransformer\n",
    "\n",
    "##Stockmarket\n",
    "url_oil = 'https://www.finanzen.net/rohstoffe/oelpreis'\n",
    "url_gas = 'https://www.finanzen.net/rohstoffe/erdgas-preis-natural-gas'\n",
    "url_coal = 'https://www.finanzen.net/rohstoffe/kohlepreis'\n",
    "url_uran = 'https://www.finanzen.net/rohstoffe/uranpreis'\n",
    "\n",
    "\n",
    "##Carbon\n",
    "url_carbon = \"https://www.investing.com/commodities/carbon-emissions-historical-data\"\n",
    "filepath_historic_carbon = '../final-submission/merged_data/data_collection/carbon_price_historic.csv'\n",
    "\n",
    "##Covid Lockdown Data\n",
    "\n",
    "FILE_URL = 'https://pada.psycharchives.org/bitstream/9ff033a9-4084-4d0e-87eb-aa963a1324a5'\n",
    "covid_df = pd.read_csv(FILE_URL, sep=\",\", header=[0])\n",
    "print(covid_df.head().iloc[:,:5])\n",
    "\n",
    "# dict with influence of measure (see readme)\n",
    "measure_influence = {\n",
    "    'leavehome': 1,\n",
    "    'dist': 0,\n",
    "    'msk': 1,\n",
    "    'shppng': 2,\n",
    "    'hcut': 2,\n",
    "    'ess_shps': 2,\n",
    "    'zoo': 0,\n",
    "    'demo': 0,\n",
    "    'school': 1,\n",
    "    'church': 0,\n",
    "    'onefriend': 0,\n",
    "    'morefriends': 0,\n",
    "    'plygrnd': 0,\n",
    "    'daycare': 2,\n",
    "    'trvl': 1,\n",
    "    'gastr': 2\n",
    "}\n",
    "# dict with state relative population of country\n",
    "state_percentages = {\n",
    "    'Baden-Wuerttemberg': 0.133924061,\n",
    "    'Bayern': 0.158676851,\n",
    "    'Berlin': 0.044670274,\n",
    "    'Brandenburg': 0.030491172,\n",
    "    'Bremen': 0.008169464,\n",
    "    'Hamburg': 0.022560236,\n",
    "    'Hessen': 0.075833,\n",
    "    'Mecklenburg-Vorpommern': 0.019245033,\n",
    "    'Niedersachsen': 0.096398323,\n",
    "    'Nordrhein-Westfalen': 0.214840756,\n",
    "    'Rheinland-Pfalz': 0.049301337,\n",
    "    'Saarland': 0.011744796,\n",
    "    'Sachsen': 0.048299274,\n",
    "    'Sachsen-Anhalt': 0.025752514,\n",
    "    'Schleswig-Holstein': 0.035026746,\n",
    "    'Thueringen': 0.025066162\n",
    "}\n",
    "\n",
    "## Smard\n",
    "\n",
    "#-------------translation for Balancing:------------------\n",
    "balancing_id={\n",
    "    #automatic frequency, tag=af\n",
    "    \"automatic_frequency\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Volume activated (+) [MWh] Calculated resolutions\":\"af_E_Volume_Activated_Plus_MWh\",\n",
    "        \"Volume activated (-) [MWh] Calculated resolutions\":\"af_E_Volume_Activated_Minus_MWh\",\n",
    "        \"Activation price (+) [€/MWh] Calculated resolutions\":\"af_Activation_Price_Plus_EUR_MWh\",\n",
    "        \"Activation price (-) [€/MWh] Calculated resolutions\":\"af_Activation_Price_Minus_EUR_MWh\",\n",
    "        \"Volume procured (+) [MW] Calculated resolutions\":\"af_E_Volume_Procured_Plus_MW\",\n",
    "        \"Volume procured (-) [MW] Calculated resolutions\":\"af_E_Volume_Procured_Minus_MW\",\n",
    "        \"Procurement price (+) [€/MW] Calculated resolutions\":\"af_Procurement_Price_Plus_EUR_MW\",\n",
    "        \"Procurement price (-) [€/MW] Calculated resolutions\":\"af_Procurement_Price_Minus_EUR_MW\",\n",
    "    },\n",
    "    #tag=mf\n",
    "    \"manual_frequency\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Volume activated (+) [MWh] Calculated resolutions\":\"mf_E_Volume_Activated_Plus_MWh\",\n",
    "        \"Volume activated (-) [MWh] Calculated resolutions\":\"mf_E_Volume_Activated_Minus_MWh\",\n",
    "        \"Activation price (+) [€/MWh] Calculated resolutions\":\"mf_Activation_Price_Plus_EUR_MWh\",\n",
    "        \"Activation price (-) [€/MWh] Calculated resolutions\":\"mf_Activation_Price_Minus_EUR_MWh\",\n",
    "        \"Volume procured (+) [MW] Calculated resolutions\":\"mf_E_Volume_Procured_Plus_MW\",\n",
    "        \"Volume procured (-) [MW] Calculated resolutions\":\"mf_E_Volume_Procured_Minus_MW\",\n",
    "        \"Procurement price (+) [€/MW] Calculated resolutions\":\"mf_Procurement_Price_Plus_EUR_MW\",\n",
    "        \"Procurement price (-) [€/MW] Calculated resolutions\":\"mf_Procurement_Price_Minus_EUR_MW\",\n",
    "    },\n",
    "     #balancing energy\n",
    "    \"balancing_energy\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Volume (+) [MWh] Calculated resolutions\":\"E_Volume_Calculated_Plus_MWh\",\n",
    "        \"Volume (-) [MWh] Calculated resolutions\":\"E_Volume_Calculated_Minus_MWh\",\n",
    "        \"Price [€/MWh] Calculated resolutions\":\"Price_Calculated_EUR_MWh\",\n",
    "        \"Net income [€] Calculated resolutions\":\"Net_Income_EUR\",\n",
    "    },\n",
    "    #costs\n",
    "    \"costs\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Balancing services [€] Calculated resolutions\":\"Balancing_Services_Calculated_EUR\",\n",
    "        \"Network security [€] Calculated resolutions\":\"Network_Security_Calculated_EUR\",\n",
    "        \"Countertrading [€] Calculated resolutions\":\"Countertrading_Calculated_EUR\",\n",
    "    },\n",
    "    #frequency_containment_reserve\n",
    "    \"frequency_containment\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Volume procured [MW] Calculated resolutions\":\"E_Volume_Procured_Calculated_MW\",\n",
    "        \"Procurement price [€/MW] Calculated resolutions\":\"Price_Procument_Calculated_EUR/MW\"\n",
    "    },\n",
    "    \"imported_balancing_services\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Austria [MWh] Calculated resolutions\":\"import_E_Austria_Calculated_MWh\",\n",
    "    },\n",
    "    \"exported_balancing_services\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Austria [MWh] Calculated resolutions\":\"export_E_Austria_Calculated_MWh\",\n",
    "    }         \n",
    "}    \n",
    "\n",
    "#actual consumption tag=actual\n",
    "electricity_consumption_id={\n",
    "    \"actual\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Total (grid load) [MWh] Calculated resolutions\":\"actual_E_Total_Gridload_MWh\",\n",
    "        \"Residual load [MWh] Calculated resolutions\":\"actual_E_Residual_Load_MWh\",\n",
    "        \"Hydro pumped storage [MWh] Calculated resolutions\":\"actual_E_Hydro_Pumped_Storage_MWh\",\n",
    "    },\n",
    "    #forecasted consumption tag=forecast\n",
    "    \"forecast\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Total (grid load) [MWh] Calculated resolutions\":\"forecast_E_Total_Gridload_MWh\",\n",
    "        \"Residual load [MWh] Calculated resolutions\":\"forecast_actual_E_Residual_Load_MWh\"\n",
    "    }\n",
    "}\n",
    "\n",
    "electricity_generation_id={\n",
    "    #actual generation\n",
    "    \"actual\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Biomass [MWh] Calculated resolutions\":\"actual_generation_E_Biomass_MWh\",\n",
    "        \"Hydropower [MWh] Calculated resolutions\":\"actual_generation_E_Hydropower_MWh\",\n",
    "        \"Wind offshore [MWh] Calculated resolutions\":\"actual_generation_E_Windoffshore_MWh\",\n",
    "        \"Wind onshore [MWh] Calculated resolutions\":\"actual_generation_E_Windonshore_MWh\",\n",
    "        \"Photovoltaics [MWh] Calculated resolutions\":\"actual_generation_E_Photovoltaics_MWh\",\n",
    "        \"Other renewable [MWh] Calculated resolutions\":\"actual_generation_E_OtherRenewable_MWh\",\n",
    "        \"Nuclear [MWh] Calculated resolutions\":\"actual_generation_E_Nuclear_MWh\",\n",
    "        \"Lignite [MWh] Calculated resolutions\":\"actual_generation_E_Lignite_MWh\",\n",
    "        \"Hard coal [MWh] Calculated resolutions\":\"actual_generation_E_HardCoal_MWh\",\n",
    "        \"Fossil gas [MWh] Calculated resolutions\":\"actual_generation_E_FossilGas_MWh\",\n",
    "        \"Hydro pumped storage [MWh] Calculated resolutions\":\"actual_generation_E_HydroPumpedStorage_MWh\",\n",
    "        \"Other conventional [MWh] Calculated resolutions\":\"actual_generation_E_OtherConventional_MWh\"\n",
    "    },\n",
    "    \n",
    "    #forecastet generation day ahead\n",
    "    \"forecast\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Biomass [MWh] Calculated resolutions\":\"forecast_generation_E_Biomass_MWh\",\n",
    "        \"Hydropower [MWh] Calculated resolutions\":\"forecast_generation_E_Hydropower_MWh\",\n",
    "        \"Wind offshore [MWh] Calculated resolutions\":\"forecast_generation_E_Windoffshore_MWh\",\n",
    "        \"Wind onshore [MWh] Calculated resolutions\":\"forecast_generation_E_Windonshore_MWh\",\n",
    "        \"Photovoltaics [MWh] Calculated resolutions\":\"forecast_generation_E_Photovoltaics_MWh\",\n",
    "        \"Other renewable [MWh] Calculated resolutions\":\"forecast_generation_E_OtherRenewable_MWh\",\n",
    "        \"Nuclear [MWh] Calculated resolutions\":\"forecast_generation_E_Nuclear_MWh\",\n",
    "        \"Lignite [MWh] Calculated resolutions\":\"forecast_generation_E_Lignite_MWh\",\n",
    "        \"Hard coal [MWh] Calculated resolutions\":\"forecast_generation_E_HardCoal_MWh\",\n",
    "        \"Fossil gas [MWh] Calculated resolutions\":\"forecast_generation_E_FossilGas_MWh\",\n",
    "        \"Hydro pumped storage [MWh] Calculated resolutions\":\"forecast_generation_E_HydroPumpedStorage_MWh\",\n",
    "        \"Other [MWh] Calculated resolutions\":\"forecast_generation_E_Other_MWh\",\n",
    "        \"Total [MWh] Original resolutions\":\"forecast_generation_E_Total_MWh\",\n",
    "        \"Photovoltaics and wind [MWh] Calculated resolutions\":\"forecast_generation_E_PhotovoltaicsAndWind_MWh\",\n",
    "        \"Other [MWh] Original resolutions\":\"forecast_generation_E_Original_MWh\"\n",
    "    },\n",
    "\n",
    "    #installed generation capacity\n",
    "    #key=instGenCapacity\n",
    "    \"installed_generation_capacity\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Biomass [MW] Calculated resolutions\":\"instGenCapacity_E_Biomass_MW\",\n",
    "        \"Hydropower [MW] Calculated resolutions\":\"instGenCapacity_E_Hydropower_MW\",\n",
    "        \"Wind offshore [MW] Calculated resolutions\":\"instGenCapacity_E_Windoffshore_MW\",\n",
    "        \"Wind onshore [MW] Calculated resolutions\":\"instGenCapacity_E_Windonshore_MW\",\n",
    "        \"Photovoltaics [MW] Calculated resolutions\":\"instGenCapacity_E_Photovoltaics_MW\",\n",
    "        \"Other renewable [MW] Calculated resolutions\":\"instGenCapacity_E_OtherRenewable_MW\",\n",
    "        \"Nuclear [MW] Calculated resolutions\":\"instGenCapacity_E_Nuclear_MW\",\n",
    "        \"Lignite [MW] Calculated resolutions\":\"instGenCapacity_E_Lignite_MW\",\n",
    "        \"Hard coal [MW] Calculated resolutions\":\"instGenCapacity_E_HardCoal_MW\",\n",
    "        \"Fossil gas [MW] Calculated resolutions\":\"instGenCapacity_E_FossilGas_MW\",\n",
    "        \"Hydro pumped storage [MW] Calculated resolutions\":\"instGenCapacity_E_HydroPumpedStorage_MW\",\n",
    "        \"Other conventional [MW] Calculated resolutions\":\"instGenCapacity_E_OtherConventional_MW\"\n",
    "    }\n",
    "}\n",
    "\n",
    "market_id={\n",
    "    #key=dayAhead\n",
    "    \"day_ahead_prices\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Germany/Luxembourg [€/MWh] Original resolutions\":\"dayAhead_Price_GermanyAndLuxembourg_EUR_MWh\",\n",
    "        \"∅ DE/LU neighbours [€/MWh] Original resolutions\":\"dayAhead_Price_GermanyAndLuxembourgAverage_EUR_MWh\",\n",
    "        \"Belgium [€/MWh] Original resolutions\":\"dayAhead_Price_Belgium_EUR_MWh\",\n",
    "        \"Denmark 1 [€/MWh] Original resolutions\":\"dayAhead_Price_Denmark1_EUR_MWh\",\n",
    "        \"Denmark 2 [€/MWh] Original resolutions\":\"dayAhead_Price_Denmark2_EUR_MWh\",\n",
    "        \"France [€/MWh] Original resolutions\":\"dayAhead_Price_France_EUR_MWh\",\n",
    "        \"Netherlands [€/MWh] Original resolutions\":\"dayAhead_Price_Netherlands_EUR_MWh\",\n",
    "        \"Norway 2 [€/MWh] Original resolutions\":\"dayAhead_Price_Norway2_EUR_MWh\",\n",
    "        \"Austria [€/MWh] Original resolutions\":\"dayAhead_Price_Austria_EUR_MWh\",\n",
    "        \"Poland [€/MWh] Original resolutions\":\"dayAhead_Price_Poland_EUR_MWh\",\n",
    "        \"Sweden 4 [€/MWh] Original resolutions\":\"dayAhead_Price_Sweden4_EUR_MWh\",\n",
    "        \"Switzerland [€/MWh] Original resolutions\":\"dayAhead_Price_Switzerland_EUR_MWh\",\n",
    "        \"Czech Republic [€/MWh] Original resolutions\":\"dayAhead_Price_CzechRepublic_EUR_MWh\",\n",
    "        \"DE/AT/LU [€/MWh] Original resolutions\":\"dayAhead_Price_DE/AT/LU_EUR_MWh\",\n",
    "        \"Northern Italy [€/MWh] Original resolutions\":\"dayAhead_Price_NothernItaly_EUR_MWh\",\n",
    "        \"Slovenia [€/MWh] Original resolutions\":\"dayAhead_Price_Slovenia_EUR_MWh\",\n",
    "        \"Hungary [€/MWh] Original resolutions\":\"dayAhead_Price_Hungary_EUR_MWh\"\n",
    "    },\n",
    "    \n",
    "    \"cross_border_physical\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Net export [MWh] Calculated resolutions\":\"E_NetExport_crossBorderPhysical_MWh\",\n",
    "        \"Netherlands (export) [MWh] Calculated resolutions\":\"E_NetherlandExport_corssBorderPhysical_MWh\",\n",
    "        \"Netherlands (import) [MWh] Calculated resolutions\":\"E_NetherlandImport_corssBorderPhysical_MW\",\n",
    "        \"Switzerland (export) [MWh] Calculated resolutions\":\"E_SwitzerlandExport_corssBorderPhysical_MWh\",\n",
    "        \"Switzerland (import) [MWh] Calculated resolutions\":\"E_SwitzerlandImport_corssBorderPhysical_MWh\",\n",
    "        \"Denmark (export) [MWh] Calculated resolutions\":\"E_DenmarkExport_corssBorderPhysical_MWh\",\n",
    "        \"Denmark (import) [MWh] Calculated resolutions\":\"E_Denmark_Import_corssBorderPhysical_MWh\",\n",
    "        \"Czech Republic (export) [MWh] Calculated resolutions\":\"E_CzechrepublicExport_corssBorderPhysical_MWh\",\n",
    "        \"Czech Republic (import) [MWh] Calculated resolutions\":\"E_CzechrepublicImport_corssBorderPhysical_MWh\",\n",
    "        \"Luxembourg (export) [MWh] Calculated resolutions\":\"E_LuxembourgExport_corssBorderPhysical_MWh\",\n",
    "        \"Luxembourg (import) [MWh] Calculated resolutions\":\"E_LuxembourgImport_corssBorderPhysical_MWh\",\n",
    "        \"Sweden (export) [MWh] Calculated resolutions\":\"E_SwedenExport_corssBorderPhysical_MWh\",\n",
    "        \"Sweden (import) [MWh] Calculated resolutions\":\"E_SwedenImportv_corssBorderPhysical_MWh\",\n",
    "        \"Austria (export) [MWh] Calculated resolutions\":\"E_AustriaExport_corssBorderPhysical_MWh\",\n",
    "        \"Austria (import) [MWh] Calculated resolutions\":\"E_AustriaImport_corssBorderPhysical_MWh\",\n",
    "        \"France (export) [MWh] Calculated resolutions\":\"E_FranceExport_corssBorderPhysical_MWh\",        \n",
    "        \"France (import) [MWh] Calculated resolutions\":\"E_FranceImport_corssBorderPhysical_MWh\",\n",
    "        \"Poland (export) [MWh] Calculated resolutions\":\"E_PolandExport_corssBorderPhysical_MWh\",\n",
    "        \"Poland (import) [MWh] Calculated resolutions\":\"E_PolandImport_corssBorderPhysical_MWh\",\n",
    "        \"Norway (export) [MWh] Calculated resolutions\":\"E_NorwayExport_corssBorderPhysical_MWh\",\n",
    "        \"Norway (import) [MWh] Calculated resolutions\":\"E_NorwayImport_corssBorderPhysical_MWh\",\n",
    "        \"Belgium (export) [MWh] Calculated resolutions\":\"E_BelgiumExport_corssBorderPhysical_MWh\",\n",
    "        \"Belgium (import) [MWh] Calculated resolutions\":\"E_BelgiumImport_corssBorderPhysical_MWh\",\n",
    "    },\n",
    "    \"scheudled_commercial_exchanges\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Net export [MWh] Calculated resolutions\":\"E_NetExport_MWh\",\n",
    "        \"Netherlands (export) [MWh] Calculated resolutions\":\"E_NetherlandExport_MWh\",\n",
    "        \"Netherlands (import) [MWh] Calculated resolutions\":\"E_NetherlandImport_MW\",\n",
    "        \"Switzerland (export) [MWh] Calculated resolutions\":\"E_SwitzerlandExport_MWh\",\n",
    "        \"Switzerland (import) [MWh] Calculated resolutions\":\"E_SwitzerlandImport_MWh\",\n",
    "        \"Denmark (export) [MWh] Calculated resolutions\":\"E_DenmarkExport_MWh\",\n",
    "        \"Denmark (import) [MWh] Calculated resolutions\":\"E_Denmark_Import_MWh\",\n",
    "        \"Czech Republic (export) [MWh] Calculated resolutions\":\"E_CzechrepublicExport_MWh\",\n",
    "        \"Czech Republic (import) [MWh] Calculated resolutions\":\"E_CzechrepublicImport_MWh\",\n",
    "        \"Luxembourg (export) [MWh] Calculated resolutions\":\"E_LuxembourgExport_MWh\",\n",
    "        \"Luxembourg (import) [MWh] Calculated resolutions\":\"E_LuxembourgImport_MWh\",\n",
    "        \"Sweden (export) [MWh] Calculated resolutions\":\"E_SwedenExport_MWh\",\n",
    "        \"Sweden (import) [MWh] Calculated resolutions\":\"E_SwedenImport_MWh\",\n",
    "        \"Austria (export) [MWh] Calculated resolutions\":\"E_AustriaExport_MWh\",\n",
    "        \"Austria (import) [MWh] Calculated resolutions\":\"E_AustriaImport_MWh\",\n",
    "        \"France (export) [MWh] Calculated resolutions\":\"E_FranceExport_MWh\",        \n",
    "        \"France (import) [MWh] Calculated resolutions\":\"E_FranceImport_MWh\",\n",
    "        \"Poland (export) [MWh] Calculated resolutions\":\"E_PolandExport_MWh\",\n",
    "        \"Poland (import) [MWh] Calculated resolutions\":\"E_PolandImport_MWh\",\n",
    "        \"Norway (export) [MWh] Calculated resolutions\":\"E_NorwayExport_MWh\",\n",
    "        \"Norway (import) [MWh] Calculated resolutions\":\"E_NorwayImport_MWh\",\n",
    "        \"Belgium (export) [MWh] Calculated resolutions\":\"E_BelgiumExport_MWh\",\n",
    "        \"Belgium (import) [MWh] Calculated resolutions\":\"E_BelgiumImport_MWh\",\n",
    "    }\n",
    "}\n",
    "\n",
    "##weather\n",
    "#Define stations\n",
    "combine_historicforecast_bool =False\n",
    "station_ids_r = [ \"01262\", \"01975\", \"02667\"]\n",
    "station_ids_f = [ \"10870\", \"10147\", \"10513\"]\n",
    "station_place = [ \"Muenchen\", \"Hamburg\", \"KoelnBonn\" ]\n",
    "#folderstructure\n",
    "output_folder = \"./merged_data/scripts/weather/\"\n",
    "station_folder = \"./merged_data/scripts/weather/stations\"\n",
    "computing_folder = \"./merged_data/scripts/weather/computing_folder\"\n",
    "stations_combined = \"./merged_data/scripts/weather/stations_combined\"\n",
    "data_collection_folder=\"../final-submission/merged_data/data_collection\"\n",
    "forecas_folder=\"../final-submission/merged_data/forecast\"\n",
    "#Basis-URL for dwd-data\n",
    "base_url_review = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/\"\n",
    "url_forecast = \"https://dwd.api.proxy.bund.dev/v30/stationOverviewExtended\"\n",
    "#collums to remove   \n",
    "columns_remove_clouds = [\"STATIONS_ID\",\"eor\", \"QN_8\",\"V_N_I\"]\n",
    "columns_remove_pressure = [\"STATIONS_ID\",\"eor\", \"QN_8\"]\n",
    "columns_remove_sun = [\"STATIONS_ID\",\"eor\", \"QN_7\"]\n",
    "columns_remove_temp = [\"STATIONS_ID\",\"QN_9\", \"eor\"]\n",
    "columns_remove_wind = [\"STATIONS_ID\",\"eor\", \"QN_3\"]\n",
    "columns_remove_precipitation = [\"STATIONS_ID\",\"eor\", \"QN_8\", \"WRTR\", \"RS_IND\"]\n",
    "\n",
    "columns_remove_forecast = ['isDay','dewPoint2m']\n",
    "#URL-endings for historical data\n",
    "data_types = {\n",
    "    \"temperature_historical\": \"air_temperature/historical/\",\n",
    "    \"temperature_recent\": \"air_temperature/recent/\",\n",
    "    \"cloudiness_historical\": \"cloudiness/historical/\",\n",
    "    \"cloudiness_recent\": \"cloudiness/recent/\",\n",
    "    \"pressure_historical\": \"pressure/historical/\",\n",
    "    \"pressure_recent\": \"pressure/recent/\",\n",
    "    \"sun_historical\": \"sun/historical/\",\n",
    "    \"sun_recent\": \"sun/recent/\",\n",
    "    \"wind_historical\": \"wind/historical/\",\n",
    "    \"wind_recent\": \"wind/recent/\",\n",
    "    \"precipitation_recent\": \"precipitation/recent/\",\n",
    "    \"precipitation_historical\": \"precipitation/historical/\",\n",
    "}\n",
    "#header for API\n",
    "headers_weather = {\n",
    "    \"accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "##Stockmarket\n",
    "##Stockmarket\n",
    "\n",
    "def directory_exists(filepath):\n",
    "    directory = os.path.dirname(filepath)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# gets data from finanzen.net with the given url, the filename and resource have to be put in, it updates an already existing file, to not use selenium\n",
    "def get_Data(url, filename, resource, before):\n",
    "\n",
    "    #ellaborate header needed, otherwise finanzen.net will give an access denied error\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Referer': 'https://www.finanzen.net',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    }\n",
    "\n",
    "    session = requests.Session()\n",
    "    response = session.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    #used website inspection to find the right table from the website\n",
    "    table = soup.find('table', class_='table table--content-right')\n",
    "\n",
    "    if table:\n",
    "        headers = [th.get_text(strip=True) for th in table.find_all('th')]\n",
    "\n",
    "        #we only need schluss and date, the other columns are irrelevant\n",
    "        datum_index = headers.index('Datum')\n",
    "        schlusskurs_index = headers.index('Schlusskurs')\n",
    "        rows = table.find_all('tr')[1:] \n",
    "        extracted_data = []\n",
    "\n",
    "        for row in rows:\n",
    "            columns = row.find_all('td')\n",
    "            if len(columns) > max(datum_index, schlusskurs_index): \n",
    "                datum = columns[datum_index].get_text(strip=True)\n",
    "                schlusskurs = columns[schlusskurs_index].get_text(strip=True)\n",
    "                schlusskurs = schlusskurs.replace(',', '.')\n",
    "                extracted_data.append({'Date': datum, resource: schlusskurs})\n",
    "\n",
    "        df = pd.DataFrame(extracted_data)\n",
    "\n",
    "    else:\n",
    "        print(\"Table not found\")\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%d.%m.%Y')\n",
    "\n",
    "    old = pd.read_csv(before)\n",
    "    old['Date'] = pd.to_datetime(old['Date'], format='%Y-%m-%d')  \n",
    "\n",
    "    df_filtered = df[~df['Date'].isin(old['Date'])]\n",
    "\n",
    "    if not df_filtered.empty:\n",
    "        old = pd.concat([old, df_filtered], ignore_index=True)\n",
    "\n",
    "    old['Date'] = pd.to_datetime(old['Date'], format='%Y-%m-%d')\n",
    "\n",
    "    # data is in the wrong order, reverses it\n",
    "    old = old.sort_values(by='Date')\n",
    "\n",
    "    # Save the updated and sorted data to a new CSV file\n",
    "    old.to_csv(filename, index=False)\n",
    "    old.to_csv(before, index=False)\n",
    "\n",
    "    print(\"Data saved as\", filename)\n",
    "\n",
    "#the data is missing hour, as it is only daily, fills weekend gaps also\n",
    "def fill_missing_hours(csv):\n",
    "    df = pd.read_csv(csv)\n",
    "\n",
    "    value_Name = df.columns[1]\n",
    "\n",
    "    # Manually parse the 'date' column using the correct format (DD.MM.YY)\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "    df['Date'] = df['Date'].dt.normalize()\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # start 2015\n",
    "    full_hourly_range = pd.date_range(start='01.01.2015', end=df.index.max() + pd.Timedelta(days=1), freq='h')[:-1]\n",
    "\n",
    "    # put prefered null value here\n",
    "    df_full = df.reindex(full_hourly_range, fill_value=pd.NA)\n",
    "    df_full.reset_index(inplace=True)\n",
    "    df_full.rename(columns={'index': 'Date'}, inplace=True)\n",
    "    df_full[value_Name] = df_full.groupby(df_full['Date'].dt.floor('D'))[value_Name].transform(lambda group: group.ffill().bfill())\n",
    "\n",
    "    # fills emptys\n",
    "    df_full.fillna({value_Name:np.nan}, inplace=True)\n",
    "    df_full.to_csv(csv, index=False)\n",
    "    print('Missing Hours Filled: ', csv)\n",
    "\n",
    "def get_carbon_data(url, filepath):\n",
    "    \n",
    "    # Scrape today's carbon data from website\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        table = soup.find(\"table\")\n",
    "        if not table:\n",
    "            print(\"No table found on the webpage.\")\n",
    "            return\n",
    "\n",
    "        rows = table.find_all(\"tr\")[1:2]  # Extract first data row\n",
    "        data = [[cell.text.strip() for cell in row.find_all(\"td\")[:2]] for row in rows]\n",
    "\n",
    "        todays_carbon_data = pd.DataFrame(data, columns=['Date', 'carbon_price_EURO'])\n",
    "        todays_carbon_data['Date'] = pd.to_datetime(todays_carbon_data['Date']).dt.date\n",
    "\n",
    "        # Expand each date into hourly rows\n",
    "        todays_carbon_data_expanded = pd.concat(\n",
    "            [pd.DataFrame({'Date': pd.date_range(date, periods=24, freq='H'), 'carbon_price_EURO': price})\n",
    "             for date, price in zip(todays_carbon_data['Date'], todays_carbon_data['carbon_price_EURO'])]\n",
    "        )\n",
    "\n",
    "        # print(expanded_df)\n",
    "        return todays_carbon_data_expanded\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scraping todays carbon data: {e}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Load and clean historic carbon data\n",
    "        historic_carbon_data = pd.read_csv(filepath, parse_dates=[\"Date\"]).drop(\n",
    "            columns=['Open', 'High', 'Low', 'Vol.', 'Change %']\n",
    "        ).rename(columns={'Price': 'carbon_price_EURO'})\n",
    "\n",
    "        # Expand with missing dates and hourly data\n",
    "        full_date_range = pd.date_range(historic_carbon_data['Date'].min(), datetime.today(), freq='D')\n",
    "        historic_carbon_data = historic_carbon_data.set_index('Date').reindex(full_date_range).resample('H').ffill().reset_index()\n",
    "        historic_carbon_data.columns = ['Date', 'carbon_price_EURO']\n",
    "\n",
    "        # Final adjustments\n",
    "        historic_carbon_data['carbon_price_EURO'] = pd.to_numeric(historic_carbon_data['carbon_price_EURO'], errors='coerce').ffill()\n",
    "        historic_carbon_data = historic_carbon_data[historic_carbon_data['Date'] >= '2015-01-01']\n",
    "        # drop last row\n",
    "        historic_carbon_data = historic_carbon_data.iloc[:-1]\n",
    "\n",
    "        #print(historic_carbon_data.head())\n",
    "        #print(historic_carbon_data.tail())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error while loading historic carbon data: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Append today's data to the historic data\n",
    "        carbon_data = pd.concat([historic_carbon_data, todays_carbon_data_expanded])\n",
    "        \n",
    "        print(carbon_data.head())\n",
    "        print(carbon_data.tail())\n",
    "\n",
    "        return carbon_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while concatenating historic and todays carbon data: {e}\")\n",
    "\n",
    "\n",
    "##Entsoe\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10), retry=retry_if_exception_type(Exception))\n",
    "def query_entsoe_data(query_func, country_code, start, end):\n",
    "    try:\n",
    "        df = query_func(country_code, start=start, end=end)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying data: {e}\")\n",
    "        raise\n",
    "    return df\n",
    "\n",
    "def merge_data(query_func):\n",
    "    data_old = query_entsoe_data(query_func, country_code_old, start, change_date)\n",
    "    \n",
    "    data_new = query_entsoe_data(query_func, country_code_new, change_date, end)\n",
    "\n",
    "    if not isinstance(data_old, pd.DataFrame):\n",
    "        data_old = data_old.to_frame()\n",
    "    if not isinstance(data_new, pd.DataFrame):\n",
    "        data_new = data_new.to_frame()\n",
    "    \n",
    "    if not data_old.empty and not data_new.empty:\n",
    "        if len(data_old.columns) != len(data_new.columns):\n",
    "            same_columns = list(set(data_old.columns) & set(data_new.columns))\n",
    "            data_old = data_old[same_columns]\n",
    "            data_new = data_new[same_columns]\n",
    "        else:\n",
    "            data_new.columns = data_old.columns\n",
    "    df_combined = pd.concat([data_old, data_new])\n",
    "    df_combined.index = df_combined.index.tz_convert('UTC')\n",
    "    return df_combined\n",
    "\n",
    "def save_df_with_timestamp(df, filename):\n",
    "    df_copy = df.copy()\n",
    "    df_copy.index.name = 'timestamp'\n",
    "    df_copy.to_csv(filename)\n",
    "\n",
    "\n",
    "##Covid Lockdown Data\n",
    "def evaluate_date(request_date):\n",
    "    if request_date in list(covid_df):\n",
    "        truncated_covid_df = covid_df[['state', 'Measure ', request_date]]\n",
    "        sum_value = 0\n",
    "        for index, row in truncated_covid_df.iterrows():\n",
    "            if row.isnull().values.any(): continue  # if any value in row is missing\n",
    "            if measure_influence[row['Measure ']] == 0: continue  # if measure has no influence\n",
    "            sum_value += ((int(row[request_date]) / 5) + 0.6) * state_percentages[row['state']] * measure_influence[\n",
    "                row['Measure ']]  # see readme documentation\n",
    "        return sum_value\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "##smard\n",
    "def main():\n",
    "\n",
    "    output_path = sys.argv[1]\n",
    "\n",
    "    dict_ids = [balancing_id[\"automatic_frequency\"],\n",
    "                balancing_id[\"balancing_energy\"],\n",
    "                balancing_id[\"costs\"],\n",
    "                balancing_id[\"exported_balancing_services\"],\n",
    "                balancing_id[\"frequency_containment\"],\n",
    "                balancing_id[\"imported_balancing_services\"],\n",
    "                balancing_id[\"manual_frequency\"],\n",
    "                electricity_consumption_id[\"actual\"],\n",
    "                electricity_consumption_id[\"forecast\"],\n",
    "                electricity_generation_id[\"actual\"],\n",
    "                electricity_generation_id[\"forecast\"],\n",
    "                market_id[\"cross_border_physical\"],\n",
    "                market_id[\"scheudled_commercial_exchanges\"],\n",
    "                market_id[\"day_ahead_prices\"]    \n",
    "    ]\n",
    "    \n",
    "    final_df = None\n",
    "\n",
    "    for i in range(3):\n",
    "        working_df = download(i)\n",
    "        working_df = new_format(working_df, dict_ids[i])\n",
    "\n",
    "        #if i > 0:\n",
    "        working_df=working_df.drop(working_df.columns[1],axis=1)\n",
    "        #only called once\n",
    "        if final_df is None:\n",
    "            final_df = working_df\n",
    "        else:\n",
    "            final_df = pd.merge(final_df, working_df, on=working_df.columns[0], how='outer')\n",
    "            #final_df = pd.merge(final_df, working_df, on=working_df.columns[0], how='inner', copy=True)\n",
    "    \n",
    "    final_df=final_df[final_df.duplicated(keep=False) == False]\n",
    "\n",
    "    final_df.to_csv(output_path, sep=',', index=False)\n",
    "\n",
    "    #use gzip to compress .csv outputfile to <file_out>.gz\n",
    "    path_object = Path(output_path)\n",
    "    output_pathgz = path_object.with_suffix('.gz')\n",
    "    final_df.to_csv(output_pathgz, sep=',', index=False, compression='gzip')\n",
    "\n",
    "\n",
    "def download_and_merge_multiple_csv(module_ids):\n",
    "    steps = [\"1420066800000\",\"1600000000000\",str(int(datetime.today().timestamp()))+'000']\n",
    "    csvfiles = []\n",
    "    for timestamp_from, timestamp_to in zip(steps,steps[1:]):\n",
    "        response = requests.post('https://www.smard.de/nip-download-manager/nip/download/market-data',\n",
    "                                 data='{\"request_form\":[{\"format\":\"CSV\",\"moduleIds\":'+module_ids+',\"region\":\"DE\",\"timestamp_from\":'+timestamp_from+',\"timestamp_to\":'+timestamp_to+',\"type\":\"discrete\",\"language\":\"en\",\"resolution\":\"hour\"}]}')\n",
    "        csvfiles.append(response.content.decode('utf-8-sig'))\n",
    "    csvfile_data = csvfiles[0] + csvfiles[1][csvfiles[1].index('\\n'):]\n",
    "    return csvfile_data\n",
    "\n",
    "\n",
    "def download(download_id):\n",
    "    #14 different files\n",
    "    match download_id:\n",
    "        # AUTOMATIC FREQUENCY RESTORATION\n",
    "        case 0:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[18004368,18004369,18004370,18004351,18004371,18004372,18004373,18004374]')\n",
    "        # BALANCING ENERGY\n",
    "        case 1:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[15004383,15004384,15004382,15004390]')\n",
    "        # COSTS\n",
    "        case 2:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[16004391,16000419,16000418]')\n",
    "        # EXPORTED BALANCING SERVICES\n",
    "        case 3:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[20004385]')\n",
    "        #FREQUENCY CONTAINMENT RESERVE\n",
    "        case 4:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[17004363, 17004367]')\n",
    "        # IMPORTED BALANCING SERVICES\n",
    "        case 5:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[21004386]')\n",
    "        # MANUAL FREQUENCY RESTORATION RESERVE\n",
    "        case 6:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[19004377,19004375,19004376,19004352,19004378,19004379,19004380,19004381]')\n",
    "\n",
    "        #electricity consumption, actual\n",
    "        case 7:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[5000410,5004387,5004359]')\n",
    "        #forecast consumption\n",
    "        case 8:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[6000411,6004362]')\n",
    "        #electricity generation actual\n",
    "        case 9:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[1001224,1004066,1004067,1004068,1001223,1004069,1004071,1004070,1001226,1001228,1001227,1001225]')\n",
    "        #electricity generation forecast\n",
    "        case 10:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[2000122,2005097,2000715,2003791,2000123,2000125]')\n",
    "        #MARKET\n",
    "        # CROSSBORDER FLOWS\n",
    "        case 11:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[31004963,31004736,31004737,31004740,31004741,31004988,31004990,31004992,31004994,31004738,31004742,31004743,31004744,31004880,31004881,31004882,31004883,31004884,31004885,31004886,31004887,31004888,31004739]')\n",
    "        # CROSSBORDER SCHEDULED FLOWS\n",
    "        case 12:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[22004629,22004722,22004724,22004404,22004409,22004545,22004546,22004548,22004550,22004551,22004552,22004405,22004547,22004403,22004406,22004407,22004408,22004410,22004412,22004549,22004553,22004998,22004712]')\n",
    "        # DAYAHEAD\n",
    "        case 13:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[8004169,8004170,8000251,8005078,8000252,8000253,8000254,8000255,8000256,8000257,8000258,8000259,8000260,8000261,8000262,8004996,8004997]')\n",
    "\n",
    "    download_df = pd.read_csv(StringIO(csvfile_data), sep=\";\", header=[0], na_values='-', low_memory=False)\n",
    "    return download_df\n",
    "\n",
    "\n",
    "def new_format(df, my_dict):\n",
    "        \n",
    "    #use fitting dict to rename table head\n",
    "    df.rename(columns=my_dict, inplace=True)\n",
    "    \n",
    "    #change Datetime_format; replace '-' with np.nan\n",
    "    df['Start_Date'] = pd.to_datetime(df['Start_Date'])\n",
    "    df['End_Date'] = pd.to_datetime(df['End_Date'])\n",
    "    df.replace(\"-\",np.nan, inplace=True)\n",
    "\n",
    "    #remove , seperator for thousand\n",
    "    df.replace(\",\",\"\", inplace=True, regex=True)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def my_merge(fin_df, work_df, i):\n",
    "\n",
    "    #if i > 0:\n",
    "        #work_df=work_df.drop(work_df.columns[1],axis=1)\n",
    "    work_df=work_df.drop(work_df.columns['End_Date'],axis=1)\n",
    "    #fin_df = pd.merge(fin_df, work_df, on=work_df.columns[0], how='inner', copy=True)\n",
    "    fin_df = pd.merge(fin_df, work_df, on=work_df.columns[0], how='outer')\n",
    "\n",
    "\n",
    "##weather\n",
    "\n",
    "#Definitions of funktions for weather\n",
    "def combine_historic(station_r, place): \n",
    "  #combine data\n",
    "  try:\n",
    "    file_r = os.path.join(station_folder, station_r, f\"{station_r}_data_combined.csv\")\n",
    "    \n",
    "    #read data\n",
    "    df_r = pd.read_csv(file_r)\n",
    "    combined_df=df_r\n",
    "    output_file = os.path.join(stations_combined, f\"{place}_review.csv\")\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Comibe: {station_r} -> {output_file}\")\n",
    "\n",
    "  except FileNotFoundError as e:\n",
    "    print(f\"File not found: {e}\")\n",
    "  except Exception as e:\n",
    "    print(f\"Error while computing{station_r}: {e}\")\n",
    "def combine_all_stations():\n",
    "  files = [f for f in os.listdir(stations_combined) if f.endswith('.csv')]\n",
    "\n",
    "  #rename collums to station name\n",
    "  for file in files:\n",
    "    file_path = os.path.join(stations_combined, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    #extract filename\n",
    "    file_name = os.path.splitext(file)[0]\n",
    "    columname=[df.columns[0]] + [f'{col}_{file_name}' for col in df.columns[1:]]\n",
    "    df.columns = columname\n",
    "    print(f'Renamend collums for {file_name}')\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "  #combine dataframes  \n",
    "  all_data_frames = []\n",
    "  for file in files:\n",
    "    file_path = os.path.join(stations_combined, file)  \n",
    "    \n",
    "    #load data and add to list\n",
    "    try:\n",
    "      df = pd.read_csv(file_path, delimiter=\",\", parse_dates=[\"date\"], date_format=\"%Y%m%d%H\")\n",
    "      all_data_frames.append(df)\n",
    "      print(f\"Add data from: {file_path}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Error while loading {file}: {e}\")\n",
    "  \n",
    "  #if loaded -> combine\n",
    "  if all_data_frames:\n",
    "    combined_data = all_data_frames[0]\n",
    "    for df in all_data_frames[1:]:\n",
    "      df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")                \n",
    "      combined_data = pd.merge(combined_data, df, on=[  \"date\"], how=\"outer\")\n",
    "    combined_data = combined_data.sort_values(by=[  \"date\"]).drop_duplicates(subset=[  \"date\"], keep='last')\n",
    "\n",
    "  #save\n",
    "  final_filename = os.path.join(data_collection_folder, f\"weather.csv\")\n",
    "  combined_data.to_csv(final_filename, index=False)\n",
    "  print(f\"Combined data saved: {final_filename}\")\n",
    "def start_combine_historic():\n",
    "    max_workers = min(os.cpu_count(), len(station_ids_r))  \n",
    "    print(f\"Start cmbination of {len(station_ids_r)} stations with {max_workers} threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_station = {executor.submit(combine_historic, station_r, place): (station_r,  place) for station_r,  place in zip(station_ids_r,  station_place) }    \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()\n",
    "                print(f\"All data combined for {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error while combination of station {station_id}: {e}\")\n",
    "def combine_forecast():\n",
    "\n",
    "  files = [f for f in os.listdir(station_folder) if f.endswith('.csv')]\n",
    "\n",
    "  #rename collumns to stationname\n",
    "  for file in files:\n",
    "    file_path = os.path.join(station_folder, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    #extract filename\n",
    "    file_name = os.path.splitext(file)[0]\n",
    "    columname=[df.columns[0]] + [f'{col}_{file_name}' for col in df.columns[1:]]\n",
    "    df.columns = columname\n",
    "    print(f'renamed collums for {file_name}')\n",
    "    df.to_csv(file_path, index=False)\n",
    " \n",
    "  all_data_frames = []\n",
    "  for file in files:\n",
    "    file_path = os.path.join(station_folder, file)  \n",
    "    \n",
    "    #load data and add to list\n",
    "    try:\n",
    "      df = pd.read_csv(file_path, delimiter=\",\", parse_dates=[\"date\"], date_format=\"%Y%m%d%H\")\n",
    "      all_data_frames.append(df)\n",
    "      print(f\"data added from {file_path}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Error while loading file {file}: {e}\")\n",
    "  \n",
    "   \n",
    "  if all_data_frames:\n",
    "    combined_data = all_data_frames[0]\n",
    "    for df in all_data_frames[1:]:\n",
    "      df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "      combined_data = pd.merge(combined_data, df, on=[  \"date\"], how=\"outer\")\n",
    "    combined_data = combined_data.sort_values(by=[  \"date\"]).drop_duplicates(subset=[  \"date\"], keep='last')\n",
    "\n",
    "\n",
    "  final_filename = os.path.join(forecas_folder, f\"weather_forecast.csv\")\n",
    "  combined_data.to_csv(final_filename, index=False)\n",
    "  print(f\"Saved combined forecast: {final_filename}\")\n",
    "def create_folder():\n",
    "  os.makedirs(computing_folder, exist_ok=True)\n",
    "  os.makedirs(stations_combined, exist_ok=True)\n",
    "  for station in station_ids_r:\n",
    "    output_folder_station = os.path.join(computing_folder, station)\n",
    "    os.makedirs(output_folder_station, exist_ok=True)\n",
    "    station_folder =os.path.join(output_folder,'stations',station)\n",
    "    os.makedirs(station_folder, exist_ok=True)\n",
    "\n",
    "#function to load forecast\n",
    "def station_folderget_weather_data_for_station_review(station_id):\n",
    "    output_filepath = os.path.join(computing_folder,station_id)\n",
    "    print(f\"storage location  {output_filepath}, computing_folder {computing_folder}, station_id {station_id}\")    \n",
    "    for data_type, endpoint in data_types.items():\n",
    "        url = base_url_review + endpoint\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        #lookup zip-file\n",
    "        for line in response.text.splitlines():\n",
    "            if station_id in line and \"zip\" in line:\n",
    "                filename = re.search(r'href=\"(.*?)\"', line).group(1)\n",
    "                file_url = url + filename\n",
    "                \n",
    "                print(f\"Download of: {file_url}\")\n",
    "                file_response = requests.get(file_url)\n",
    "                file_response.raise_for_status()\n",
    "\n",
    "                with zipfile.ZipFile(io.BytesIO(file_response.content)) as z:\n",
    "                    if data_type == \"cloudiness_historical\" or data_type == \"cloudiness_recent\":\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_n_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    elif data_type == \"pressure_historical\" or data_type == \"pressure_recent\":\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_p0_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    elif data_type == \"sun_historical\" or data_type == \"sun_recent\":\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_sd_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    elif data_type == \"wind_historical\" or data_type == \"wind_recent\":\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_ff_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    elif data_type == \"precipitation_historical\" or data_type == \"precipitation_recent\":\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_rr_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    else:\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_tu_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    \n",
    "                    if not txt_files:\n",
    "                        print(f\"No TXT file in the expected format for station {station_id} found.\")\n",
    "                        continue  \n",
    "\n",
    "                    txt_filename = txt_files[0]\n",
    "                    with z.open(txt_filename) as f:\n",
    "                        try:\n",
    "                            df = pd.read_csv(f, sep=\";\", encoding=\"utf-8\")\n",
    "                            if df.empty:\n",
    "                                print(f\"Warning: The file {txt_filename} is empty.\")\n",
    "                            else:\n",
    "                                print(\"Data loaded for:\", txt_filename)\n",
    "                                if data_type == \"temperature_historical\":\n",
    "                                    new_filename = f\"temp_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"temperature_recent\":\n",
    "                                    new_filename = f\"temp_{station_id}_recent.txt\"\n",
    "                                elif data_type == \"cloudiness_historical\":\n",
    "                                    new_filename = f\"clouds_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"cloudiness_recent\":\n",
    "                                    new_filename = f\"clouds_{station_id}_recent.txt\"\n",
    "                                elif data_type == \"pressure_historical\":\n",
    "                                    new_filename = f\"pressure_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"pressure_recent\":\n",
    "                                    new_filename = f\"pressure_{station_id}_recent.txt\"\n",
    "                                elif data_type == \"sun_historical\":\n",
    "                                    new_filename = f\"sun_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"sun_recent\":\n",
    "                                    new_filename = f\"sun_{station_id}_recent.txt\"\n",
    "                                elif data_type == \"wind_historical\":\n",
    "                                    new_filename = f\"wind_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"wind_recent\":\n",
    "                                    new_filename = f\"wind_{station_id}_recent.txt\"      \n",
    "                                elif data_type == \"precipitation_historical\":\n",
    "                                    new_filename = f\"precipitation_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"precipitation_recent\":\n",
    "                                    new_filename = f\"precipitation_{station_id}_recent.txt\"\n",
    "                                output_filename = os.path.join(output_filepath, new_filename)                                \n",
    "                                df.to_csv(output_filename, sep=\";\", encoding=\"utf-8\", index=False)\n",
    "                                print(f\" Saved weather-file as: {output_filepath}\")   \n",
    "                                print(f\" Saved file as: {os.path.abspath(output_filepath)}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error while loading file {txt_filename}: {e}\")\n",
    "    cut_historic_bevor_2015(station_id)\n",
    "\n",
    "def download_weather_data_for_all_stations_review(station_ids):\n",
    "    max_workers = min(os.cpu_count(), len(station_ids))   \n",
    "    print(f\"Start doanload of {len(station_ids)} stations with {max_workers} threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_station = {executor.submit(station_folderget_weather_data_for_station_review, station_id): station_id for station_id in station_ids}        \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()\n",
    "                print(f\"download succeded for{station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error while downloading {station_id}: {e}\")\n",
    "def cut_historic_bevor_2015(station_id):\n",
    "    computing_folder_station = os.path.join(computing_folder, station_id)\n",
    "    station_files = [f for f in os.listdir(computing_folder_station) if re.match(r'(.+)_hist\\.txt', f)]    \n",
    "    for file in station_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        with open(file_path, 'r') as infile:\n",
    "            lines = infile.readlines()\n",
    "        \n",
    "        filtered_lines = []\n",
    "        for line in lines[:1]:\n",
    "            filtered_lines.append(line)\n",
    "        for line in lines[1:]:\n",
    "            columns = line.strip().split(';')\n",
    "            if len(columns) > 1:  \n",
    "                mess_datum = columns[1]\n",
    "                year = int(mess_datum[:4])                \n",
    "                if year >= 2015:\n",
    "                    filtered_lines.append(line)\n",
    "\n",
    "        with open(file_path, 'w') as outfile:\n",
    "            outfile.writelines(filtered_lines)\n",
    "        print(f\"Historically shortened until 2015: {file}\")\n",
    "    remove_columns_review(station_id)\n",
    "\n",
    "def start_cut_historic_bevor_2015(station_ids):\n",
    "    max_workers = min(os.cpu_count(), len(station_ids))   \n",
    "    print(f\"start shortening till 2015 for {len(station_ids)} stations with {max_workers} threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_station = {executor.submit(cut_historic_bevor_2015, station_id): station_id for station_id in station_ids}        \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()\n",
    "                print(f\"Files shortend to 2015 for {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error while shortening files to 2015 for {station_id}: {e}\")\n",
    "\n",
    "def remove_columns_review(station_id):\n",
    "    print('Start Remove Columns')\n",
    "    computing_folder_station =os.path.join(computing_folder, station_id)\n",
    "    temp_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"temp_\") and f.endswith(\".txt\")]\n",
    "    clouds_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"clouds_\") and f.endswith(\".txt\")]\n",
    "    pressure_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"pressure_\") and f.endswith(\".txt\")]\n",
    "    sun_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"sun_\") and f.endswith(\".txt\")]\n",
    "    wind_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"wind_\") and f.endswith(\".txt\")]\n",
    "    precipitation_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"precipitation_\") and f.endswith(\".txt\")]\n",
    "    \n",
    "    for file in clouds_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True)\n",
    "            df = df.drop(columns=[col for col in columns_remove_clouds if col in df.columns])\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"Colums removed from{file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error while processing{file}: {e}\")\n",
    "    \n",
    "    for file in pressure_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True)\n",
    "            df = df.drop(columns=[col for col in columns_remove_pressure if col in df.columns])\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"removed collums from {file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error while prcessing file: {file}: {e}\")\n",
    "\n",
    "    for file in sun_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True) \n",
    "            df = df.drop(columns=[col for col in columns_remove_sun if col in df.columns])\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"removed collums from {file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error while prcessing file: {file}: {e}\")\n",
    "\n",
    "    for file in temp_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True)            \n",
    "            df = df.drop(columns=[col for col in columns_remove_temp if col in df.columns])\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"removed collums from {file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error while prcessing file: {file}: {e}\")\n",
    "\n",
    "    for file in wind_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True) \n",
    "            df = df.drop(columns=[col for col in columns_remove_wind if col in df.columns]) \n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"removed collums from {file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error while prcessing file: {file}: {e}\")\n",
    "\n",
    "    for file in precipitation_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True)\n",
    "            df = df.drop(columns=[col for col in columns_remove_precipitation if col in df.columns])\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"removed collums from {file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error while prcessing file: {file}: {e}\")\n",
    "    combine_historic_recent(station_id)\n",
    "\n",
    "def start_remove_columns_review(station_ids):\n",
    "    max_workers = min(os.cpu_count(), len(station_ids))   \n",
    "    print(f\"Start remove collumns {len(station_ids)} stations with {max_workers} threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_station = {executor.submit(remove_columns_review, station_id): station_id for station_id in station_ids}        \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()  \n",
    "                print(f\"Collumns deleted for {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error while deletion of collumns {station_id}: {e}\")\n",
    "def combine_historic_recent(station_id):\n",
    "    computing_folder_station = os.path.join(computing_folder, station_id)\n",
    "    station_files = [f for f in os.listdir(computing_folder_station) if re.match(r'(.+)_' + station_id + r'_(hist|recent)\\.txt', f)]\n",
    "    file_pairs = {}\n",
    "    for file in station_files:\n",
    "        match = re.match(r'(.+)_' + station_id + r'_(hist|recent)\\.txt', file)\n",
    "        if match:\n",
    "            wettertyp, period = match.groups()\n",
    "            key = f\"{wettertyp}_{station_id}\"\n",
    "            if key not in file_pairs:\n",
    "                file_pairs[key] = {}\n",
    "            file_pairs[key][period] = os.path.join(computing_folder_station, file)\n",
    "\n",
    "    #combine historic an current data\n",
    "    for key, file_pair in file_pairs.items():\n",
    "        if 'hist' in file_pair and 'recent' in file_pair:\n",
    "            hist_df = pd.read_csv(file_pair['hist'], delimiter=\";\")\n",
    "            recent_df = pd.read_csv(file_pair['recent'], delimiter=\";\")\n",
    "            hist_df[\"MESS_DATUM\"] = pd.to_datetime(hist_df[\"MESS_DATUM\"], format=\"%Y%m%d%H\", errors=\"coerce\")\n",
    "            recent_df[\"MESS_DATUM\"] = pd.to_datetime(recent_df[\"MESS_DATUM\"], format=\"%Y%m%d%H\", errors=\"coerce\")\n",
    "\n",
    "            \n",
    "            combined_df = pd.concat([hist_df, recent_df]).drop_duplicates(subset=[\"MESS_DATUM\"], keep='last')\n",
    "            combined_df = combined_df.sort_values(by=[\"MESS_DATUM\"])\n",
    "            combined_df[\"MESS_DATUM\"] = combined_df[\"MESS_DATUM\"].dt.strftime(\"%Y%m%d%H\")\n",
    "\n",
    "            combined_filename = os.path.join(computing_folder_station, f\"{key}_combined.txt\")\n",
    "            combined_df.to_csv(combined_filename, sep=\";\", index=False)\n",
    "            print(f\"Combined data saved: {combined_filename}\")\n",
    "        else:\n",
    "            print(f\"Missing file for {key}\")\n",
    "    combine_all_station_data_review(station_id)\n",
    "\n",
    "def start_combine_historic_recent(station_ids):\n",
    "    max_workers = min(os.cpu_count(), len(station_ids))   \n",
    "    print(f\"Start combination of historic and current data for {len(station_ids)} stations with {max_workers} threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_station = {executor.submit(combine_historic_recent, station_id): station_id for station_id in station_ids}        \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()\n",
    "                print(f\"combined historic and current data for {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error while combining historic and current data {station_id}: {e}\")\n",
    "def combine_all_station_data_review(station_id):\n",
    "    computing_folder_station = os.path.join(computing_folder, station_id)\n",
    "    station_folder_station = os.path.join(station_folder, station_id) \n",
    "    combined_files = [f for f in os.listdir(computing_folder_station) if f.endswith(f\"_{station_id}_combined.txt\")]\n",
    "    all_data_frames = []\n",
    "    for file in combined_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", parse_dates=[\"MESS_DATUM\"], date_format=\"%Y%m%d%H\")\n",
    "            all_data_frames.append(df)\n",
    "            print(f\"data added from {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error while loading file {file}: {e}\")\n",
    "    if all_data_frames:\n",
    "        combined_data = all_data_frames[0]\n",
    "        for df in all_data_frames[1:]:\n",
    "            df[\"MESS_DATUM\"] = pd.to_datetime(df[\"MESS_DATUM\"], format=\"%Y%m%d%H\", errors=\"coerce\")\n",
    "            combined_data = pd.merge(combined_data, df, on=[  \"MESS_DATUM\"], how=\"outer\")\n",
    "        combined_data = combined_data.sort_values(by=[  \"MESS_DATUM\"]).drop_duplicates(subset=[  \"MESS_DATUM\"], keep='last')\n",
    "        combined_data[\"MESS_DATUM\"] = combined_data[\"MESS_DATUM\"].dt.strftime(\"%Y%m%d%H\")\n",
    "        \n",
    "        # change header\n",
    "        header_mapping = {\n",
    "            \"STATIONS_ID\": \"STATIONS_ID\",\n",
    "            \"MESS_DATUM\": \"date\",\n",
    "            \"V_N_I\": \"Wolken_Interp\",\n",
    "            \"V_N\": \"clouds\",\n",
    "            \"P\": \"stationPressure_hPa\",\n",
    "            \"P0\": \"surfacePressure_hPa\",\n",
    "            \"SD_SO\": \"sunshine_min\",\n",
    "            \"TT_TU\": \"T_temperature_C\",\n",
    "            \"RF_TU\": \"humidity_Percent\",\n",
    "            \"F\": \"wind_speed_ms\",\n",
    "            \"D\": \"wind_direction_degree\",\n",
    "            \"R1\": \"precipitationTotal_mm\",\n",
    "            \"RS_IND\": \"precipitation_indicator\"\n",
    "\n",
    "        }\n",
    "    \n",
    "        combined_data.rename(columns=header_mapping, inplace=True)\n",
    "        final_filename = os.path.join(station_folder_station, f\"{station_id}_data_combined.csv\")\n",
    "        combined_data.to_csv(final_filename, sep=\",\", index=False)\n",
    "        print(f\"All data combined for station {station_id} saved in: {final_filename}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"No combined data for station {station_id} found.\")\n",
    "def start_combine_all_station_data_review(station_ids):\n",
    "    max_workers = min(os.cpu_count(), len(station_ids))   \n",
    "    print(f\"Start of combination of data for {len(station_ids)} stations with {max_workers} threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_station = {executor.submit(combine_all_station_data_review, station_id): station_id for station_id in station_ids}        \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()\n",
    "                print(f\"Files combined for {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error while combination of station {station_id}: {e}\")\n",
    "#Weather forecastfunktions:\n",
    "def get_weather_data_for_station_forecast(station_id, station_place):\n",
    "    params = {\n",
    "        \"stationIds\": station_id\n",
    "    }\n",
    "    #prepare request\n",
    "    request = requests.Request(\"GET\", url_forecast, headers=headers_weather, params=params)\n",
    "    prepared_request = request.prepare()\n",
    "    \n",
    "    response = requests.Session().send(prepared_request)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        filename = os.path.join(computing_folder, f\"weather_forecast_{station_place}.json\")\n",
    "        with open(filename, \"w\") as file:\n",
    "            json.dump(data, file, indent=4)\n",
    "        print(f\"Forecast was saved in {filename}\")\n",
    "        with open(filename) as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        for station_id, station_data in data.items():\n",
    "            forecast_data = station_data[\"forecast1\"]\n",
    "            start_time = forecast_data[\"start\"]\n",
    "            time_step = forecast_data[\"timeStep\"]\n",
    "\n",
    "            date = [dt.utcfromtimestamp((start_time + i * time_step) / 1000) for i in range(len(forecast_data[\"temperature\"]))]\n",
    "            \n",
    "            variables = {\n",
    "                \"T_temperature_C\": forecast_data.get(\"temperature\", []),\n",
    "                \"T_temperature_standarddeviation_C\": forecast_data.get(\"temperatureStd\", []),\n",
    "                \"precipitationTotal_mm\": forecast_data.get(\"precipitationTotal\", []),\n",
    "                \"sunshine_min\": forecast_data.get(\"sunshine\", []),\n",
    "                \"dewPoint2m\": forecast_data.get(\"dewPoint2m\", []),\n",
    "                \"surfacePressure_hPa\": forecast_data.get(\"surfacePressure\", []),\n",
    "                \"humidity_Percent\": forecast_data.get(\"humidity\", []),\n",
    "                \"isDay_bool\": forecast_data.get(\"isDay\", [])\n",
    "            }\n",
    "            max_length = max(len(date), *(len(values) for values in variables.values()))\n",
    "            date.extend([None] * (max_length - len(date)))\n",
    "            for key, values in variables.items():\n",
    "                variables[key].extend([None] * (max_length - len(values)))\n",
    "            df = pd.DataFrame({\n",
    "                \"date\": date,\n",
    "                **variables\n",
    "            })             \n",
    "            df[\"T_temperature_C\"] = df[\"T_temperature_C\"].apply(lambda x: x / 10 if pd.notnull(x) else x)\n",
    "            df[\"T_temperature_standarddeviation_C\"] = df[\"T_temperature_standarddeviation_C\"].apply(lambda x: x / 10 if pd.notnull(x) else x)\n",
    "            df[\"surfacePressure_hPa\"] = df[\"surfacePressure_hPa\"].apply(lambda x: x / 10 if pd.notnull(x) else x)\n",
    "            df[\"humidity_Percent\"] = df[\"humidity_Percent\"].apply(lambda x: x / 10 if pd.notnull(x) else x)\n",
    "            df[\"date\"] = df[\"date\"].apply(lambda x: x.strftime(\"%Y%m%d%H\"))\n",
    "            df.to_csv(os.path.join(station_folder, f\"weather_forecast_{station_place}.csv\"), index=False)\n",
    "            print(f\"Weather prediction in weather_forecast_{station_place}.csv convertet\")\n",
    "    else:\n",
    "        print(f\"Error while request {response.status_code}\")\n",
    "def download_weatherforecast_data_for_all_stations_forecast(station_ids, station_places):\n",
    "    for (station_id , station_place) in zip(station_ids, station_places):\n",
    "        print(f\"Start download of Station {station_id}...\")\n",
    "        get_weather_data_for_station_forecast(station_id, station_place)\n",
    "        print()\n",
    "def remove_columns_forecast():\n",
    "    print(\"Start removing columns\")\n",
    "    forecast_files = [f for f in os.listdir(station_folder) if f.startswith(\"weather_forecast_\")]  \n",
    "    print(f\"File: {forecast_files}...\")\n",
    "    for file in forecast_files:\n",
    "        print(f\"Start removing columns for {file}...\")\n",
    "        file_path = os.path.join(station_folder, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\",\", skipinitialspace=True)  \n",
    "            print(f\"Columns in dataframe: {list(df.columns)}\") \n",
    "            df = df.drop(columns=[col for col in columns_remove_forecast if col in df.columns])\n",
    "            df.to_csv(file_path, sep=\",\", index=False)\n",
    "            print(f\"removed collums from {file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error while prcessing file: {file}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "##Stockmarket\n",
    "get_Data(url_oil, '../final-submission/merged_data/data_collection/oilWti.csv', 'Oil WTI', \"../final-submission/merged_data/data_collection/oilWtiOld.csv\")\n",
    "get_Data(url_gas, '../final-submission/merged_data/data_collection/naturalGas.csv', 'Natural Gas', \"../final-submission/merged_data/data_collection/naturalGasOld.csv\")\n",
    "get_Data(url_coal, '../final-submission/merged_data/data_collection/coal.csv', 'Coal', \"../final-submission/merged_data/data_collection/CoalOld.csv\")\n",
    "get_Data(url_uran, '../final-submission/merged_data/data_collection/uran.csv', 'Uran', '../final-submission/merged_data/data_collection/uranOld.csv')\n",
    "\n",
    "fill_missing_hours('../final-submission/merged_data/data_collection/oilWti.csv')\n",
    "fill_missing_hours('../final-submission/merged_data/data_collection/naturalGas.csv')\n",
    "fill_missing_hours('../final-submission/merged_data/data_collection/coal.csv')\n",
    "fill_missing_hours('../final-submission/merged_data/data_collection/uran.csv')\n",
    "\n",
    "df1 = pd.read_csv('../final-submission/merged_data/data_collection/oilWti.csv')\n",
    "df2 = pd.read_csv('../final-submission/merged_data/data_collection/naturalGas.csv')\n",
    "df3 = pd.read_csv('../final-submission/merged_data/data_collection/coal.csv')\n",
    "df35 = pd.read_csv('../final-submission/merged_data/data_collection/uran.csv')\n",
    "\n",
    "merged_df = pd.merge(df1, df2, on='Date', how='outer')\n",
    "merged_df = pd.merge(merged_df, df3, on='Date', how='outer')\n",
    "merged_df = pd.merge(merged_df, df35, on='Date', how='outer')\n",
    "\n",
    "merged_df.to_csv('../final-submission/merged_data/data_collection/merged_data.csv', index=False)\n",
    "\n",
    "print(\"CSV files have been merged and saved.\")\n",
    "\n",
    "end_time = time.time()\n",
    "verstrichene_zeit = end_time - start_time\n",
    "print(f'Ausführungszeit nach Stockmarket: {verstrichene_zeit} Sekunden')\n",
    "\n",
    "\n",
    "##Carbon\n",
    "\n",
    "start_time_carbon = time.time()\n",
    "carbon_df = get_carbon_data(url_carbon, filepath_historic_carbon)\n",
    "carbon_df.to_csv('../final-submission/merged_data/data_collection/carbon.csv', index=False)\n",
    "end_time_carbon = time.time()\n",
    "verstrichene_zeit_carbon = end_time_carbon - start_time_carbon\n",
    "print(f'Ausführungszeit nach Carbon: {verstrichene_zeit_carbon} Sekunden')\n",
    "\n",
    "\n",
    "##entsoe\n",
    "\n",
    "start_time_entsoe = time.time()\n",
    "df4 = pd.read_csv('../final-submission/merged_data/data_collection/day_ahead_prices.csv')\n",
    "#df4.drop(df4.columns[2], axis=1, inplace=True)\n",
    "df4.columns.values[1] = 'day_ahead_prices_EURO'\n",
    "df5 = pd.read_csv('../final-submission/merged_data/data_collection/load_forecast.csv')\n",
    "#df5.drop(df5.columns[2], axis=1, inplace=True)\n",
    "df6 = pd.read_csv('../final-submission/merged_data/data_collection/generation_forecast.csv')\n",
    "#df6.drop(df6.columns[2], axis=1, inplace=True)\n",
    "df7 = pd.read_csv('../final-submission/merged_data/data_collection/intraday_wind_solar_forecast.csv')\n",
    "#df7.drop(df7.columns[4], axis=1, inplace=True)\n",
    "df8 = pd.read_csv('../final-submission/merged_data/data_collection/day_ahead_wind_solar_forecast.csv')\n",
    "#df8.drop(df8.columns[4], axis=1, inplace=True)\n",
    "\n",
    "# rename timestamp to Date\n",
    "df4.rename(columns={'timestamp': 'Date'}, inplace=True)\n",
    "df5.rename(columns={'timestamp': 'Date'}, inplace=True)\n",
    "df6.rename(columns={'timestamp': 'Date'}, inplace=True)\n",
    "df7.rename(columns={'timestamp': 'Date'}, inplace=True)\n",
    "df8.rename(columns={'timestamp': 'Date'}, inplace=True)\n",
    "\n",
    "merged_df2 = pd.merge(df5, df4, on='Date', how='outer')\n",
    "merged_df2 = pd.merge(merged_df2, df6, on='Date', how='outer')\n",
    "merged_df2 = pd.merge(merged_df2, df7, on='Date', how='outer')\n",
    "merged_df2 = pd.merge(merged_df2, df8, on='Date', how='outer')\n",
    "\n",
    "merged_df2.to_csv('../final-submission/merged_data/data_collection/merged_data2.csv', index=False)\n",
    "\n",
    "df = pd.read_csv('../final-submission/merged_data/data_collection/merged_data2.csv')\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df_filtered = df[df['Date'].dt.minute == 0]\n",
    "df_filtered['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_filtered.to_csv('../final-submission/merged_data/data_collection/merged_data3.csv', index=False)\n",
    "\n",
    "end_time_entsoe = time.time()\n",
    "verstrichene_zeit_entsoe = end_time_entsoe - start_time_entsoe\n",
    "print(f'Ausführungszeit nach Merge vor save als csv: {verstrichene_zeit_entsoe} Sekunden')\n",
    "\n",
    "merged_df.to_csv('../final-submission/merged_data/data_collection/merged_data_multi_2.csv', index=False)\n",
    "\n",
    "end_time_entsoe = time.time()\n",
    "verstrichene_zeit_entsoe = end_time_entsoe - start_time_entsoe\n",
    "print(f'Ausführungszeit nach Merge_to_csv: {verstrichene_zeit_entsoe} Sekunden')\n",
    "\n",
    "end_time_entsoe = time.time()\n",
    "verstrichene_zeit_entsoe = end_time_entsoe - start_time_entsoe\n",
    "print(f'Ausführungszeit komplett: {verstrichene_zeit_entsoe} Sekunden')\n",
    "end_time = time.time()\n",
    "verstrichene_zeit = end_time - start_time\n",
    "print(f'Ausführungszeit nach Entsoe: {verstrichene_zeit} Sekunden')\n",
    "\n",
    "##Covid Lockdown Data\n",
    "# generate and populate dataframe with all dates from 2015-1-1 - today\n",
    "from datetime import date, timedelta\n",
    "\n",
    "working_dt = date(2015, 1, 1)\n",
    "end_dt = date(date.today().year, date.today().month, date.today().day)\n",
    "delta = timedelta(days=1)\n",
    "\n",
    "data_rows = []\n",
    "\n",
    "# populate df\n",
    "while working_dt <= end_dt:\n",
    "    factor = evaluate_date(working_dt.isoformat())\n",
    "    date = working_dt.isoformat()\n",
    "    for hour in range(24):\n",
    "        timestamp = pd.Timestamp(working_dt.isoformat()) + pd.Timedelta(hours=hour)\n",
    "        data_rows.append({'Date': timestamp, 'Covid factor': factor})  # Add to rows list\n",
    "    working_dt += delta\n",
    "\n",
    "covid_factors_df = pd.DataFrame(data_rows)\n",
    "print(covid_factors_df.head)\n",
    "\n",
    "covid_factors_df.to_csv('../final-submission/merged_data/data_collection/covid.csv', index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "verstrichene_zeit = end_time - start_time\n",
    "print(f'Ausführungszeit nach Covidzahlen: {verstrichene_zeit} Sekunden')\n",
    "\n",
    "##Smard\n",
    "output_path = '../final-submission/merged_data/data_collection/smard.csv'\n",
    "\n",
    "dict_ids = [balancing_id[\"automatic_frequency\"],\n",
    "            balancing_id[\"balancing_energy\"],\n",
    "            balancing_id[\"costs\"],\n",
    "            balancing_id[\"exported_balancing_services\"],\n",
    "            balancing_id[\"frequency_containment\"],\n",
    "            balancing_id[\"imported_balancing_services\"],\n",
    "            balancing_id[\"manual_frequency\"],\n",
    "            electricity_consumption_id[\"actual\"],\n",
    "            electricity_consumption_id[\"forecast\"],\n",
    "            electricity_generation_id[\"actual\"],\n",
    "            electricity_generation_id[\"forecast\"],\n",
    "            market_id[\"cross_border_physical\"],\n",
    "            market_id[\"scheudled_commercial_exchanges\"],\n",
    "            market_id[\"day_ahead_prices\"]    \n",
    "    ]\n",
    "    \n",
    "final_df = None\n",
    "\n",
    "for i in range(13):\n",
    "    working_df = download(i)\n",
    "    working_df = new_format(working_df, dict_ids[i])\n",
    "\n",
    "    if i > 0:\n",
    "       working_df=working_df.drop(working_df.columns[1],axis=1)\n",
    "        #only called once\n",
    "    if final_df is None:\n",
    "            final_df = working_df\n",
    "    else:\n",
    "        final_df = pd.merge(final_df, working_df, on=working_df.columns[0], how='inner', copy=True)\n",
    "    \n",
    "final_df.to_csv(output_path, sep=',', index=False)\n",
    "\n",
    "#use gzip to compress .csv outputfile to <file_out>.gz\n",
    "path_object = Path(output_path)\n",
    "output_pathgz = path_object.with_suffix('.gz')\n",
    "final_df.to_csv(output_pathgz, sep=',', index=False, compression='gzip')\n",
    "\n",
    "end_time = time.time()\n",
    "verstrichene_zeit = end_time - start_time\n",
    "print(f'Ausführungszeit nach Smard: {verstrichene_zeit} Sekunden')\n",
    "\n",
    "##weather\n",
    "start_time_w = time.time()\n",
    "create_folder()\n",
    "download_weather_data_for_all_stations_review(station_ids_r)\n",
    "\n",
    "end_time_w = time.time()\n",
    "verstrichene_zeit = end_time_w - start_time_w\n",
    "print(f'Ausführungszeit Wetter: {verstrichene_zeit} Sekunden')\n",
    "download_weatherforecast_data_for_all_stations_forecast(station_ids_f, station_place)\n",
    "remove_columns_forecast()\n",
    "\n",
    "end_time_w = time.time()\n",
    "verstrichene_zeit = end_time_w - start_time_w\n",
    "print(f'Ausführungszeit Wetter: {verstrichene_zeit} Sekunden')\n",
    "\n",
    "start_combine_historic()\n",
    "enend_time_wd = time.time()\n",
    "verstrichene_zeit = end_time_w - start_time_w\n",
    "print(f'Ausführungszeit Wetter: {verstrichene_zeit} Sekunden')\n",
    "combine_all_stations()\n",
    "combine_forecast()\n",
    "\n",
    "end_time_w = time.time()\n",
    "verstrichene_zeit = end_time_w - start_time_w\n",
    "print(f'Ausführungszeit Wetter: {verstrichene_zeit} Sekunden')\n",
    "\n",
    "end_time = time.time()\n",
    "verstrichene_zeit = end_time - start_time\n",
    "print(f'Ausführungszeit nach dem Wetter: {verstrichene_zeit} Sekunden')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##Zusammenfassung\n",
    "df_res = pd.read_csv('../final-submission/merged_data/data_collection/merged_data.csv')\n",
    "df_ens = pd.read_csv('../final-submission/merged_data/data_collection/merged_data3.csv')\n",
    "df_smard = pd.read_csv('../final-submission/merged_data/data_collection/smard.csv')\n",
    "df_smard = df_smard.rename(columns={'Start_Date': 'Date'})\n",
    "df_smard.to_csv('../final-submission/merged_data/data_collection/smard.csv', index=False)\n",
    "df_smard = pd.read_csv('../final-submission/merged_data/data_collection/smard.csv')\n",
    "print(df_smard.head())\n",
    "df_smard['Date'] = pd.to_datetime(df_smard['Date'])\n",
    "df_filteredSmard = df_smard[df_smard['Date'].dt.minute == 0]\n",
    "df_filteredSmard['Date'] = pd.to_datetime(df_filteredSmard['Date']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_filteredSmard.to_csv('../final-submission/merged_data/data_collection/smard.csv', index=False)\n",
    "df_smard = pd.read_csv('../final-submission/merged_data/data_collection/smard.csv')\n",
    "\n",
    "df_weather = pd.read_csv('../final-submission/merged_data/data_collection/weather.csv')\n",
    "df_weather = df_weather.rename(columns={'date': 'Date'})\n",
    "df_covid = pd.read_csv('../final-submission/merged_data/data_collection/covid.csv')\n",
    "df_social = pd.read_csv('../final-submission/merged_data/data_collection/major_social_events.csv')\n",
    "df_carbon = pd.read_csv('../final-submission/merged_data/data_collection/carbon.csv')\n",
    "\n",
    "merge_big = pd.merge(df_ens, df_res, on='Date', how='outer')\n",
    "merge_big = pd.merge(merge_big, df_smard, on='Date', how='outer')\n",
    "merge_big = pd.merge(merge_big, df_social, on='Date', how='outer')\n",
    "merge_big = pd.merge(merge_big, df_carbon, on='Date', how='outer')\n",
    "merge_big = pd.merge(merge_big, df_weather, on='Date', how='outer')\n",
    "merge_big = pd.merge(merge_big, df_covid, on='Date', how='outer')\n",
    "\n",
    "#add weekdays and Holidays\n",
    "merge_big['Date'] = pd.to_datetime(merge_big['Date'])\n",
    "merge_big['month'] = merge_big['Date'].dt.month\n",
    "merge_big['weekday'] = merge_big['Date'].dt.weekday  # 0=Montag, 6=Sonntag\n",
    "merge_big['week_of_year'] = merge_big['Date'].dt.isocalendar().week\n",
    "merge_big['is_weekend'] = merge_big['weekday'].isin([5, 6])\n",
    "german_holidays = holidays.Germany(years=range(merge_big['Date'].dt.year.min(),\n",
    "                                               merge_big['Date'].dt.year.max() + 1))\n",
    "merge_big['date'] = merge_big['Date'].dt.date\n",
    "merge_big['is_holiday'] = merge_big['date'].isin(german_holidays)\n",
    "merge_big = merge_big.loc[:, ~merge_big.columns.str.endswith('_y')]\n",
    "merge_big.columns =merge_big.columns.str.replace('_x$', '', regex=True)\n",
    "\n",
    "df_unique = merge_big.drop_duplicates('Date', keep ='first')\n",
    "\n",
    "df_unique.to_csv('../final-submission/merged_data/allData.csv', index=False)\n",
    "\n",
    "print(\"CSV files have been merged and saved.\")\n",
    "end_time = time.time()\n",
    "verstrichene_zeit = end_time - start_time\n",
    "print(f'Ausführungszeit komplett: {verstrichene_zeit} Sekunden')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_test_val_split(df, target_column, train_interval, test_interval, val_interval):\n",
    "    \"\"\"Split dataset on the in project determined timeranges for train, test, validation.\n",
    "\n",
    "    :param df: full input dataframe\n",
    "    :param target_column: column to predict/forecast later on\n",
    "    :returns: dataframes for X_train, y_train, X_test, y_test, X_val, y_val\n",
    "    \"\"\"\n",
    "    df['Date'] = pd.DatetimeIndex(df['Date'].values)\n",
    "    df = df.set_index('Date')\n",
    "    train_df = df[train_interval[0]:train_interval[1]]\n",
    "    val_df = df[val_interval[0]:val_interval[1]]\n",
    "    test_df = df[test_interval[0]:test_interval[1]]\n",
    "\n",
    "    X_train = train_df.drop(target_column, axis=1)\n",
    "    X_val = val_df.drop(target_column, axis=1)\n",
    "    X_test = test_df.drop(target_column, axis=1)\n",
    "    y_train = train_df[[target_column]]\n",
    "    y_val = val_df[[target_column]]\n",
    "    y_test = test_df[[target_column]]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def fill_missing_with_xgboost(df, target_column):\n",
    "    \"\"\"\n",
    "    Fills nan values in the target_column of the DataFrame using an XGBoost regressor trained\n",
    "    with the other columns as features.\n",
    "\n",
    "    :param df: input dataframe\n",
    "    :param target_column: column which contains nan-values, that needs to be replaces\n",
    "    :return: dataframe without nan-values in target column\n",
    "    \"\"\"\n",
    "    # separate the target column\n",
    "    target = df[target_column]\n",
    "\n",
    "    # identify rows with and without NaN in the target column\n",
    "    missing_mask = target.isna()\n",
    "    complete_mask = ~missing_mask\n",
    "\n",
    "    # split the data into training (non-nan) and prediction (nan)\n",
    "    X_train = df.loc[complete_mask].drop(columns=[target_column])\n",
    "    y_train = target[complete_mask]\n",
    "    X_predict = df.loc[missing_mask].drop(columns=[target_column])\n",
    "\n",
    "    X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(\n",
    "        X_train, y_train, test_size=0.05, random_state=42\n",
    "    )\n",
    "    device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # train the XGBoost model\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=4000,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        device=device\n",
    "    )\n",
    "    model.fit(X_train_split, y_train_split,\n",
    "              eval_set=[(X_valid_split, y_valid_split)],\n",
    "              verbose=False)\n",
    "\n",
    "    # predict the missing values\n",
    "    predictions = model.predict(X_predict)\n",
    "\n",
    "    # fill the missing values in the original DataFrame\n",
    "    df.loc[missing_mask, target_column] = predictions\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def has_long_nan_streak(series, threshold):\n",
    "    \"\"\"Detect long continuous nan-value sequences in a series\n",
    "\n",
    "    :param series: series of floats, may contain nan\n",
    "    :param threshold: maximum continuous nan values in sequence in a row\n",
    "    :return: True if less that threshold nan-values in a row at any point in the series, otherwise False\n",
    "    \"\"\"\n",
    "    max_nan_streak = (series.isna().astype(int)\n",
    "                      .groupby(series.notna().astype(int).cumsum())\n",
    "                      .transform('sum').max())\n",
    "    return max_nan_streak > threshold\n",
    "\n",
    "\n",
    "def run_preprocessing(df, nan_streak_threshold, start_ts, end_ts):\n",
    "    time_n = time.time()\n",
    "    df = df.drop(['date', 'End_Date'], axis=1)\n",
    "    df['is_holiday'] = df['is_holiday'].values.astype(int)\n",
    "    df['is_weekend'] = df['is_weekend'].values.astype(int)\n",
    "\n",
    "    unique_df = df.loc[:, ~df.T.duplicated()]\n",
    "\n",
    "    unique_df['Date'] = df.index.values\n",
    "    unique_df['Date'] = pd.DatetimeIndex(unique_df['Date'].values)\n",
    "    unique_df = unique_df.set_index('Date')\n",
    "    start_date = start_ts\n",
    "    end_date = end_ts\n",
    "    unique_df = unique_df[start_date:end_date]\n",
    "\n",
    "    unique_df = unique_df.reset_index()\n",
    "    timestamps = unique_df['Date']\n",
    "    unique_df = unique_df.drop(['Date'], axis=1)\n",
    "\n",
    "    columns_to_keep = ~unique_df.apply(has_long_nan_streak, threshold=nan_streak_threshold, axis=0)\n",
    "    no_nan_streaks_df = unique_df.loc[:, columns_to_keep]\n",
    "\n",
    "    features = list(no_nan_streaks_df.columns)\n",
    "    new_df = no_nan_streaks_df.copy(deep=True)\n",
    "    for i, c in enumerate(features):\n",
    "        if no_nan_streaks_df[c].isna().sum() > 0:\n",
    "            new_df = fill_missing_with_xgboost(new_df, target_column=c)\n",
    "        print(f'feature {i+1}: {c} done after: {int((time.time() - time_n)/60)}')\n",
    "\n",
    "    new_df['Date'] = timestamps\n",
    "\n",
    "    print(f'preprocessing done in: {int((time.time() - time_n)/60)}')\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Gathering Domain Knowledge"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Due to the complexity of the German electricity market and the vastness of factors pertaining to the formation of electricity prices, we began our efforts with a collection of relevant domain knowledge.\n",
    "The present section presents our findings on the German energy market, specifically the day-ahead price and key influences pertaining to its formation.\n",
    "This exploration then informs both implementation details and data selection that will be discussed in later sections.\n",
    "\n",
    "### The German Energy Market\n",
    "#### Electricity Markets Within Germany\n",
    "As explained by Schumacher et al. (2015), in electricity markets such as Germany, the price of electricity is primarily determined by the instantaneous relationship between energy consumption and energy production. Energy that has been produced is rarely stored in a quantity that is meaningful in relation to the prevailing market price. The market value may therefore fluctuate on a moment-by-moment basis in response to outages, consumption spikes, or production surges.\n",
    "This naturally introduces an element of unpredictability, which may not be conducive to the stability and predictability that procucers and consumers of electricity require in order to plan and optimize their operating shedules and procedures.\n",
    "Consequently, submarkets have emerged that offer longer-term contracts, enabling customers to procure electricity in advance for a period of up to six years, typically at a premium for these assurances. Submarkets offering electricity for shorter periods may also be established, with auctions taking place for the following day or even intra-day trading.\n",
    "Similar to other markets, the ability to effectively predict electricity prices is of significant importance to market player trying to sell or purchase power at both opportune points in time and price points.\n",
    "It is important to note that in Germany only approximately 20% of electricity volume trading occurs on exchange markets, with the remainder being directly traded between producers and consumers, usually in an industrial context. Nevertheless, even contracts that are negotiated directly between consumers and producers of electricity directly, frequently establish their prices in accordance with the prices set by the exchanges.\n",
    "\n",
    "\n",
    "#### Day-Ahead Prices\n",
    "One of the main subarkets established to reduce uncertainty for consumers and producers of power is the so caled day-ahead-price.\n",
    "As the main forecast target of the present work, the following subsection outlines the creation of the day-ahead price, as described in Nestle et al. (2009) and Schumacher et al. (2015).\n",
    "By midday, market participants submit their bids and offers, which include the quantity and delivery time for the following day. Based on these aforementioned bids and offers, a wholesale price for each hour of the forthcoming day is calculated.\n",
    "Ultimately, the price of electricity is determined by ranking the offers in descending order of price, with lower-priced generation given precedence until no more bids and offers can be matched.\n",
    "The highest production price that is still accepted becomes the agreed-upon price, resulting in varying margins between producers.\n",
    "For many goods, their price is mostly driven by profits, initial- and marginal costs. In the context of the present work, initial costs are those associated with the installation of power plants and infrastructure, marginal costs those pertaining to production and upkeep. \n",
    "Lower-priced electricity generation tends to be that, with low associated costs.\n",
    "While renewable energy sources may require some installation costs, they do not have to include fuel prices in their calculations and can operate with low marginal costs, given good conditions such as wind or sunshine.\n",
    "Therefore, these conditions affect the production volume of such lower priced energy and thereby heavily influences market prices. \n",
    "Examples of this would be either days with low wind speeds and little sunshine, on which the volume of cheap, renewable energy would be much lower, raising the need for alternative power plants to fill the gap with higher priced energy.\n",
    "\n",
    "#### Importance Of Prediction\n",
    "While these dynamic factors are important for providers and customers for economic planning as previously described, prediction and planning are also crucial for the grids stability and ability to reliably provide electricity to consumers.\n",
    "Producers and consumers are grouped in balancing groups (Bilanzkreis) as described by Dumancic (2024), with transmission system operators balancing supply and demand.\n",
    "\n",
    "Should the delicate balance of production and consumption be tilted, drastic consequences, such as blackouts and outages for electricity grids could occur as outlined by Horáček (2010).\n",
    "Unplanned imbalance in the form of over or underproduction of electricity in such a balancing group is therefore financially penalized, creating further incentives to make accurate predictions. \n",
    "\n",
    "### Conclusion\n",
    "This chapter provides an overview of the German energy market. The bidding process is explained, where producers and consumers submit their offers and demands, respectively, and the resulting day-ahead prices are calculated. Additionally, the influence of different energy sources on the resulting prices is discussed.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Sources\n",
    "In order to predict the day-ahead-price as accurately as possible, a variety of input data was chosen. The following aims to explain and inform these choices.\n",
    "As seen in Niedermayer (2023) and Hein et al. (2020), the installed net rated capacity of electricity in Germany is steadily increasing. This phenomenon must be taken into account when analyzing any variable in isolation and observing a raise.\n",
    "According to Bosch et al. (2023) and Hein et al. (2020), main energy sources include fossil gas, lignite, coal, wind, and photovoltaics. From the production side, main drivers of energy prices are therefore factors pertaining to these elements, such as fuel prices or weather conditions.\n",
    "This chapter explains the different types of data used to represent these sources in the forecasts.\n",
    "On the consumer side, the selection of pertinent data poses a more substantial challenge, as their behavior may be more challenging to categorize than that of, say, the primary five types of producers.\n",
    "In the context of this study, it was hypothesized that the majority of energy consumption exhibits periodic patterns that are evident in the available data, such as time of day and date. Additionally, data concerning social events was incorporated into the analysis. Furthermore, data pertaining to electricity production was also included in the study, as it is conceivable that such data could influence energy consumption. For instance, meteorological data, such as temperature, could potentially impact the necessity for heating, consequently affecting electricity consumption. \n",
    "Finally, the process of downloading the data in the previous utils section is described and results of the data analysis are provided."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Presentation And Discussion\n",
    "In this section the different data sources and their according data frame columns are shown and explained.\n",
    "Furthermore, background information regarding obtaining the data is given, along with the introduction of issues that will make preprocessing necessary."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### SMARD Electricity Market Data\n",
    "SMARD is an information platform for the German electricity market run by the federal power network agency \"Bundesnetzagentur\".\n",
    "On the SMARD website there are datasets containing electricity generation, balancing, consumption and commercial exchanges available for download, and real time data is updated with high frequency.\n",
    "Furthermore, background information about these datasets and the electricity is given, explaining the types and context of available data. This is especially important for datasets which went through different circumstances or determination methods within our selected period of time, as these changes might make some additional preprocessing necessary (as explained in <ins>&#8594;Data Cleaning/SMARD-Data Preprocessing</ins>).\n",
    "SMARD presents a reliable data source due to it being a government backed service, and the wide variety of datasets downloadable freely.\n",
    "\n",
    "The dataset contains the following columns. Additional information can be found in the [SMARD user guide, 2024]."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| Column Name                                    | Unit                 | Application                     | Description                                                                                                                                                                                                                                                                                      |\n",
    "|------------------------------------------------|----------------------|---------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Date                                           | YYYY-MM-DD HH:mm:ss  | Timestamp                       | Date and start hour timestamp                                                                                                                                                                                                                                                                    |\n",
    "| End_​Date                                       | YYYY-MM-DD HH:mm:ss  | Timestamp                       | Date and end hour timestamp                                                                                                                                                                                                                                                                      |\n",
    "| af_​E_​Volume_​Activated_​Plus_​MWh                 | MWh                  | Automatic Frequency Restoration | Energy volume activated to counter an energy deficit                                                                                                                                                                                                                                             |\n",
    "| af_​E_​Volume_​Activated_​Minus_​MWh                | MWh                  | Automatic Frequency Restoration | Energy volume activated to counter an energy surplus                                                                                                                                                                                                                                             |\n",
    "| af_​Activation_​Price_​Plus_​EUR_​MWh               | €/MWh                | Automatic Frequency Restoration | Balancing energy price to counter an energy deficit                                                                                                                                                                                                                                              |\n",
    "| af_​Activation_​Price_​Minus_​EUR_​MWh              | €/MWh                | Automatic Frequency Restoration | Balancing energy price to counter an energy surplus                                                                                                                                                                                                                                              |\n",
    "| af_​E_​Volume_​Procured_​Plus_​MW                   | MW                   | Automatic Frequency Restoration | Power procured to counter an energy deficit                                                                                                                                                                                                                                                      |\n",
    "| af_​E_​Volume_​Procured_​Minus_​MW                  | MW                   | Automatic Frequency Restoration | Power procured to counter an energy surplus                                                                                                                                                                                                                                                      |\n",
    "| af_​Procurement_​Price_​Plus_​EUR_​MW               | €/MW                 | Automatic Frequency Restoration | Price of procured power to counter an energy deficit                                                                                                                                                                                                                                             |\n",
    "| af_​Procurement_​Price_​Minus_​EUR_​MW              | €/MW                 | Automatic Frequency Restoration | Price of procured power to counter an energy surplus                                                                                                                                                                                                                                             |\n",
    "| E_​Volume_​Calculated_​Plus_​MWh                   | MWh                  | Balancing                       | Total balancing energy used to physically balance an energy deficit                                                                                                                                                                                                                              |\n",
    "| E_​Volume_​Calculated_​Minus_​MWh                  | MWh                  | Balancing                       | Total balancing energy used to physically balance an energy surplus                                                                                                                                                                                                                              |\n",
    "| Price_​Calculated_​EUR_​MWh                       | €/MWh                | Balancing                       | A charge payable by parties causing imbalances for costs/revenue arising from the use of balancing energy                                                                                                                                                                                        |\n",
    "| Net_​Income_​EUR                                 | €                    | Balancing                       | Net income to the transmission system operators after settling the imbalance accounts with balance responsible parties                                                                                                                                                                           |\n",
    "| Balancing_​Services_​Calculated_​EUR              | €                    | Balancing costs                 | The total costs for maintaining system stability and security comprise balancing energy income and expenses                                                                                                                                                                                      |\n",
    "| Network_​Security_​Calculated_​EUR                | €                    | Balancing costs                 | The total costs incurred for network security measures                                                                                                                                                                                                                                           |\n",
    "| Countertrading_​Calculated_​EUR                  | €                    | Balancing costs                 | The total costs incurred for countertrading                                                                                                                                                                                                                                                      |\n",
    "| export_​E_​Austria_​Calculated_​MWh                | MWh                  | Exported balancing services     | Volume of balancing energy activated in the control areas concerned                                                                                                                                                                                                                              |\n",
    "| E_​Volume_​Procured_​Calculated_​MW                | MW                   | Frequency containment reserve   | Volume of procured balancing services, corresponds to accepted aggregated offers per procurement period                                                                                                                                                                                          |\n",
    "| Price_​Procument_​Calculated_​EUR/MW              | €/MW                 | Frequency containment reserve   | Price of procured balancing services                                                                                                                                                                                                                                                             |\n",
    "| import_​E_​Austria_​Calculated_​MWh                | €/MW                 | Imported balancing services     | Volume of balancing energy activated in the control areas concerned                                                                                                                                                                                                                              |\n",
    "| mf_​E_​Volume_​Activated_​Plus_​MWh                 | MWh                  | Manual Frequency Restoration    | Energy volume activated to counter an energy deficit                                                                                                                                                                                                                                             |\n",
    "| mf_​E_​Volume_​Activated_​Minus_​MWh                | MWh                  | Manual Frequency Restoration    | Energy volume activated to counter an energy surplus                                                                                                                                                                                                                                             |\n",
    "| mf_​Activation_​Price_​Plus_​EUR_​MWh               | €/MWh                | Manual Frequency Restoration    | Balancing energy price to counter an energy deficit                                                                                                                                                                                                                                              |\n",
    "| mf_​Activation_​Price_​Minus_​EUR_​MWh              | €/MWh                | Manual Frequency Restoration    | Balancing energy price to counter an energy surplus                                                                                                                                                                                                                                              |\n",
    "| mf_​E_​Volume_​Procured_​Plus_​MW                   | MW                   | Manual Frequency Restoration    | Power procured to counter an energy deficit                                                                                                                                                                                                                                                      |\n",
    "| mf_​E_​Volume_​Procured_​Minus_​MW                  | MW                   | Manual Frequency Restoration    | Power procured to counter an energy surplus                                                                                                                                                                                                                                                      |\n",
    "| mf_​Procurement_​Price_​Plus_​EUR_​MW               | €/MW                 | Manual Frequency Restoration    | Price of procured power to counter an energy deficit                                                                                                                                                                                                                                             |\n",
    "| mf_​Procurement_​Price_​Minus_​EUR_​MW              | €/MW                 | Manual Frequency Restoration    | Price of procured power to counter an energy surplus                                                                                                                                                                                                                                             |\n",
    "| actual_​E_​Total_​Gridload_​MWh                    | MWh                  | Electricity consumption         | Actual total energy grid load                                                                                                                                                                                                                                                                    |\n",
    "| actual_​E_​Residual_​Load_​MWh                     | MWh                  | Electricity consumption         | Total actual consumption minus the feed-in from photovoltaic, onshore wind and offshore wind installations                                                                                                                                                                                       |\n",
    "| actual_​E_​Hydro_​Pumped_​Storage_​MWh              | MWh                  | Electricity consumption         | Actual hydro pumped storage consumption                                                                                                                                                                                                                                                          |\n",
    "| forecast_​E_​Total_​Gridload_​MWh                  | MWh                  | Electricity consumption         | Forecasted total energy grid load                                                                                                                                                                                                                                                                |\n",
    "| forecast_​actual_​E_​Residual_​Load_​MWh            | MWh                  | Electricity consumption         | Total forecasted consumption minus the forecasted feed-in from photovoltaic, onshore wind and offshore wind installations                                                                                                                                                                        |\n",
    "| actual_​generation_​E_​Biomass_​MWh                | MWh                  | Electricity Generation          | Actual biomass electricity generation                                                                                                                                                                                                                                                            |\n",
    "| actual_​generation_​E_​Hydropower_​MWh             | MWh                  | Electricity Generation          | Actual hydropower electricity generation                                                                                                                                                                                                                                                         |\n",
    "| actual_​generation_​E_​Windoffshore_​MWh           | MWh                  | Electricity Generation          | Actual wind offshore electricity generation                                                                                                                                                                                                                                                      |\n",
    "| actual_​generation_​E_​Windonshore_​MWh            | MWh                  | Electricity Generation          | Actual wind onshore electricity generation                                                                                                                                                                                                                                                       |\n",
    "| actual_​generation_​E_​Photovoltaics_​MWh          | MWh                  | Electricity Generation          | Actual photovoltaics electricity generation                                                                                                                                                                                                                                                      |\n",
    "| actual_​generation_​E_​OtherRenewable_​MWh         | MWh                  | Electricity Generation          | Actual geothermal energy, landfill gas, sewage gas and pit gas electricity generation                                                                                                                                                                                                            |\n",
    "| actual_​generation_​E_​Nuclear_​MWh                | MWh                  | Electricity Generation          | Actual nuclear electricity generation                                                                                                                                                                                                                                                            |\n",
    "| actual_​generation_​E_​Lignite_​MWh                | MWh                  | Electricity Generation          | Actual fossil brown coal electricity generation                                                                                                                                                                                                                                                  |\n",
    "| actual_​generation_​E_​HardCoal_​MWh               | MWh                  | Electricity Generation          | Actual fossil hard coal electricity generation                                                                                                                                                                                                                                                   |\n",
    "| actual_​generation_​E_​FossilGas_​MWh              | MWh                  | Electricity Generation          | Actual fossil gas electricity generation                                                                                                                                                                                                                                                         |\n",
    "| actual_​generation_​E_​HydroPumpedStorage_​MWh     | MWh                  | Electricity Generation          | Actual hydro pumped storage electricity generation                                                                                                                                                                                                                                               |\n",
    "| actual_​generation_​E_​OtherConventional_​MWh      | MWh                  | Electricity Generation          | Actual electricity generation from derived gas from coal, mineral oil, waste, oxygen steel furnace gas, blast furnace gas, refinery gas, gas with a high proportion of hydrogen, other byproducts of production (for example steel and coke production) and mixtures of more than one fuel type. |\n",
    "| forecast_​generation_​E_​Total_​MWh                | MWh                  | Electricity generation          | Dayahead forecasted total electricity generation                                                                                                                                                                                                                                                 |\n",
    "| forecast_​generation_​E_​PhotovoltaicsAndWind_​MWh | MWh                  | Electricity generation          | Dayahead forecasted photovoltaics and wind electricity generation                                                                                                                                                                                                                                |\n",
    "| forecast_​generation_​E_​Windoffshore_​MWh         | MWh                  | Electricity generation          | Dayahead forecasted wind offshore electricity generation                                                                                                                                                                                                                                         |\n",
    "| forecast_​generation_​E_​Windonshore_​MWh          | MWh                  | Electricity generation          | Dayahead forecasted wind onshore electricity generation                                                                                                                                                                                                                                          |\n",
    "| forecast_​generation_​E_​Photovoltaics_​MWh        | MWh                  | Electricity generation          | Dayahead forecasted photovoltaics electricity generation                                                                                                                                                                                                                                         |\n",
    "| forecast_​generation_​E_​Original_​MWh             | MWh                  | Electricity generation          | Difference between the total forecasted dayahead generation and the forecasted dayahead generation from wind and photovoltaic installations                                                                                                                                                      |\n",
    "| E_​«COUNTRY»Export_​corssBorderPhysical_​MWh      | MWh                  | Market physical flows           | Physical crossborder flows to «COUNTRY» (Netherlands, Switzerland, Denmark, Czech Republic, Luxembourg, Sweden, Austria, France, Poland, Norway, Belgium)                                                                                                                                        |\n",
    "| E_​«COUNTRY»Import_​corssBorderPhysical_​MWh      | MWh                  | Market physical flows           | Physical crossborder flows from «COUNTRY» (Netherlands, Switzerland, Denmark, Czech Republic, Luxembourg, Sweden, Austria, France, Poland, Norway, Belgium)                                                                                                                                      |\n",
    "| E_​«COUNTRY»Export_​MWh                          | MWh                  | Market exchanges                | Scheduled commercial exchange exports to «COUNTRY» (Netherlands, Switzerland, Denmark, Czech Republic, Luxembourg, Sweden, Austria, France, Poland, Norway, Belgium)                                                                                                                             |\n",
    "| E_​«COUNTRY»Import_​MW                           | MWh                  | Market exchanges                | Scheduled commercial exchange imports from «COUNTRY» (Netherlands, Switzerland, Denmark, Czech Republic, Luxembourg, Sweden, Austria, France, Poland, Norway, Belgium)                                                                                                                           |\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pd.read_csv('../final-submission/merged_data/data_collection/smard.csv')[1000:].head()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Weather Data\n",
    "The weather is a pivotal factor in the calculation of the day-ahead price, given its direct impact on supply and demand in the energy market. Meteorological conditions, such as temperatures, wind speeds, and solar radiation, influence both energy consumption, e.g. for heating or cooling, and energy generation from renewable sources like wind and solar energy.\n",
    "\n",
    "To record the meteorological conditions in Germany, three representative weather stations were selected, covering the northern, central, and southern regions. This geographical division facilitates the consideration of distinct climatic conditions in the respective regions, in conjunction with other data sources.\n",
    "\n",
    "The meteorological data for these stations is obtained from the OpenWeather platform of the German Weather Service (DWD) (https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/). This involves the integration of historical and current weather data, with the values from 2015 onwards being merged into a dataframe, as can be seen in the table below. This extensive data set is then enriched with the current forecasts for the following days, thus enabling more accurate mapping of the day-ahead price.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "weather_data=pd.read_csv(\"../final-submission/merged_data/data_collection/weather.csv\")\n",
    "print(weather_data.head())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following table provides a comprehensive list of all weather data. The measured values have been recorded for the cities of Munich, Hamburg and Bonn/Cologne.\n",
    "\n",
    "| Column Name                          | Description                                        |\n",
    "|--------------------------------------|--------------------------------------------------|\n",
    "| date                                 | Date of the measurement                          |\n",
    "| precipitationTotal_mm_{city}         | Total precipitation in mm for {city}            |\n",
    "| clouds_{city}                        | Cloud coverage in % for {city}                   |\n",
    "| sunshine_min_{city}                   | Sunshine duration in minutes for {city}         |\n",
    "| stationPressure_hPa_{city}           | Station air pressure in hPa for {city}          |\n",
    "| surfacePressure_hPa_{city}           | Surface air pressure in hPa for {city}          |\n",
    "| wind_speed_ms_{city}                 | Wind speed in m/s for {city}                    |\n",
    "| wind_direction_degree_{city}         | Wind direction in degrees for {city}            |\n",
    "| T_temperature_C_{city}               | Temperature in °C for {city}                    |\n",
    "| humidity_Percent_{city}              | Humidity in % for {city}                        |\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Entsoe Energy Market Data\n",
    "In a manner analogous to the operation of individual balancing groups, which are responsible for balancing the production and consumption of electricity to ensure the stability of the grid, electricity can be also traded and balanced between countries.\n",
    "A substantial number of European countries are participants in the SMARD exchange system, as discussed by Ortner and Totschnig (2019). Consequently, the cost of electricity in Germany is influenced by the cross-border flow of electricity with neighboring countries and their respective energy consumption and production.\n",
    "The load of energy transmission lines, the energy generation forecast and physical energy crossborder flows between Germany and its neighbours therefore promise to be relevant deciding factors of the day-ahead-price.\n",
    "Thus, we identified ENTSO-E as a valuable information source.\n",
    "The ENTSO-E (European Network of Transmission System Operators for Electricity) Transparency Platform, launched in 2015, serves as Europe's central hub for electricity market data.\n",
    "Created to fulfill EU Regulation 543/2013, which mandates electricity market operators to publish standardized data about generation, transmission, and consumption through a central European transparency platform, it collects and publishes data from over 42 transmission system operators across Europe. The platform transformed what was once a fragmented landscape of national data sources into a unified repository, providing crucial information about power generation, consumption, and cross-border flows. For market participants and analysts, this data source is invaluable as it offers insights into the fundamental drivers of electricity prices, including generation mix, grid constraints, and demand patterns.\n",
    "When querying the data, we had to deal with a change of the german bidding zone in 2018. A bidding zone is an area in which the energy price is traded at the same price. The german market underwent a significant structural change on October 1, 2018, when the joint German-Austrian-Luxembourg bidding zone (DE_AT_LU) was split. This combining data from both the old (DE_AT_LU) and new (DE_LU) market configurations, by retrieving these datasets separately and then merging them.\n",
    "In total, we fetched the following data from the ENTSO-E platform:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Day-ahead prices in Euro/MWh\n",
    "- Load forecast in MWh (prediction of electricity demand based on historical patterns and factors like weather, climate, and socioeconomic conditions)\n",
    "- Generation forecast in MWh (estimate of total scheduled net electricity production per bidding zone)\n",
    "- Wind and solar generation forecasts in MWh\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "day_ahead_prices = pd.read_csv('merged_data/data_collection/day_ahead_prices.csv')\n",
    "print(day_ahead_prices.head())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "load_forecast = pd.read_csv('merged_data/data_collection/load_forecast.csv')\n",
    "print(load_forecast.head())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "generation_forecast = pd.read_csv('merged_data/data_collection/generation_forecast.csv')\n",
    "print(generation_forecast.head())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "intraday_wind_solar_forecast = pd.read_csv('merged_data/data_collection/intraday_wind_solar_forecast.csv')\n",
    "print(intraday_wind_solar_forecast.head())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "day_ahead_wind_solar_forecast = pd.read_csv('merged_data/data_collection/day_ahead_wind_solar_forecast.csv')\n",
    "print(day_ahead_wind_solar_forecast.head())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Stock Market Resource Data\n",
    "Since our energy consumption cannot yet be fully satisfied with renewable energy sources, there is still a need for fossil-based energy production and nuclear power. To reflect this in our dataset, resource prices on the market need to be considered. Trading in the stock market is not limited to company stocks but also includes resources. Resources like oil, natural gas, coal, and uranium are important for energy production and were therefore chosen for examination. These resources do not have static pricing and are listed and traded on the stock market. Their prices fluctuate, and the daily closing price is used for the calculations. It could be speculated that hourly values might yield better results, but sources for these values could not be procured and were therefore not considered. The stock market data also has the highest rate of missing values, as there are no values for weekends and bank holidays when the stock markets are closed.\n",
    "\n",
    "The selected data is presented in US Dollars for the resources: Oil WTI, Natural Gas, Coal, and Uranium. The daily closing values were chosen and replicated for each hour of the day."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Oil WTI: US Dollars per Barrel\n",
    "- Natural Gas: US Dollars per MMBtu\n",
    "- Coal: US Dollars per Ton\n",
    "- Uranium: US Dollars per 250 Pounds"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "tail_res = pd.read_csv('../final-submission/merged_data/data_collection/merged_data.csv')\n",
    "print(tail_res.tail())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Carbon Emission Futures\n",
    "\n",
    "Carbon emission futures data represents the market price of permits that companies must purchase to cover their carbon dioxide emissions under cap-and-trade systems such as the European Union Emissions Trading System (EU ETS) [European Commission, EU ETS, 2024].\n",
    "\n",
    "These permits, also known as emission allowances, are traded in financial markets, where contracts are bought and sold for delivery at a specific future date and with prices influenced by factors such as regulatory policies, market demand, and emission reduction targets. The maturity dates are set at quarterly intervals to provide market participants with the opportunity to manage their compliance obligations and trade allowances across various timeframes.\n",
    "\n",
    "The data provides a quantitative measure of the cost of carbon emissions while each allowance grants the holder the right to emit one metric ton of carbon dioxide (CO₂) or its equivalent in other greenhouse gases.\n",
    "\n",
    "\n",
    "Since carbon costs directly impact electricity generation expenses, especially for fossil fuel-based power plants, fluctuations in emission futures prices can influence day-ahead energy prices, making them a relevant feature for predictive modeling in this project. \n",
    "Incorporating carbon emission futures as a feature allows the model to reflect the economic and regulatory pressures that shape energy production costs, capturing a critical aspect of market dynamics.\n",
    "The data was sourced from Investing.com and specifically utilizes the Carbon Emissions Futures Historical dataset [Investing, Carbon Emissions Futures, 2024]. It consists of daily records spanning from 2015 to the present, offering a consistent and detailed historical dataset for analysis.\n",
    "\n",
    "A preprocessing step was performed to align the dataset with the hourly format of day-ahead prices by assigning the same daily carbon emission futures price to all 24 hours of the corresponding day.\n",
    "\n",
    "The processed dataset includes a column ```carbon_price_EURO```, which assigns the daily carbon futures price in euros to each hour within the \"Date\" column."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pd.read_csv('../final-submission/merged_data/data_collection/carbon.csv')[22:].head()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Major Social Events\n",
    "\n",
    "Major social events in Germany such as \"Carnival\", \"Oktoberfest\", \"Berlinale\", and the \"Super Bowl\" were included in this analysis.\n",
    "These events, while not occurring daily, can lead to significant variations in energy consumption, particularly in terms of electricity usage in public spaces, transportation, and entertainment venues. For example, during Carnival or Oktoberfest, there is often an uptick in energy demand in cities like Cologne or Munich, due to increased activity in hospitality and public services. Similarly, the Berlinale film festival or the Super Bowl can lead to higher energy consumption driven by larger gatherings and public viewing events.\n",
    "\n",
    "By incorporating major social events as a feature, the model is trained to be able to account for these irregular, yet predictable, fluctuations in demand. The goal is to improve the accuracy of day-ahead price forecasting by capturing the non-seasonal, event-driven dynamics that can significantly impact energy markets.\n",
    "\n",
    "The specific dates of these events from 2015 to 2025 were manually derived based on historical records and scheduled occurrences. Each event is represented as an independent binary feature (1 if the event occurs, 0 otherwise), allowing the model to clearly distinguish events while allowing multiple events to be represented on the same day."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pd.read_csv('../final-submission/merged_data/data_collection/major_social_events.csv')[62398:].head()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Covid Pandemic Data\n",
    "\n",
    "The COVID-19 Pandemic was a very impactful and disruptive event in recent history.\n",
    "The pandemic and especially governmental containment and prevention measures had a high influence on human behaviour and the economy, therefore exerting a strong influence on energy demands.\n",
    "For instance the resulting shutdown of the economy implied a halt to production facilities and gastronomy, and the recommendation to not leave your home meant the private homes of people, who would otherwise be out for most of the day, had to be temperature regulated during day hours, whereas they might not have used their home AC or heating while at work.\n",
    "\n",
    "While certain social and economic repercussions of measures in the past are still evident today, the direct influence of political actions on energy demands is assumed to be limited to the time in which respective actions were active.\n",
    "These actions however differed in time and extent in the federal states, and a suitable dataset needs to represent that.\n",
    "Therefore, a dataset containing different types of political actions in the federal states and their corresponding time frames is needed.\n",
    "Such a dataset can be found as part of a publication containing daily distinctions of 16 types of political measures, each rated on a severity scale of 0-2 (\"free\" to \"fully restricted\"), and listed for each of the 16 federal states of Germany. [Steinmetz et al., 2022]\n",
    "\n",
    "This dataset required preprocessing to be adjusted for our use case. Political measures were therefore each valued with factors of 0, 1 or 2 to eliminate measures with no or unlikely relation to energy demand, for example the recommendation to keep a distance of 1.5m to other persons, or weigh them accordingly if a strong influence can be assumed, for example closure of kindergartens and daycare possibly forcing workers to stay at home.\n",
    "Furthermore, the weighted political actions were multiplied by the population percentage residing the according federal state, to adjust for the amount of people affected.\n",
    "On days with no active political measures affecting energy demands, or on days that were not part of the dataset, the value 0 was filled in.\n",
    "The resulting daily index number was then used to create an hourly dataset by filling in all 24 hours with the daily value.\n",
    "\n",
    "The processed dataset now contains a column \"Covid factor\" with this daily index number, corresponding to every hour and date in the \"Date\" column."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pd.read_csv('../final-submission/merged_data/data_collection/covid.csv')[45430:].head()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### News Embeddings\n",
    "\n",
    "In Germany's energy market, predicting day-ahead electricity prices requires considering multiple market influences. News articles could provide valuable context for these predictions, and embedding models offer a way to utilize this information. Previous research has shown success with news embeddings in financial markets [Picasso et al. 2019] and UK energy markets [Yun Bai et al. 2024].\n",
    "\n",
    "Day-ahead prices are influenced by factors often reported in news media, including power plant outages, policy changes, international market developments, and geopolitical events. To handle the large volume of daily articles, we proposed generating embeddings to convert this unstructured information into structured vectors.\n",
    "\n",
    "We used the \"all-MiniLM-L6-v2\" embedding model, which creates 384-dimensional vectors, later reduced to 8 dimensions using PCA for faster experimentation. For data sources, we evaluated NewsAPI and The Guardian's API, querying articles with energy-related keywords. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Inspection of embeddings\n",
    "\n",
    "guardian_embeddings = pd.read_csv('merged_data/data_collection/guardian_embeddings.csv', parse_dates=['timestamp'])\n",
    "newsapi_embeddings = pd.read_csv('merged_data/data_collection/newsapi_embeddings.csv', parse_dates=['timestamp'])\n",
    "day_ahead_prices = pd.read_csv('merged_data/data_collection/day_ahead_prices.csv', parse_dates=['timestamp'])\n",
    "\n",
    "print(\"Guardian embeddings example:\")\n",
    "print(guardian_embeddings.tail())\n",
    "\n",
    "print(\"NewsApi embeddings example:\")\n",
    "print(newsapi_embeddings.tail())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Discarded Ideas\n",
    "####  GDP And Inflation\n",
    "An example of an idea that was considered but did not end up in the final solution is the inclusion\n",
    "of GDP (Gross Domestic Product) and inflation data in the calculations. The challenge encountered\n",
    "with these data sets is that most sources for inflation and GDP only publish results annually, meaningonly a single value per year would be available in our dataset. The impact of such a value on our final prediction would therefore be negligible, as other data sources, like stock market information, likely already contain the relevant information to some degree. Furthermore, an earlier section visualizing the SMARD data showed just how strongly the surge in electricity prices in recent years surpassed\n",
    "inflation. Additionally, a selection of countries would need to be made, and as with high energy\n",
    "usage companies, it would be unclear whether to use data from global, european, or a smaller set\n",
    "of countries.\n",
    "\n",
    "#### Company Stocks\n",
    "Initially an idea was to include the prices of company stocks with high energy usage, but this was discarded as stock prices are not directly tied to energy prices. Additionally, selecting which companies to include would have been challenging, as it would require deciding whether to focus on global, european, or only german companies.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Merging\n",
    "\n",
    "In our workflow, different groups within the team gathered their respective portions of the data. While this approach enabled us to collect more data, it also required unifying the various datasets into a common format. The primary challenge was the varying time intervals at which data points were available. For instance, some values were updated every 15 minutes, while others were only available once per day. We decided to use hourly values, as this would match the frequency of the final forecast. Data points that provided only one value per day would use that value for every hour of the day. Additionally, on days when certain data points had no values, the corresponding entries were left blank. Although different methods for filling in missing data were considered, this approach was deemed the best, as the frequency and duration of missing values varied vastly across data points. In instances where data points were available at intervals smaller than hourly steps, we decided to use the value from the full hour. Other methods, such as using the average over the hour, were considered but discarded, as this would have required a considerable number of checks across all data points. This could be interesting to explore in future work.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Cleaning\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### SMARD-Data Preprocessing\n",
    "The SMARD-Dataset contains many different variables provided by smard.de (SMARD-dataset explained in more detail in Section <ins>&#8594;SMARD Electricity Data</ins>). However the data is only available in the form of multiple different tables in different files, therefore important steps of preprocessing and merging are needed to actually use the data and make it compatible with the other non-SMARD data sources.\n",
    "At first, the names of the columns have to be changed to differentiate them. The reason for that lies in the way the data is provided; e.g., 'price actual' and 'price forecast' are not stored in the same table and have to be downloaded separately but are both initially just named 'price'.\n",
    "Therefore, the actual name is only implied by the download source, and after a merge of the different tables, it would lead to a name collision.\n",
    "To solve this issue, every column name gets extended with an explicit prefix.\n",
    "\n",
    "Another required preprocessing step is to deal with \"not a number\" (short, NaN) values because some of our models cannot work with this data type but also because some variables are missing too many values, which has a negative impact on the quality in terms of correlation for said variables.\n",
    "\n",
    "Two popular options to deal with empty values are:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Fill NaN with the integer value 0.\n",
    "- Fill NaN with the mean of the corresponding column.\n",
    "\n",
    "The impact of both methods is compared in section <ins>&#8594; Data Analysis</ins>\n",
    "\n",
    "Dealing with DateTime type values can also be a challenge because, e.g., linear regression, which can be used to further examine properties of the gathered data, cannot work with the data type out of the box. A workaround for this problem is to encode the different segments, like days, hours, etc., to integer values, each representing one part of the time format."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Preprocessing For LSTM Based Models"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Since among other models we are using LSTM's for forecasting, the dataset format needs to fulfill certain conditions. The feature and target sequences needs to be continuous in time. This means the absence of NaN-values. Therefore, several preprocessing steps are applied to ensure this condition is satisfied.\n",
    "\n",
    "1. Every feature, that contain at least one consecutive sequence of NaN-values longer that 168hours or 7days are dropped from the dataset.\n",
    "2. For every feature that is not dropped by step (1), the NaN-values are going to be replaced.\n",
    "\n",
    "This follows the assumption that it is hard to replace NaN-values from very long NaN-sequences. The replacing is then done by training a XGBoost Regressor model which predict the missing values. This model is trained on the features of the dataset which will not be replaced and the feature containing the NaN-values as target. That's done for every such feature one after another."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# create dataframe for lstm models\n",
    "start_ts = pd.Timestamp('2015-01-05 00:00:00')\n",
    "end_ts = pd.Timestamp('2024-11-30 00:00:00')\n",
    "nan_streak_th = 168     # 24h * 7 = 7days\n",
    "lstm_df = run_preprocessing(df=merge_big, nan_streak_threshold=nan_streak_th, start_ts=start_ts, end_ts=end_ts)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "When only using categorical encoded features for time-related features like the month, day of week or hour they show no cyclicity. However, there is clearly cyclic behaviour contained in them. It makes sense that the connection between two consecutive hours like 23h and 0h is stronger than between 0h and 5h. The same applies to month and day of the week as well.\n",
    "To incorporate this cyclic behavior of time-related features, they were encoded using trigonometric functions. To achieve this, each such feature gets transformed into two: one sine and one cosine feature.\n",
    "(Code source: https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "def sin_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "\n",
    "\n",
    "def cos_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "lstm_df[\"hour\"] = lstm_df[\"Date\"].dt.hour   \n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# trigonometric encoding of time realted features\n",
    "lstm_df['hour_sin'] = sin_transformer(24).fit_transform(lstm_df['hour'].values)\n",
    "lstm_df['hour_cos'] = cos_transformer(24).fit_transform(lstm_df[\"hour\"].values)\n",
    "\n",
    "lstm_df['day_of_week_sin'] = sin_transformer(7).fit_transform(lstm_df['weekday'].values)\n",
    "lstm_df['day_of_week_cos'] = cos_transformer(7).fit_transform(lstm_df[\"weekday\"].values)\n",
    "\n",
    "lstm_df['month_sin'] = sin_transformer(12).fit_transform(lstm_df['month'].values)\n",
    "lstm_df['month_cos'] = cos_transformer(12).fit_transform(lstm_df[\"month\"].values)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "LSTM based models should be able to learn from past day ahead prices. To achieve this the values of t - 24h are added as features to the dataset.\n",
    "With this, the previous day can be taken as input sequence to predict the upcoming day ahead prices."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prev_day_prices = lstm_df[:-24]['day_ahead_prices_EURO'].values\n",
    "lstm_df = lstm_df[24:]\n",
    "lstm_df['prev_range_prices'] = prev_day_prices"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Missing Data\n",
    "\n",
    "During data analysis, the separate columns of the final data frame were checked for missing data, or more accurately, empty entries. These missing values can have various causes. For example, in the stock market data, there are no values for weekends or holidays.\n",
    "\n",
    "The columns with the highest percentage of missing values were, as expected, the stock market data, with missing values ranging from nearly 50% for uranium to almost 31% for coal. Fortunately, the other columns were more complete, with data points for special occasions and calendar observations (e.g., day of the week) having 0% missing values.\n",
    "A selection of the final data set and their missing values in percent are given below.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| Data Point                                      | Percent Missing                        |\n",
    "|---------------------------------------------|--------------------------|\n",
    "| Uran                                        | 49.78       |\n",
    "| Actual Aggregated                           | 37.89       |\n",
    "| Oil WTI                                     | 31.22        |\n",
    "| Natural Gas                                 | 31.36        |\n",
    "| Coal                                        | 30.63       |\n",
    "| sunshine_min_Muenchen_review                | 25.36       |\n",
    "| actual_generation_E_Nuclear_MWh             | 10.52        |\n",
    "| Forecasted Load                             | 2.22        |\n",
    "| carbon_price_EURO                           | 2.24       |\n",
    "| day_ahead_prices_EURO                       | 1.08       |\n",
    "| wind_speed_ms_Hamburg_review                | 1.09       |\n",
    "| T_temperature_C_Muenchen_review             | 0.78       |\n",
    "| actual_E_Total_Gridload_MWh                 | 0.79       |\n",
    "| actual_E_Residual_Load_MWh                  | 0.79       |\n",
    "| actual_generation_E_Biomass_MWh             | 0.79       |\n",
    "| actual_generation_E_Hydropower_MWh          | 0.79       |\n",
    "| actual_generation_E_Windoffshore_MWh        | 0.79       |\n",
    "| actual_generation_E_Windonshore_MWh         | 0.79       |\n",
    "| actual_generation_E_Photovoltaics_MWh       | 0.79       |\n",
    "| actual_generation_E_OtherRenewable_MWh      | 0.79       |\n",
    "| actual_generation_E_Lignite_MWh             | 0.79       |\n",
    "| actual_generation_E_HardCoal_MWh            | 0.79       |\n",
    "| actual_generation_E_FossilGas_MWh           | 0.79       |\n",
    "| actual_generation_E_HydroPumpedStorage_MWh  | 0.79       |\n",
    "| actual_generation_E_OtherConventional_MWh   | 0.79       |\n",
    "| month                                       | 0.0                      |\n",
    "| weekday                                     | 0.0                      |\n",
    "| week_of_year                                | 0.0                      |\n",
    "| is_weekend                                  | 0.0                      |\n",
    "| is_holiday                                  | 0.0                      |\n",
    "| timestamp                                   | 0.0                      |\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "\n",
    "In this chapter, the different sources of data used were explained. Weather, stock market prices, holidays, special events, and COVID-19 pandemic data were sourced, and their usage and possible improvements for future work were discussed. The data was analyzed from different perspectives, such as correlation, missing values, and feature importance in the models used."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualization And Story Telling"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The topic of this chapter is the models used for forecasting. The theory behind the models—Linear Regression, Long-Term Short-Term Memory (LSTM), Chronos, and Temporal Fusion Transformer—is explained in detail. At the end, the framework AutoGluon is introduced, where multiple models are trained simultaneously. Approaches for improving the AutoGluon forecasts are also described."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Analysis\n",
    "#### SMARD-Data:\n",
    "The SMARD Data for Germany contains hourly prices ranging from the first of January 2015 to the present day, making up more than 81'000 data points that will form the basis of our future predictions.\n",
    "As the final target of our prediction, the day ahead price featured in the SMARD Data is of special interest to us.\n",
    "Therefore, an initial visual analysis and basic exploration of the data was conducted at the project's inception. This preliminary investigation was intended to provide a clearer understanding of the project's subject matter and to potentially identify key aspects warranting further inspection in later chapters.\n",
    "\n",
    "##### Initial Observations\n",
    "Looking at the price given in €/MWh, various interesting observations can be made:\n",
    "Firstly, negative electricity prices exist. This occured over 1600 times, with the price being exactly 0€ around 65 times. This prompted a further investigation into into the formation of electricity prices. Negative wholesale electricity prices are a phenomenon that arises when high and inflexible electricity generation, frequently from renewable sources such as wind and solar, coincides with low demand, particularly during periods of low consumption, such as holidays. During these times, producers have to pay for offloading electricity onto the grid that exceeds demand, resulting in negative prices. This typically occurs when renewable generation exceeds demand, as evidenced by the case of Easter Monday 2019 in Germany, where the country's entire electricity needs were met by renewable energy production.\n",
    "Further information can be found on the smard website itself [Smard, Negative wholesale prices, 2025].\n",
    "\n",
    "\n",
    "The lowest recorded price in the data set was -500€, occuring at 2022-08-23 20:00:00. The second lowest price with -399€ was achieved the same day. These are stark outliers, with only 13 entries below the -100€ mark. The sum being exactly 500.00€ in particular stands out as peculiar.\n",
    "Finally, the highest recorded price in the data set was 1047.11€, occurring at  2023-07-02 14:00:00. In total, only 5 entries surpass the 900€ mark, all of them past 2023.\n",
    "An initial plotting of the entire data immediately draws attention to the uniformity of the graph prior to the second half of 2021 compared to the much larger variance in the latter part of the diagram. Increased volatility might be attributed to various factors previously mentioned: As more volatile renewable energy sources make up a larger share of the total energy production, the influence of relevant weather factors gets amplified. \n",
    "In February 2022, Russia's full-scale invasion of Ukraine began, influencing various fuel prices by causing uncertanty in the markets.\n",
    "As part of the efforts to shift away from fossil fuels, Germany has also created legislation incentivising and mandating a transformation towards electrical heating. Exemplary for this is the \"Gebäudeenergiegesetz\", mandating the usage of heat pumps in certain conditions from 2024 onwards and offering financial supports for home owners seeking to change their heating. \n",
    "It is therefore likely that with the increase of electrical heating, bad weather conditions impact the electricity price more severely while also increasing demand to heat buildings.\n",
    "Contrary, on days with good weather conditions, a lessened demand for heating meets higher production of renewables.\n",
    "\n",
    "![Overwiev of dayahead price development](src/image.png)\n",
    "\n",
    "However, at this scale, the data might not be very indicative. To gain further insight, a simple decomposition is done on the span of the last four weeks.\n",
    "Here, the seasonal window clearly shows each day, with 28 distinct repeating patterns, showcasing the individual days and indicating a strong daily cycle. The trend window on the other hand, indicates a lowered prices on weekends.\n",
    "\n",
    "![Decomposition of dayahead prices into seasonal patterns, trend window and residual](src/image-2.png)\n",
    "\n",
    "A plot of the electricity price throughout the course of the day reveals two peaks, occurring at approximately 6 a.m. and 5 p.m., respectively. Additionally, the price reaches its lowest point during the night, at 2 a.m. This indicates a pronounced decline in energy consumption, as the production of energy from solar sources is also reduced during nocturnal hours. Conversely, solar production is most active between the hours of 11 a.m. and 4 p.m.\n",
    "\n",
    "![Distribution of electricity price per hour of the day since 2015](src/image-3.png)\n",
    "\n",
    "Secondly, let us examine the daily trend over the course of a week. By averaging the values of the entire data set per weekday, it becomes evident that there is a clear downward trend in prices over the weekend, with Sunday in particular being a low point. This can be speculated to be related to lowered consumption.\n",
    "\n",
    "![Distribution of electricity price per weekday since 2015](src/image-4.png)\n",
    "\n",
    "Averaging the months of the year, a further difference is visible. This can be speculated to both pertain to temperatures and therefore heating and weather influences.\n",
    "\n",
    "![Distribution of electricity price per month since 2015](src/image-7.png)\n",
    "\n",
    "Averaging the individual years, a striking spike during the year 2022 stands out.\n",
    "\n",
    "![Distribution of electricity price per year since 2015](src/image-5.png)\n",
    "\n",
    "This might both be due to factors previously outlined at the beginning of the section, however, that would not sufficiently explain the reduction of prices in subsequent years. A first idea might be that a slight increase due to inflation is to be expected, however the spike clearly supersedes the inflation of 7,9% that year. [Finanztools, Inflationsraten Deutschland, 2025]\n",
    "\n",
    "![Average change in price compared to the previous year since 2016](src/image-6.png)\n",
    "\n",
    "It is therefore likely, that the start of Russia's invasion of Ukraine heavily influenced the prices by driving up fossil fuel prices due to sanctions and stop of imports. Later that year the explosions at the Nord-Stream-Pipelines occured.\n",
    "These geopolitical events and their complex interconnected ramifications also influenced prices in neighbouring countries and thereby the intra-border flow.\n",
    "Inside Germany, the shift towards renewable energy sources was accelerated and the usage of gas heavily disincentiviced through pricing in order to keep gas storages filled for winter, to enable Germans to continue to heat.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Feature Selection\n",
    "An important step in the process of finding the right features to train the model is to examine the correlation between the variables in the given dataset. Since there are many different features, a heatmap plot is a good way to display them all while still providing a practical overview.\n",
    "\n",
    "To get a more precise feature selection, the Python library pandas provides a .corr() function which can be used to get the values satisfying a chosen condition, e.g., getting the ten most correlating values relating to 'price'."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#loada dataframe from: /merged_data/data_collection/smard.csv\n",
    "\n",
    "#for local testing: '/home/user/ai-project/push_to_final_submission/final-submission//merged_data/data_collection/smard.csv'\n",
    "a_smard_df = pd.read_csv('merged_data/data_collection/smard.csv')\n",
    "a_smard_df=a_smard_df[a_smard_df.duplicated(keep=False) == False]\n",
    "\n",
    "\n",
    "\n",
    "#dataslice with fewest NaN entries\n",
    "a_smard_df_slice = a_smard_df[(pd.to_datetime(a_smard_df['Date'])>=pd.to_datetime('2022-01-31 00:00:00')) & \n",
    "(pd.to_datetime(a_smard_df['Date'])<=pd.to_datetime('2022-05-31 00:00:00'))]\n",
    "\n",
    "a_smard_df = a_smard_df.drop(columns=['Date','End_Date'])\n",
    "a_smard_df_slice = a_smard_df_slice.drop(columns=['Date','End_Date'])\n",
    "\n",
    "#plot Abb. 1\n",
    "a_smard_df_corr=a_smard_df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.title(\"Correlation between features (fig. 1)\")\n",
    "sns.heatmap(a_smard_df_corr, vmin=-1, vmax=1, center=0, cmap='vlag')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This already provides interesting insights into the data and the impact of the variables. Some are shown as an almost complete white line, which means that they do not have significant correlation with any other variable.\n",
    "This can be caused by multiple reasons, e.g., the variable is missing too many values or the variable has simply no impact on the other ones.\n",
    "The more saturation the colour of a square has, the more two variables correlate with each other. Blue implies a negative direction (values rise/fall in opposite directions) and red a positive one (values rise/fall in the same direction).\n",
    "Below in figure a) and figure b), the different strategies \"filling with mean\" and \"filling with zero\" get compared and visualized.\n",
    "\n",
    "The usage of only a selected part of the data set seems to change the correlation between the \"Price\" and the selected variable \"af_Activation_Price_Minus_MWh\". This specific variable got further examined since it seems to be one of the most important features in terms of correlation to the price, at least in the smard dataset. The importance is shown in table c)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mean_df = a_smard_df.fillna(a_smard_df.mean())\n",
    "mean_df_slice = a_smard_df_slice.fillna(a_smard_df_slice.mean())\n",
    "\n",
    "zero_df = a_smard_df.apply(lambda col: col.fillna(0), axis=0)\n",
    "zero_df_slice = a_smard_df_slice.apply(lambda col: col.fillna(0), axis=0)\n",
    "\n",
    "plt.scatter(x = mean_df['af_Activation_Price_Minus_EUR_MWh'], y = mean_df['Price_Calculated_EUR_MWh'], color='blue', alpha=0.4, label='mean')\n",
    "plt.scatter(x = zero_df['af_Activation_Price_Minus_EUR_MWh'], y = zero_df['Price_Calculated_EUR_MWh'], color='green', alpha=0.4, label='zero')\n",
    "\n",
    "plt.xlabel(\"af_Activation_Price_Minus_EUR_MWh\")\n",
    "plt.ylabel(\"Price_Calculated_EUR_MWh\")\n",
    "plt.legend()\n",
    "plt.title(\"Correlation if NaN is filled with mean/zero on the complete time frame; fig. a)\")\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.scatter(x = mean_df_slice['af_Activation_Price_Minus_EUR_MWh'], y = mean_df_slice['Price_Calculated_EUR_MWh'], color='blue', alpha=0.4, label='mean')\n",
    "plt.scatter(x = zero_df_slice['af_Activation_Price_Minus_EUR_MWh'], y = zero_df_slice['Price_Calculated_EUR_MWh'], color='green', alpha=0.4, label='zero')\n",
    "\n",
    "plt.xlabel(\"af_Activation_Price_Minus_EUR_MWh\")\n",
    "plt.ylabel(\"Price_Calculated_EUR_MWh\")\n",
    "plt.legend()\n",
    "plt.title(\"Correlation if NaN is filled with mean/zero for a smaller selected time frame; fig b)\")\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Another method to compare the two methods is to print out the variables with the highest correlation after the corresponding changes to the dataset. This comparison was made on the complete time frame.\n",
    "The following table shows which variables have the highest correlation with the variable \"Price_Calculated_EUR_MWh\".\n",
    "\n",
    "Table c)\n",
    "\n",
    "| Ranking (descending) | Dataset with NaN values            | Dataset with NaN values replaced by mean | Dataset with NaN values replaced by 0      |\n",
    "|----------------------|------------------------------------|------------------------------------------|--------------------------------------------|\n",
    "| 1                    | af_​Activation_​Price_​Plus_​EUR_​MWh   | af_​Activation_​Price_​Plus_​EUR_​MWh         | Net_​Income_​EUR                           |\n",
    "| 2                    | af_​Activation_​Price_​Minus_​EUR_​MWh  | Net_​Income_​EUR                           | Balancing_​Services_​Calculated_​EUR       |\n",
    "| 3                    | mf_​Activation_​Price_​Plus_​EUR_​MWh   | af_​Activation_​Price_​Minus_​EUR_​MWh        | E_​NorwayImport_​MWh                       |\n",
    "| 4                    | Net_​Income_​EUR                     | af_​E_​Volume_​Activated_​Minus_​MWh          | E_​NorwayImport_​corssBorderPhysical_​MWh  |\n",
    "\n",
    "The results show that the choice between filling with zero or filling with the mean can have an impact on the data.\n",
    "\n",
    "However, more significant is the difference between the usage of a certain smaller section containing much less empty values instead of just using everything that is available. \n",
    "Therefore, the priority should be to use less but complete data instead of a dataset with big gaps."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Comparing Feature Importances\n",
    "Since our data collection resulted in 157 features, the influence each data point has on the final prediction is unknown. In this part of the report, different methods of calculating these feature importances are presented with their results.\n",
    "\n",
    "#### AutoGluon\n",
    "To find out which features AutoGluon focuses on, the final AutoGluon model was analyzed using Permutation Feature Importance. Feature importance is calculated by shuffling the values of a feature and observing whether the model’s performance declines. In the plot below, the feature importance of the best model is shown. The values for importance are uncharacteristically low and do not provide an accurate view of the actual feature importance in our model, as only one of the various models in the ensemble can use all of the features. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"temp/feature_importance.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Sort the DataFrame by 'importance' from lowest to highest\n",
    "df = df.sort_values(by='importance')\n",
    "\n",
    "# Set up the figure and axes\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Determine colors: red for negative values, blue for positive values\n",
    "colors = ['red' if val < 0 else 'blue' for val in df['importance']]\n",
    "\n",
    "# Plot the 'importance' column with the first column as x labels\n",
    "plt.bar(df.iloc[:, 0], df['importance'], color=colors, alpha=0.7)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Variable')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance')\n",
    "\n",
    "# Rotate the x labels for better readability\n",
    "plt.xticks(rotation=90, ha='right')\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Temporal Fusion Transformer\n",
    "\n",
    "One of the benefits of the TFT model for time series forecasting is the fact that it allows for interpretation mostly in the form of feature importance for both encoder and decoder. Moreover, given its embedded multi-head attention mechanism, the level of attention as a function of the time index for the pastime horizon can also be analyzed.\n",
    "\n",
    "##### Attention\n",
    "\n",
    "<img src=\"src/attention.png\" alt=\"Alt text\" title=\"Title\" width=\"500\" />\n",
    " It can be observed that most of the model attention takes place at the morning and evening times of the past horizon (typically higher electricity prices), as well as in the midday (typically lowest electricity prices).\n",
    "\n",
    "##### Encoder Importance\n",
    "\n",
    "<img src=\"src/encoder_variables_importance.png\" alt=\"Alt text\" title=\"Title\" width=\"500\" />\n",
    "\n",
    "Intuitively, electricity prices themselves represent the most important variable to be considered by the model at the time of prediction, namely when encoding the input data. Moreover, considering the fact that besides renewable sources, coal has the largest generation capacity in Germany, the price of this fossil fuel constitutes the second-largest importance for variable encoding.\n",
    "\n",
    "##### Decoder Importance\n",
    " <img src=\"src/decoder_variables_importance.png\" alt=\"Alt text\" title=\"Title\" width=\"500\" />\n",
    " Finally, at the decoding stage, most importance is regarded towards natural gas prices probably given its higher volatility in comparison with coal prices. Moreover, as is typical for system electricity demand, the time of day has also a high importance at the time of forecast generation.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Baseline Models Benchmark"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### LSTM Based Models"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "At first the dataset which is used by all the upcoming LSTM models is loaded. After loading, it is split into three intervals: training, testing, and validation."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_train, y_train, X_test, y_test, X_val, y_val = train_test_val_split(lstm_df, target_column='day_ahead_prices_EURO', \n",
    "                                                                      train_interval=[pd.Timestamp('2015-01-05 00:00:00'),\n",
    "                                                                                      pd.Timestamp('2023-11-30 23:00:00')], \n",
    "                                                                      test_interval=[pd.Timestamp('2024-06-01 00:00:00'),\n",
    "                                                                                     pd.Timestamp('2024-11-30 23:00:00')],\n",
    "                                                                      val_interval=[pd.Timestamp('2023-12-01 00:00:00'),\n",
    "                                                                                    pd.Timestamp('2024-05-31 23:00:00')])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### LSTM"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "An LSTM (Long Short-Term Memory)[Hochreiter et al. 1997] is a type of RNN (Recurrent Neural Network) designed to address the limitations of traditional RNNs, particularly in handling long-term dependencies in sequential data ([Bengio et al. 1993], [Schmidhuber et al. 2003]). When trying to learn relationships over extended sequences, LSTMs incorporate mechanisms that allow them to retain information over long periods. This makes them especially well-suited for tasks like time series forecasting, where understanding patterns and dependencies over time is crucial.\n",
    "To also incorporate multiple input features which might contribute to the day ahead prices, a multivariate LSTM is used. It takes an input sequence for each input feature and outputs a fixed length target sequence."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define model\n",
    "MultiLSTM = MultivariateBiLSTM(features=list(X_train.columns), target='day_ahead_prices_EURO')\n",
    "# train model\n",
    "training_history = MultiLSTM.train(X_train=X_train, y_train=y_train,\n",
    "                                X_val=X_val, y_val=y_val,\n",
    "                                X_test=X_test, y_test=y_test,\n",
    "                                n_epochs=200, batch_size=1024, learning_rate=0.001)\n",
    "# predict\n",
    "prediction_MultiLSTM = multiLSTM.run_prediction(X_test).set_index('timestamp')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Encoder Decoder LSTM"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Regular multilayer LSTM face several challenges, particularly in tasks that require a variable length in- or output sequence[Hochreiter et al. 1997]. In time series forecasting this is the case when the steps ahead which need to be predicted differ. Additionally, standard multilayer LSTM can suffer from information bottlenecks. This is because the entire input needs to be compressed to a fixed-sized context vector before generating the output which might lead to information loss especially for long input sequences[Sutskever et al. 2014]. Even though LSTM have the ability to handle the vanishing gradient problem better than normal RNN, very long sequences can still lead to a performance decrease ([Zhao et al. 2020], [Kandadi et al. 2025]).\n",
    "\n",
    "To address these issues, Seq2Seq (Sequence-to-sequence) models[Sutskever et al. 2014] were introduced. These models consist of two multilayer LSTM's. The first one, the Encoder, processes an entire sequence of input features at each timestamp. Meaning each block in the first layer handles one feature. Encoder output and hidden states are then passes to the second multilayer LSTM, the Decoder. Whose output is then passed through a linear layer to transform the output dimension to match the required output sequence length. As a final step and to ensure the data is in a range between 0 and 1 an sigmoid activation is applied.\n",
    "\n",
    "In the below code the Encoder Decoder LSTM is created. For this the use_attention is set to False, which applies equal weights to the encoder output."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define model\n",
    "encdecLSTM = EncoderDecoderAttentionLSTM(target_length=24, features=list(X_train.columns), target='day_ahead_prices_EURO',\n",
    "                                         hidden_size=256, num_layers=6, use_attention=False)\n",
    "# train model\n",
    "training_history = encdecLSTM.train(X_train=X_train, y_train=y_train,\n",
    "                                    X_val=X_val, y_val=y_val,\n",
    "                                    X_test=X_test, y_test=y_test,\n",
    "                                    n_epochs=1000, batch_size=2048, learning_rate=0.001)\n",
    "# predict\n",
    "prediction_encdecLSTM = encdecLSTM.predict(X=X_train, exp_dir=None).set_index('timestamp')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Incorparate Bahdanau Attention"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Seq2Seq models without attention face challenges like limited context representation and difficulty in capturing long-range dependencies. Instead of relying on a single fixed-length context vector, Bahdanau attention [Bahdanau et al. 2014] computes a weighted combination of input sequence elements for each output time step."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Attention Mechanism With Table](src/attention_graphic_with_table.png \"Attention Mechanism With Table\")"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The above described attention mechanism is used in the Encoder Decoder Attention LSTM as seen in the green Attention block in the graphic below."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Encoder Decoder Attention LSTM](src/enc_dec_att_LSTM_autoreg.png \"Encoder Decoder Attention LSTM\")"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "When using the attention mechanism, the context vector is created by calculating a weighted sum of the encoder's output, while without attention only the encoder’s output is used. This context vector then gets processed by the decoder to generate the output sequence step-by-step. Which allows the model to focus on the most relevant parts of the input sequence when predicting each target value. By leveraging this mechanism, the model can better handle complex temporal relationships. This significantly improves performance, especially for long sequences, as the model does not have to compress all information into a single bottleneck."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define model\n",
    "encdecattLSTM = EncoderDecoderAttentionLSTM(target_length=24, features=list(X_train.columns), target='day_ahead_prices_EURO',\n",
    "                                            hidden_size=256, num_layers=6, use_attention=True)\n",
    "# train model\n",
    "training_history = encdecattLSTM.train(X_train=X_train, y_train=y_train,\n",
    "                                       X_val=X_val, y_val=y_val,\n",
    "                                       X_test=X_test, y_test=y_test,\n",
    "                                       n_epochs=1000, batch_size=2048, learning_rate=0.001)\n",
    "# predict\n",
    "prediction_encdecattLSTM = encdecattLSTM.predict(X=X_train, exp_dir=None).set_index('timestamp')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Compare LSTM Models"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To evaluate which LSTM based model from the above three performs the best, the MAE is used as metrics."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# compare LSTM based models on test data\n",
    "BenchMaker = BenchmarkMaker(export_dir='result')\n",
    "BenchMaker.load_dataframes(predictions={'MultivarLSTM': prediction_MultiLSTM,\n",
    "                                        'EncDecAttLSTM': prediction_encdecLSTM,\n",
    "                                        'EncDecLSTM': prediction_encdecattLSTM}, prices=y_test)\n",
    "BenchMaker.calc_errors()\n",
    "\n",
    "BenchMaker.plot_compare_mae()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Compare LSTM](src/compare_LSTM_models_on_val.png \"Compare LSTM\")"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As seen in the above graphic is the MAE for the Encoder Decoder Attention LSTM model much lower than the error of the other two other LSTM based models. Especially against the Encoder Decoder LSTM without attention, it shows a much better performance, even though the models architectures are very similar. This is a good example on how good a model can get when incorporating an attention mechanism."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Chronos"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Chronos is a framework for pre-trained probabilistic time series models introduced by Ansari et al (2024). As can be seen in the figure below it tokenizes time series values into a fixed vocabulary through scaling and quantization and trains transformer-based large language models on these tokens. Chronos is designed without time-series-specific architecture except from the tokenization technique, resulting in a minimalist yet effective approach. Still, the framework achieved remarkable results in in-domain experiments and demonstrated competitive zero-shot performance, comparable to models specifically trained on similar tasks.\n",
    "\n",
    "![High-level depiction of Chronos copied from Ansari et al.](src/Chronos_Process.png \"Title\")\n",
    "\n",
    "Chronos is built on Google's T5 architecture, which was introduced by Vaswani et al. (2017) and has gained significant popularity since then. However, a key difference between Chronos and the original T5 architecture is the reduced vocabulary size of 4096 compared to 32128 of the original T5 architecture, which results in varying parameter counts. Chronos offers five distinct models, with sizes ranging from 8 million to 710 million parameters."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Chronos can further be optimized by fine-tuning it on additional data. To improve its performance in forecasting the electricity market we used the day-ahead electricity prices from ENTSO-E as a domain-specific dataset. The tiny model was chosen for its practicality, as it can be fine-tuned and utilized for forecasting tasks even on a standard laptop. To gain deeper insights into the impact of the dataset size and training steps on model performance, we conducted fine-tuning experiments in four distinct ways.\n",
    "\n",
    "As explained in [Section 3](#gathering-domain-knowledge) the day-ahead-prices for energy has experienced heightened volatility in recent years. To evaluate the impact of dataset characteristics on model performance, we divided the data into two subsets. The first dataset contains day-ahead prices from January 2022 to December 2023, and the second one data spanning from January 2015 to December 2023. The smaller dataset focuses primarily on recent, highly volatile market conditions, reflecting current dynamics. In contrast, the larger dataset spans a longer historical period, capturing a broader range of market scenarios. This approach enables a direct comparison to determine whether the smaller, more focused dataset enhances adaptability to recent volatility or if the larger dataset provides a more comprehensive foundation due to its diversity. Ansari et al. propose fine-tuning for 1000 steps and achieved remarkable results. In a single training step, a batch of sequences from the dataset is processed to optimize the model’s weights. The input consists of 32 sequences - the batch size and each sequence contains 512 tokens - the context length. Additionally, to the proposed 1000 training steps we investigated results for 10000 training steps. To determine which configuration performs best for the challenge of predicting 24-hour day-ahead energy prices, we conducted hourly forecasts day by day for an entire year, from the beginning of December 2023 to the end of November 2024. For each forecast day, the context data consisted of the most recent 512 hourly day-ahead prices."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The results are computed as following. For each 24 values of one day, we calculated the root mean squared error and the absolute error. For each day the mean of these values are calculated and over the whole year the mean is created again. The results for one year of the Chronos-T5 (Tiny) model and its fine-tuned versions are presented in the table below.\n",
    "\n",
    "|Chronos-T5 (Tiny)    |1. Zero-Shot  |2. Fine-Tuned Data: 2015 Steps: 1000|3. Fine-Tuned Data: 2015 Steps: 10000|4. Fine-Tuned Data: 2022 Steps: 1000|5. Fine-Tuned Data: 2022 Steps: 10000|\n",
    "|------------|------------|------------|------------|------------|------------|\n",
    "| RMSE              | 25.82  | <b>22.74</b>    | 23.50 | 24.55| 26.41 |\n",
    "| MAE               | 20.13  | <b>17.25</b>    | 17.97 | 18.59 | 20.26 |"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As can be seen in the results not all fine-tuned models outperform the baseline. Models fine-tuned on the larger dataset, which includes data from 2015, tend to perform better than those fine-tuned on the smaller, more recent dataset. This may be due to the fact that, although the energy market is currently highly volatile, less volatile days dominate the market, making the larger dataset, with its broader range of scenarios, more beneficial. Furthermore, models trained with 1,000 steps tend to outperform those trained with 10,000 steps, likely because the latter suffer from overfitting.\n",
    "\n",
    "We initially aimed to analyze the MAE percentage and RMSE percentage values. However, because the actual day-ahead price is close to zero on some days, these metrics become extremely high and less reliable. Therefore, we focus on the RMSE and MAE absolute values for a more reliable evaluation.\n",
    "\n",
    "The model which was fine-tuned on the large dataset with 1000 training steps performs the best. This model has the lowest RMSE and MAE. On average the difference between the predicted hourly day-ahead price and its actual value is 17.254 Euros.\n",
    "\n",
    "Model five is outperformed by the baseline in all metrics."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We use both the uploaded fine-tuned models and a pretrained model to forecast a defined period. A comparison of Model One and Model Two is visualized in the following two graphs. Each model forecasts the period from February 14, 2024, to February 23, 2024, which includes the challenge date—February 18—exactly one year earlier."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "start_time = \"2024-02-14 00:00:00\"\n",
    "end_time = \"2024-02-22 23:00:00\"\n",
    "\n",
    "model1 = \"amazon/chronos-t5-tiny\"\n",
    "forecast1 = chronosForecast(model1, start_time, end_time)\n",
    "\n",
    "model2 = \"juliushanusch/chronos-tiny-fine-tuned-day-ahead-prices\"\n",
    "forecast2 = chronosForecast(model2, start_time, end_time)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Set up the figure with two subplots (1 row, 2 columns)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot for model1 (forecast1)\n",
    "axs[0].plot(forecast1['timestamp'], forecast1['actual_value'], label='Actual', color='blue', linestyle='-', linewidth=2)\n",
    "axs[0].plot(forecast1['timestamp'], forecast1['forecasted_values'], label='Forecasted', color='orange', linestyle='--', linewidth=2)\n",
    "axs[0].plot(forecast1['timestamp'], forecast1['absolute_error'], label='Absolute Error', color='red', linestyle=':', linewidth=2)\n",
    "\n",
    "axs[0].set_xlabel('Timestamp')\n",
    "axs[0].set_ylabel('Value')\n",
    "axs[0].set_title(f'Actual vs Forecasted Values with Absolute Error \\n Model: {model1}')\n",
    "axs[0].legend()\n",
    "axs[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot for model2 (forecast2)\n",
    "axs[1].plot(forecast2['timestamp'], forecast2['actual_value'], label='Actual', color='blue', linestyle='-', linewidth=2)\n",
    "axs[1].plot(forecast2['timestamp'], forecast2['forecasted_values'], label='Forecasted', color='orange', linestyle='--', linewidth=2)\n",
    "axs[1].plot(forecast2['timestamp'], forecast2['absolute_error'], label='Absolute Error', color='red', linestyle=':', linewidth=2)\n",
    "\n",
    "axs[1].set_xlabel('Timestamp')\n",
    "axs[1].set_ylabel('Value')\n",
    "axs[1].set_title(f'Actual vs Forecasted Values with Absolute Error \\n Model: {model2}')\n",
    "axs[1].legend()\n",
    "axs[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axs[0].grid(True, linewidth=0.5, linestyle='--', alpha=0.7)\n",
    "axs[1].grid(True, linewidth=0.5, linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "# Adjust the layout to make sure there is no overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As previously mentioned, several sizes of Chronos models are available. In addition to the Chronos-T5 (Tiny) model, we evaluated the performance of the Chronos-T5 (Large) model. For this model, we applied the same fine-tuning steps as we did for the Chronos-T5 (Tiny) model.\n",
    "The Chronos-T5 (Large) model contains approximately 88 times more parameters compared to the tiny model and is therefore expected to perform better than the Chronos-T5 (Tiny) model. However, this comes at the cost of significantly higher computational requirements, both in terms of training and forecasting."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The yearly Results of the Chronos-T5 (Large) model and its fine-tuned versions are presented in the table below.\n",
    "\n",
    "|Chronos-T5 (Large)    |1. Zero-Shot  |2. Fine-Tuned Data: 2015 Steps: 1000|3. Fine-Tuned Data: 2015 Steps: 10000|4. Fine-Tuned Data: 2022 Steps: 1000|5. Fine-Tuned Data: 2022 Steps: 10000|\n",
    "|------------|------------|------------|------------|------------|------------|\n",
    "| RMSE | 22.45 | <b>21.05</b> | 22.07 | 22.74 | 22.30 |\n",
    "| MAE | 17.14| <b>15.85</b>| 16.75 | 17.18 | 16.95 |"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As expected, the large model performs better compared to the tiny models. We observe similar results in fine-tuning as seen with the tiny models. Once again, the models trained on the smaller dataset outperform those trained on the larger dataset. Interestingly, the non-fine-tuned Chronos-T5 (Large) model performs quite similarly to the Chronos-T5 (Tiny) model fine-tuned on the larger dataset with 1,000 training steps.\n",
    "\n",
    "Model two emerges as the best-performing model overall. It achieves an average difference of just 15.85 Euros."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The comparison of the large fine-tuned model number two with model number one is visualized in the graphs below."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model3 = \"amazon/chronos-t5-large\"\n",
    "forecast3 = chronosForecast(model1, start_time, end_time)\n",
    "\n",
    "model4 = \"juliushanusch/chronos-large-fine-tuned-day-ahead-prices\"\n",
    "forecast4 = chronosForecast(model2, start_time, end_time)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    \n",
    "\n",
    "# Set up the figure with two subplots (1 row, 2 columns)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot for model1 (forecast1)\n",
    "axs[0].plot(forecast3['timestamp'], forecast3['actual_value'], label='Actual', color='blue', linestyle='-', linewidth=2)\n",
    "axs[0].plot(forecast3['timestamp'], forecast3['forecasted_values'], label='Forecasted', color='orange', linestyle='--', linewidth=2)\n",
    "axs[0].plot(forecast3['timestamp'], forecast3['absolute_error'], label='Absolute Error', color='red', linestyle=':', linewidth=2)\n",
    "\n",
    "axs[0].set_xlabel('Timestamp')\n",
    "axs[0].set_ylabel('Value')\n",
    "axs[0].set_title(f'Actual vs Forecasted Values with Absolute Error \\n Model: {model3}')\n",
    "axs[0].legend()\n",
    "axs[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot for model2 (forecast2)\n",
    "axs[1].plot(forecast4['timestamp'], forecast4['actual_value'], label='Actual', color='blue', linestyle='-', linewidth=2)\n",
    "axs[1].plot(forecast4['timestamp'], forecast4['forecasted_values'], label='Forecasted', color='orange', linestyle='--', linewidth=2)\n",
    "axs[1].plot(forecast4['timestamp'], forecast4['absolute_error'], label='Absolute Error', color='red', linestyle=':', linewidth=2)\n",
    "\n",
    "axs[1].set_xlabel('Timestamp')\n",
    "axs[1].set_ylabel('Value')\n",
    "axs[1].set_title(f'Actual vs Forecasted Values with Absolute Error \\n Model: {model4}')\n",
    "axs[1].legend()\n",
    "axs[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axs[0].grid(True, linewidth=0.5, linestyle='--', alpha=0.7)\n",
    "axs[1].grid(True, linewidth=0.5, linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "# Adjust the layout to make sure there is no overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The two best performing fine-tuned models have been uploaded to huggingface and are available for use."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We demonstrated that fine-tuning the models can significantly improve the performance of certain models. To further enhance the Chronos models, hyperparameter optimization could be a promising approach. By performing such optimization, it is possible to identify the best-performing configuration for a specific use case."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Temporal Fusion Transformer"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Main Concepts\n",
    "\n",
    "Temporal Fusion Transformers (TFT) are a novel framework designed to address the complexities of multi-horizon time series forecasting. First introduced by Lim et al. in 2019 [Lim et al. 2019], TFT bridges the gap between high prediction accuracy and interpretability, making it uniquely suited for real-world applications where both performance and understanding are important. Traditional time series models often struggle to accommodate diverse data types or fail to provide insights into their decision-making processes. TFT resolves these challenges by integrating modern deep learning techniques with interpretable mechanisms.\n",
    "\n",
    "The model is particularly well-suited for datasets with mixed input types, such as historical data, future known covariates, and static features that do not vary with time. TFT excels in various domains, including finance, energy demand prediction, healthcare forecasting, and supply chain optimization, where data is abundant but understanding the temporal relationships and influences is critical. Its architecture is modular, enabling flexibility and extensibility while maintaining transparency in the decision-making process.\n",
    "\n",
    "#### Model Architecture\n",
    "\n",
    "The TFT architecture comprises several components that work together to extract meaningful patterns from complex temporal data. Below is a detailed explanation of its key building blocks:\n",
    "\n",
    "1. Variable Selection Layers:\n",
    "Variable selection layers dynamically identify and prioritize relevant features from the input data at each time step. This mechanism allows the model to adapt to changing conditions and reduces noise from irrelevant inputs, improving both accuracy and interpretability.\n",
    "\n",
    "2. Gating Mechanisms:\n",
    "To prevent overfitting and enhance robustness, gating mechanisms selectively control the flow of information through the network. By suppressing irrelevant or redundant components, these gates ensure that only the most informative features contribute to the forecast.\n",
    "\n",
    "3. Sequence-to-Sequence Layer:\n",
    "TFT employs recurrent layers, such as Long Short-Term Memory (LSTM) networks, to model local temporal dependencies. These layers process sequential data and extract short-term trends, capturing temporal relationships that are critical for accurate forecasting.\n",
    "\n",
    "4. Static Covariate Encoders:\n",
    "Static covariate encoders handle time-invariant features, such as demographic information or geographical context, that provide essential context for the forecasts. These encoders integrate static information into the temporal modeling process, ensuring consistency throughout the prediction horizon.\n",
    "\n",
    "5. Interpretable Multi-Head Attention Layers:\n",
    "Attention mechanisms allow the model to focus on specific time steps that are most influential for the forecast. By assigning different weights to historical and future inputs, TFT provides insights into the long-term dependencies and temporal dynamics underlying the predictions. This component is especially valuable for the model interpretability, as it highlights which parts of the data significantly impact the results.\n",
    "\n",
    "6. Fully Connected Output Layers:\n",
    "The final layers aggregate information from the previous components and produce the multi-step forecasts. These outputs can be tailored for specific objectives, such as point predictions or probabilistic forecasts, depending on the application.\n",
    "\n",
    "A depiction of all model components and its interactions is shown below"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Overview of TFT model architecture](src/tft.png)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Implementation of day-ahead electricity price forecasting\n",
    "\n",
    "Given its flexibility to consider multiple types of covariate information, which foremost represent surrogates of supply and demand (i.e. weather forecasts, holidays, generation capacities and outages), TFT is specially well suited for the forecasting of day-ahead electricity prices. For this aim, the following variables are included in the model:\n",
    "\n",
    "| Data variable                  | Variable type                        | Units             | Source           | Comments                                                                                              |\n",
    "|:-------------------------------|:-------------------------------------|:------------------|:-----------------|:-----------------------------------------------------------------------------------------------------|\n",
    "| Electricity price              | target predicted variable            | EUR / MWh         | ENTSO-E          |                                                                                                      |\n",
    "| Day ahead forecast system load | time-varying known real input        | MW                | ENTSO-E          | Values are released hours before day ahead gate closure                                              |\n",
    "| Actual system load             | time-varying unknown real input      | MW                | ENTSO-E          |                                                                                                      |\n",
    "| Generation capacities          | time-varying known real input        | MW                | ENTSO-E          | Capacities include known generation and production outages                                           |\n",
    "| Fossil-Fuel prices             | time-varying known real input        | EUR / MWh         | Eikon datastream |                                                                                                      |\n",
    "| Sunshine hours                 | time-varying unknown real input      | minutes per hour  | DWD              | Stations considered: Brocken, München Flughafen, Hamburg-Fuhlsbüttel, Köln/Bonn, Leipzig/Halle       |\n",
    "| Wind speed                     | time-varying unknown real input      | m/s               | DWD              | Stations considered: Brocken, München Flughafen, Hamburg-Fuhlsbüttel, Köln/Bonn, Leipzig/Halle       |\n",
    "| Holidays                       | time-varying known categorical input | NA                | NA               | Holiday considering its name and each federal state                                                  |\n",
    "| Date features                  | time-varying known real input        | NA                | NA               | Encoded as sine and cosine of hour of day, day of week and month of year                              |\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "With this data considered, up to date our model has been trained considering the timeframe from October 2018 (establishment of DE_LU as bidding zone) up to November 2024 (inclusive). Furthermore, day-ahead forecasting tests have been conducted for all days of December 2024 achieving an average MAPE of 60.25 % and an average  MAE of 32.66 EUR/MWh. In this regard, extreme values for electricity prices took place, specially given very low and high values for wind energy generation, which along with solar generation, are indirectly considered as unknown real inputs in this model. Below such forecast results are depicted:\n",
    "\n",
    "![Impact of \"Dunkelflaute\" on actual and predicted electricity prices](src/max_mae.png)\n",
    "![Impact of renewable generation on actual and predicted electricity prices](src/max_mape.png)\n",
    "\n",
    "It can be seen for instance, that for the 12th of December 2024, very high electricity prices where experienced given a relative standstill in wind and solar generation (so called \"Dunkelflaute\"). In turn, for the 21st of December, large amounts of renewable generation took place, thus driving electricity prices to quite low values.\n",
    "\n",
    "Interestingly, the TFT model achieved relatively very good prediction results for several days of this month. Below are two of thes cases:\n",
    "\n",
    "![Actual vs predicted electricity prices on 2025-12-01](src/min_mae.png)\n",
    "![Actual vs predicted electricity prices on 2025-12-29](src/min_mape.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### AutoGluon"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For our final approach, we also used the open-source AutoML library AutoGluon (AG). AG has three modes of operation: Tabular, Multimodal, and Time Series. In our predictions, Time Series Forecasting is used, as it can forecast future values of multiple time series when given previous data. Since chosing a model for forecasting can be difficult and depends heavily on the given data, AG can run forecasts with many different models. This approach avoids testing every model individually and also prevents bias toward better-known models. The number of models tested and their hyperparameters can be selected separately for each implementation. Additionally, AutoGluon offers the option to combine multiple models into a Weighted Ensemble model for potentially even better results. AG compares the models with a given metric and at the end, the forecast of the best model is the output. This means that AG offers its users the ability to test multiple models and optimize the results without much prior knowledge or manual testing[Shur et al. 2023].\n",
    "\n",
    "We provided AG with our data frame and divided our features into three categories: static data, known covariates, and past covariates.\n",
    "\n",
    "- Static values: Data points like location that do not change over time; there is no such data point in our data frame.\n",
    "\n",
    "- Known covariates: Data points that are known for the future, e.g., day of the week, weather forecast, and holidays.\n",
    "\n",
    "- Past covariates: Data points that are not known in the future, e.g., stock market values, cross-border energy flows.\n",
    "\n",
    "As we have collected data from 2015 until now for around 150 model inputs, it is a considerable amount of data and presents many challenges for processing. Before running with the whole data frame, AG performs its own preprocessing, e.g., too many NULL values or incompatible data types. Another test is correlation between data points. If two data points are too closely correlated, AG removes one from the calculations to avoid spending unnecessary computation time. AG also automatically checks if certain models require longer training time than allocated by the user. In such cases, AG automatically skips training that model and moves on to the next.\n",
    "\n",
    "As mentioned above, AG trains several models by default but can also train models specifically chosen by the user. These models range from naive forecasts (which sets the forecast to the last observed value) to Temporal Fusion Models and Chronos Models mentioned above. Since these models do not need to be implemented separately, no decision needs to be made in advance, and models can simply be chosen for their results at the end. AG also provides a forecast using a Weighted Ensemble Model, which takes the best-performing models from the training and combines them for an even better forecast. Additionally, AG offers users the opportunity to use different presets and hyperparameters for each model. Presets range from medium_quality to best_quality, where the duration of training increases significantly. Hyperparameters can be chosen for each model separately, or AG can use default values.\n",
    "\n",
    "The different models that were trained by AG and their results for the validation score can be seen below. In that case, the weighted ensemble approach had the best score. Additionally, the time for predictions using the model and the fit time are given in seconds."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| Model                         | Score Val   | Pred Time Val | Fit Time Marginal |\n",
    "|--------------------------------|-------------|---------------|-------------------|\n",
    "| WeightedEnsemble               | -0.038      | 57.56         | 0.08              |\n",
    "| TemporalFusion Transformer     | -0.047      | 0.04          | 338.58            |\n",
    "| TiDE                           | -0.061      | 0.02          | 718.89            |\n",
    "| Chronos Fine Tuned [bolt_small]| -0.079      | 0.03          | 81.40             |\n",
    "| ChronosZeroShot[bolt_base]     | -0.084      | 28.90         | 39.93             |\n",
    "| DirectTabular                  | -0.090      | 0.09          | 43.85             |\n",
    "| PatchTST                       | -0.096      | 0.02          | 79.90             |\n",
    "| DeepAR                         | -0.104      | 0.10          | 222.96            |\n",
    "| Recursive Tabular              | -0.125      | 0.21          | 9.54              |\n",
    "| SeasonalNaive                  | -0.128      | 15.61         | 0.03              |\n",
    "| AutoETS                        | -0.150      | 4.47          | 0.12              |\n",
    "| DynamicOptimizedTheta          | -0.171      | 50.40         | 0.11              |\n",
    "| NPTS                           | -0.244      | 2.65          | 0.03              |\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Start Basic Autogluon Code"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "preset_list = [\"high_quality\", \"best_quality\", \"fast_training\", \"medium_quality\", \"hp1\"]\n",
    "set_time_limit = None\n",
    "output_folder_autogluon = f\"/data/horse/ws/fewa833b-time-series-forecast/AutoGluon/Felix/AllInOne\"\n",
    "\n",
    "#Start baseline\n",
    "dataset = 'dayhead'\n",
    "loaded_dataframe, file_path, data = dataset_Setup(dataset)\n",
    "correlation_output_folder = os.path.join(output_folder_autogluon, \"Correlation\", \"Baseline\")\n",
    "correlation_calculation(data, correlation_output_folder)\n",
    "provide_known_covariables = None\n",
    "set_preset = \"fast_training\"\n",
    "train_autogluon(set_preset, set_time_limit, dataset, provide_known_covariables, output_folder_autogluon,\n",
    "                loaded_dataframe, file_path, data)\n",
    "\n",
    "#start all-data\n",
    "dataset = 'all'\n",
    "correlation_output_folder = os.path.join(output_folder_autogluon, \"Correlation\", \"Less-Data\")\n",
    "correlation_calculation(data, correlation_output_folder)\n",
    "for set_preset in preset_list:\n",
    "    provide_known_covariables = None\n",
    "    train_autogluon(set_preset, set_time_limit, dataset, provide_known_covariables, output_folder_autogluon,\n",
    "                    loaded_dataframe, file_path, data)\n",
    "\n",
    "    provide_known_covariables = True\n",
    "    train_autogluon(set_preset, set_time_limit, dataset, provide_known_covariables, output_folder_autogluon,\n",
    "                    loaded_dataframe, file_path, data)\n",
    "\n",
    "#start less-data\n",
    "dataset = \"less\"\n",
    "correlation_output_folder = os.path.join(output_folder_autogluon, \"Correlation\", \"Less-Data\")\n",
    "correlation_calculation(data, correlation_output_folder)\n",
    "for set_preset in preset_list:\n",
    "    provide_known_covariables = None\n",
    "    train_autogluon(set_preset, set_time_limit, dataset, provide_known_covariables, output_folder_autogluon,\n",
    "                    loaded_dataframe, file_path, data)\n",
    "\n",
    "    provide_known_covariables = True\n",
    "    train_autogluon(set_preset, set_time_limit, dataset, provide_known_covariables, output_folder_autogluon,\n",
    "                    loaded_dataframe, file_path, data)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tuning AutoGluon\n",
    "\n",
    "In the AutoGluon group, an attempt was made to optimise the prediction with AutoGluon by using different settings. The aim was to evaluate the performance of different approaches and to analyse the importance of additional data and external influencing factors. For this purpose, different AutoGluon presets were used, including best_quality, fast_training, high_quality, medium_quality and two customised configurations with user-defined hyperparameters (hp1 and hp2).\n",
    "The models were trained on two distinct datasets:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. allData.csv: A comprehensive dataset with over 150 data points per hour, providing a detailed basis for more accurate predictions.\n",
    "2. lessData.csv: A reduced data set with around 30 data points per hour, which represents whether the model can also make good predictions with less data\n",
    "\n",
    "As part of the investigation of the effects of external factors on the prediction quality, the training runs were carried out in two variants. In the first variant, known covariates such as the day (e.g. weekday or public holiday) and weather conditions (e.g. temperature or precipitation) were included in the modelling. In the second variant, on the other hand, only the time stamps and the target value were used to optimise the models.\n",
    "\n",
    "To evaluate the effectiveness of the different approaches, a simple baseline was used as a reference point, which only used the time and the day-ahead price as input data. The aim was to determine the potential improvements in forecast accuracy through additional data points."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "file_path = \"temp/preset-comparison.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure 'Knowncoavariables' is treated as a string and replace NaN or empty values with 'False'\n",
    "df['Knowncoavariables'] = df['Knowncoavariables'].fillna('False').astype(str)\n",
    "\n",
    "# Create bar labels: Modelpreset_Dataset_Knowncoavariables\n",
    "df['Labels'] = df['Modelpreset'] + '_' + df['Dataset'] + '_' + df['Knowncoavariables']\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "# MAE Plot\n",
    "colors_mae = ['blue' if value != df['MAE SKlearn'].min() else 'red' for value in df['MAE SKlearn']]\n",
    "ax[0].bar(df['Labels'], df['MAE SKlearn'], color=colors_mae, alpha=0.7)\n",
    "ax[0].set_title('Mean Absolute Error (MAE)')\n",
    "ax[0].set_ylabel('MAE')\n",
    "ax[0].set_xticklabels(df['Labels'], rotation=45, ha='right')\n",
    "\n",
    "\n",
    "# RMSE Plot\n",
    "colors_rmse = ['green' if value != df['RMSE SKlearn'].min() else 'red' for value in df['RMSE SKlearn']]\n",
    "ax[1].bar(df['Labels'], df['RMSE SKlearn'], color=colors_rmse, alpha=0.7)\n",
    "ax[1].set_title('Root Mean Squared Error (RMSE)')\n",
    "ax[1].set_ylabel('RMSE')\n",
    "ax[1].set_xticklabels(df['Labels'], rotation=45, ha='right')\n",
    "\n",
    "# Adjust layout to avoid overlapping\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The results for the different model variants are represented in the plot above. The red columns represent the best models for the given error metric, MAE and RMSE. Our best model for the MAE metric is \"high_quality_less_False\", which means that the model used high quality as a preset with the smaller dataset and no known covariants. For RMSE the best models are \"best_quality_all_False\" and \"high_quality_all_False\". The full results can be found in the table below."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| Dataset  | Modelpreset   | Known​covariables | MAE SKlearn | RMSE SKlearn | Duration  |\n",
    "|----------|---------------|------------------|-------------|--------------|-----------|\n",
    "| less     | high_quality  |                  | 19.53       | 33.23        | 5154.46   |\n",
    "| less     | best_quality  |                  | 19.57       | 33.27        | 5080.50   |\n",
    "| less     | high_quality  | True             | 19.58       | 33.32        | 7029.28   |\n",
    "| less     | best_quality  | True             | 19.63       | 33.31        | 12851.59  |\n",
    "| all      | high_quality  |                  | 20.32       | 33.08        | 2983.92   |\n",
    "| all      | best_quality  |                  | 20.32       | 33.08        | 3000.14   |\n",
    "| all      | high_quality  | True             | 20.41       | 33.20        | 3051.78   |\n",
    "| all      | best_quality  | True             | 20.41       | 33.20        | 3009.79   |\n",
    "| all      | medium_quality| True             | 21.12       | 34.47        | 1446.24   |\n",
    "| all      | medium_quality|                  | 21.12       | 34.47        | 1460.18   |\n",
    "| all      | hp1           |                  | 22.03       | 35.50        | 2766.88   |\n",
    "| all      | hp1           | True             | 22.03       | 35.51        | 2757.89   |\n",
    "| less     | hp1           | True             | 22.16       | 35.44        | 2023.04   |\n",
    "| less     | hp1           |                  | 22.16       | 35.44        | 2105.08   |\n",
    "| less     | fast_training |                  | 25.17       | 38.68        | 2621.76   |\n",
    "| all      | fast_training | True             | 25.17       | 38.68        | 6533.26   |\n",
    "| all      | fast_training |                  | 25.17       | 38.68        | 6936.63   |\n",
    "| less     | fast_training | True             | 25.17       | 38.68        | 6740.39   |\n",
    "| dayhead  | fast_training | True             | 25.17       | 38.68        | 6910.75   |\n",
    "| dayhead  | fast_training |                  | 25.17       | 38.68        | 7143.28   |\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Furthermore, an analysis was conducted to determine the extent to which the selection of presets affects the computing time and the outcomes obtained. The investigation focused on whether less complex presets, such as fast_training, could achieve results in a significantly shorter time while maintaining a marginally lower level of accuracy under specific conditions. This trade-off between computing time and model performance is of particular relevance when models are updated with additional data at a later stage to enhance resource efficiency. However, this hypothesis could not be substantiated within the scope of the present study.\n",
    "\n",
    "In our results, the preset for fast_training consistently produced worse outcomes and required more training time than other model types. In the plot below, our results are compared to the training duration of each model. While fast_training can be ruled out, the best_quality preset performs better while also having a shorter runtime."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "file_path = \"temp/preset-comparison.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df['Knowncoavariables'] = df['Knowncoavariables'].apply(lambda x: 'true' if x is True else '')\n",
    "\n",
    "if df['MAE SKlearn'].dtype == object:  # If the column is of object type (strings)\n",
    "    df['MAE SKlearn'] = df['MAE SKlearn'].str.replace('%', '').astype(float)\n",
    "\n",
    "df = df.sort_values(by='MAE SKlearn')\n",
    "\n",
    "# y- and x-axis\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "x_positions = np.arange(len(df))\n",
    "ax1.bar(x_positions - 0.2, df['Duration'], color='orange', label='Duration (s)', width=0.4, alpha=0.7)\n",
    "ax1.set_xlabel('Preset')\n",
    "ax1.set_ylabel('Duration (seconds)', color='orange')\n",
    "ax1.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "# y-axis for MAE SKlearn\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(x_positions + 0.2, df['MAE SKlearn'], color='blue', label='MAE SKlearn', width=0.4, alpha=0.7)\n",
    "ax2.set_ylabel('MAE SKlearn', color='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "ax2.set_ylim(0, df['MAE SKlearn'].max() + 5)\n",
    "\n",
    "# title and labels\n",
    "ax1.set_xticks(x_positions)\n",
    "ax1.set_xticklabels(df['Modelpreset'] + \"_\" + df['Dataset'] + \" \" + df['Knowncoavariables'], rotation=45, ha=\"right\")\n",
    "\n",
    "plt.title('Duration and MAE SKlearn by Preset')\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<!-- ### Innovation / Improving AutoGluon -->\n",
    "### Experimental Approaches\n",
    "\n",
    "#### Leveraging News Embeddings\n",
    "\n",
    "For evaluating the influence of the news embeddings on predicting the day ahead price, we tested three configurations over a 30-day period:\n",
    "\n",
    "| Model Configuration | Validation RMSE |\n",
    "|---|---|\n",
    "| NewsAPI embeddings + day-ahead + target | 28.82 |\n",
    "| Guardian embeddings + day-ahead + target | 27.36 |\n",
    "| Day-ahead + target (baseline) | 27.86 |\n",
    "\n",
    "Using The Guardian's historical data, we extended the experiment to 10 years:\n",
    "\n",
    "| Model Configuration | Test RMSE |\n",
    "|---|---|\n",
    "| Guardian embeddings + day-ahead (386 dimensions) | 73.88 |\n",
    "| Guardian embeddings + day-ahead (PCA 8 dimensions) | 29.09 |\n",
    "| Day-ahead + target (baseline) | 22.18 |\n",
    "\n",
    "The results show that news embeddings did not improve the prediction of German day-ahead prices. Future work should focus on finding more relevant news sources for the German market and exploring more complex model architectures that can better handle the embedding data."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example inference\n",
    "\n",
    "guardian_embeddings_day_ahead, newsapi_embeddings_day_ahead, day_ahead_prices_for_prediction, ground_truth = prepare_embedding_data(guardian_embeddings, newsapi_embeddings, day_ahead_prices)\n",
    "\n",
    "guardian_predictor_path =  \"models/models/guardian_embedding_model\"\n",
    "newsapi_predictor_path = \"models/models/newsapi_embedding_model\"\n",
    "no_embeddings_predictor_path = \"models/models/no_embeddings_model\"\n",
    "\n",
    "guardian_predictions = predict_embedding_data(guardian_predictor_path, guardian_embeddings_day_ahead)\n",
    "newsapi_predictions = predict_embedding_data(newsapi_predictor_path, newsapi_embeddings_day_ahead)\n",
    "no_embeddings_predictions = predict_embedding_data(no_embeddings_predictor_path, day_ahead_prices_for_prediction)\n",
    "\n",
    "visualize_embedding_model_results(guardian_predictions, ground_truth, \"Guardian Embeddings\")\n",
    "visualize_embedding_model_results(newsapi_predictions, ground_truth, \"NewsAPI Embedddings\")\n",
    "visualize_embedding_model_results(no_embeddings_predictions, ground_truth, \"No Embeddings\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Predicting Intraday-Prices As Additional Feature\n",
    "\n",
    "Besides day-ahead prices, which are the target value of this time series forecast, electric energy is also traded on the more short-term Intraday market.\n",
    "While in general they refer to the same underlying product, there are cases in which the prices of day-ahead and intraday trades differ significantly, as the intraday market will react more short-term to unforeseen changes in supply and demand.\n",
    "These short-term differences can for example be induced by"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- extreme weather conditions offsetting forecast and actual generation of renewables affecting supply\n",
    "- extreme weather conditions generating an unforeseen surge or decline in demand\n",
    "- unplanned power plant outages creating a lack of supply\n",
    "- short-term changes in balancing affecting supply\n",
    "\n",
    "As Germany saw their share of renewables in power generation rise, the role of intraday prices for demand regulation became increasingly important.\n",
    "Therefore the \"gate closure\" of intraday trades, meaning the time between end of trading and delivery, got shortened significantly.\n",
    "In July 2015 it was changed from 45 to 30 minutes, whereas nowadays it's down to five minutes, and allows for a high reactivity to changes in supply and demand. [Smard, Großhandelspreise, 2024]\n",
    "\n",
    "Given our time series dataset with all features that resulted from data gathering, we tried to predict the intraday price to use as an additional feature in predicting the day-ahead price.\n",
    "If we were to predict the intraday prices with high accuracy, this could possibly be an important feature for improving the day-ahead prediction.\n",
    "\n",
    "Data Sources:\n",
    "The open data sources for intraday electric energy prices are unfortunately very limited, as in many cases stock market data are sold for commercial data analysis.\n",
    "\"Netztransparenz\", which is an information transparency platform by the four German transmission grid operators, publishes a dataset with quarter-hourly intraday price indices from 30th June 2020 to date.\n",
    "These values are calculated as volume-weighted average of the last stock market orders over 500MW for quarter-hourly electricity [Netztransparenz, Index-Ausgleichspreis, 2024].\n",
    "The dataset of Netztransparenz is to our knowledge the only freely available dataset for German intraday electricity prices."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To investigate the potential of using intraday prices as an additional feature for day-ahead price prediction, a systematic approach was followed, involving the prediction of intraday prices and their integration into the day-ahead prediction.\n",
    "\n",
    "Following table contains the original quarter-hourly intraday electricity price data as downloaded from the source."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pd.read_csv('../final-submission/merged_data/data_collection/Intraday_prices_2020_2025_quarter_hourly.csv').head()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To maintain consistency with the day-ahead price data and simplify analysis, the quarter-hourly intraday price data was aggregated into hourly values. This was achieved by calculating the mean of the four quarter-hourly values for each hour, resulting in a manageable dataset with hourly intraday prices that could be used for model training and prediction."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pd.read_csv('../final-submission/merged_data/data_collection/Intraday_prices_2020_2025_hourly.csv').head()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "When examining the data, some strong outliers were clearly visible. Outliers in time series data can distort trends and affect model performance. To address this, we used the z-score method, identifying values exceeding three standard deviations from the mean. This approach effectively detects anomalies while preserving the overall data distribution.\n",
    "A total of 539 outliers were detected and replaced using a forward-fill approach to maintain data continuity. The plot below illustrates these detected anomalies over time, highlighting significant fluctuations in intraday electricity prices throughout 2024.\n",
    "\n",
    "![Intraday price outliers in 2024](src/Intraday_price_outliers_2024_plot.png)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Intraday prices were predicted for the same test set time intervals as used for the day-ahead forecast. The models were trained using only the ```timestamp``` and ```intraday_price_EURO``` columns. After testing various configurations, the best results were obtained with the \"best_quality\" preset using weighted_ensemble.\n",
    "\n",
    "![Comparison of dayahead price prediction accuracy with historic and predicted intraday price as additional feature](src/Intraday_price_comparison_plots.png)\n",
    "\n",
    "The left chart above, labeled \"MAE Comparison (BASELINE)\", shows the MAE for three different scenarios: the baseline model, the model utilizing historical intraday data (assuming perfect predictions of intraday prices), and the model combining both historical and predicted intraday data.\n",
    "The baseline model shows the highest MAE, while both additional features reduce the error, with predicted intraday data yielding the lowest MAE in this comparison.\n",
    "\n",
    "It is suggested that the slight decrease in prediction accuracy when including historic intraday data compared to predicted intraday data is caused by less outlying predicted values in average which have less negative impact on the prediction accuracy but might not represent the actual reality.\n",
    "\n",
    "After showcasing the improvement in day-ahead predictions by integrating intraday data into the baseline model, we investigated its impact on the full dataset, which includes all available features, to evaluate its relevance for the project. As demonstrated in the chart \"MAE Comparison (ALL Features)\", contrary to the earlier findings, this approach led to a slight decrease in prediction accuracy when both predicted and/or historical intraday data were added as additional features.\n",
    "\n",
    "As a result, this approach that investigated the potential of using predicted intraday prices as an additional feature for day-ahead price prediction, was not incorporated into the final model."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Autogluon with forecasted known_covariables\n",
    "When analyzing the models, it was found that some models in AG do not consider past covariates by default, whereas the TFT model actively utilizes them. This suggests that AG may discard valuable information that could enhance prediction accuracy.\n",
    "To investigate whether prediction performance could be improved, AG was modified to use all features as covariates. It was hypothesized that a model with access to all relevant historical data would provide more accurate predictions.\n",
    "To test this assumption, it was initially presumed that all features were known for the forecast day. The corresponding values are presented in the following plot, with errors measured using the MAE.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "file_path = \"./temp/preset-known.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure 'Knowncoavariables' is treated as a string and replace NaN or empty values with 'False'\n",
    "df['Knowncoavariables'] = df['Knowncoavariables'].fillna('False').astype(str)\n",
    "\n",
    "# Create bar labels: Modelpreset_Dataset_Knowncoavariables\n",
    "df['Labels'] = df['Modelpreset'] + '_' + df['Dataset'] + '_' + df['Knowncoavariables']\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "# MAE Plot\n",
    "colors_mae = ['blue' if value != df['MAE SKlearn'].min() else 'red' for value in df['MAE SKlearn']]\n",
    "ax[0].bar(df['Labels'], df['MAE SKlearn'], color=colors_mae, alpha=0.7)\n",
    "ax[0].set_title('Mean Absolute Error (MAE)')\n",
    "ax[0].set_ylabel('MAE')\n",
    "ax[0].set_xticklabels(df['Labels'], rotation=45, ha='right')\n",
    "\n",
    "\n",
    "# RMSE Plot\n",
    "colors_rmse = ['green' if value != df['RMSE SKlearn'].min() else 'red' for value in df['RMSE SKlearn']]\n",
    "ax[1].bar(df['Labels'], df['RMSE SKlearn'], color=colors_rmse, alpha=0.7)\n",
    "ax[1].set_title('Root Mean Squared Error (RMSE)')\n",
    "ax[1].set_ylabel('RMSE')\n",
    "ax[1].set_xticklabels(df['Labels'], rotation=45, ha='right')\n",
    "\n",
    "# Adjust layout to avoid overlapping\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The best model (best_quality_all_True) was then provided with self-predicted feature data. Feature prediction was performed using AG with Chronos as the underlying model—a combination that approximated future feature values and supplied the model with estimated input data. A weight-sharing approach was employed, ensuring that all features used the same model for forecasting.\n",
    "Additionally, Chronos enables zero-shot prediction, allowing the model to be applied to new, unseen data without prior fine-tuning. This feature is particularly beneficial when training data is insufficient for specific scenarios or when the model must adapt flexibly to different input variables.\n",
    "![AG-Ensemble](./src/AG-Ensemble.svg)\n",
    "In this scenario, an MAE of 24.0815 was achieved. Unfortunately, this result was significantly worse than the prediction without known covariates. This discrepancy is likely due to the large deviations in the predicted feature values. The resulting predictions for the test period are shown in the next figure, where they are compared with the actual values and the previous best model, AG High Quality Less without Known Covariates."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df1 = pd.read_csv('./temp/comparison-forecastet-known-ag.csv', parse_dates=['timestamp'])\n",
    "df2 = pd.read_csv('./temp/comparison-ag.csv', parse_dates=['timestamp'])\n",
    "df1 = df1[(df1['timestamp'].dt.month == 10)]\n",
    "df2 = df2[(df2['timestamp'].dt.month == 10)]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df1['timestamp'], df1['actual_price'], label='Actual Day Ahead Price', marker='o')\n",
    "plt.plot(df1['timestamp'], df1['mean'], label='Predicted Day Ahead Price with predicted known-covariables', marker='x')\n",
    "plt.plot(df2['timestamp'], df2['predicted'], label='Predicted Day Ahead Price without predicted known-covariables', linestyle='--')\n",
    "\n",
    "\n",
    "plt.title('Day Ahead Price Comparison')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Day Ahead Price')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Conclusion"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "With the rapid expansion of renewable energy sources, day-ahead electricity prices have become increasingly volatile. The availability of solar and wind power plays a crucial role in determining these prices, leading to significant fluctuations.\n",
    "\n",
    "When renewable energy production is high, electricity prices can plummet, sometimes even falling below zero. This phenomenon has become more frequent in recent years: in 2022, there were 69 hours of negative pricing, but this number surged to 301 hours in 2023 and further increased to 459 hours in 2024.\n",
    "\n",
    "Conversely, during periods of low renewable output—such as in winter when sunlight and wind are scarce—electricity prices can soar to extreme levels. This was evident in early November and mid-December 2024, when day-ahead prices exceeded 800 euros per megawatt-hour (MWh).\n",
    "\n",
    "These extreme price swings present a fundamental for the presented models. Since such rare events occur infrequently, they are underrepresented in historical data. As a result, even advanced models struggle to anticipate these anomalies with precision. The absence of comparable training data means that models can recognize general trends but remain unreliable when predicting rare, high-impact events in energy markets."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this chapter, we investigated three different approaches to enhance our models and improve forecasting. We investigated Long Short-Term Memory networks (LSTMs) as a Recurrent Neural Network-based approach and refined them further by extending to an encoder-decoder LSTM, incorporating an attention mechanism, and leveraging multivariate data. Our results showed that the Encoder-Decoder-Attention LSTM delivered the best performance.\n",
    "\n",
    "We then use Chronos as a transformer-based approach and improved it through fine-tuning. Additionally, we investigated different fine-tuning configurations and demonstrated that the large model, trained on a substantial dataset with 1,000 training steps, performed the best.\n",
    "\n",
    "Temporal Fusion Transformers (TFT), a second transformer-based approach was used which is capable of handling multivariate data. We trained the model on various features and examined how it responds to extreme scenarios.\n",
    "\n",
    "We utilize AutoGluon, an AutoML framework, to combine the advantages of multiple models. To further enhance its performance, we conducted experiments incorporating news embeddings and intra-day price forecasts."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predictive Modeling"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this section, we compare our best-performing models introduced in [Section 5](#visualization-and-story-telling). The best model will be used to perform the final forecast to predict the day-ahead energy prices for the 2025-02-18."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Final Benchmark"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We compare our models against each other and a baseline model that predicts day-ahead prices using a simple 1-day shift. To evaluate their performance on the test set, we use Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). The test set consists of day-ahead prices from June 1, 2024, to November 30, 2024."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ! set path your path to the 'temp' dir\n",
    "report_temp_dir = '\\\\temp'\n",
    "\n",
    "lstm_res = pd.read_csv(report_temp_dir + '\\\\lstm_final_benchmark_test_results_48h.csv',\n",
    "                       index_col=0)[['day_ahead_price_predicted']]\n",
    "chronos_large_finetuned_res = pd.read_csv(report_temp_dir + '\\\\chronos_large_finetuned_final_benchmark_results.csv',\n",
    "                                          index_col=0)\n",
    "chronos_large_pretrained_res = pd.read_csv(\n",
    "    report_temp_dir + '\\\\chronos_large_pretrained_final_benchmark_results.csv', index_col=0)\n",
    "chronos_tiny_finetuned_res = pd.read_csv(report_temp_dir + '\\\\chronos_tiny_finetuned_final_benchmark_resuts.csv',\n",
    "                                         index_col=0)\n",
    "chronos_tiny_pretrained_res = pd.read_csv(report_temp_dir + '\\\\chronos_tiny_pretrained_final_benchmark_results.csv',\n",
    "                                          index_col=0)\n",
    "autogluon_res = pd.read_csv(report_temp_dir + '\\\\Autogluon_high_quality_less_without_known_covariables.csv',\n",
    "                            index_col=0)[['predicted']]\n",
    "tft_res = pd.read_csv(report_temp_dir + '\\\\tft_final_benchmark_results.csv', index_col=0)\n",
    "previous_day_baseline = pd.read_csv(report_temp_dir + '\\\\small_subset_lstm_cleaned.csv')[\n",
    "    ['Date', 'prev_range_prices']].set_index('Date')\n",
    "previous_day_baseline.index.names = ['timestamp']\n",
    "\n",
    "actual_day_ahead_prices = pd.read_csv(report_temp_dir + '\\\\small_subset_lstm_cleaned.csv')[\n",
    "    ['Date', 'day_ahead_prices']].set_index('Date')\n",
    "actual_day_ahead_prices.index.names = ['timestamp']"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "BenchMaker = BenchmarkMaker(export_dir='results_from_report')\n",
    "BenchMaker.load_dataframes(predictions={'Baseline t-24h': previous_day_baseline,\n",
    "                                        'TFT': tft_res,\n",
    "                                        'EncDecAtt LSTM': lstm_res,\n",
    "                                        'AutoGluon': autogluon_res,\n",
    "                                        'Chronos Finetuned Large': chronos_large_finetuned_res},\n",
    "                           prices=actual_day_ahead_prices)\n",
    "BenchMaker.calc_errors()\n",
    "\n",
    "BenchMaker.plot_compare_mae()\n",
    "BenchMaker.plot_compare_rmse()\n",
    "BenchMaker.plot_compare_predictions_hourly(start_date=pd.Timestamp('2024-06-01 00:00:00'),\n",
    "                                           end_date=pd.Timestamp('2024-06-07 23:00:00'))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "All models outperform the baseline, with the two best-performing ones being the fine-tuned Chronos-T5 (Large) model and the AutoGluon framework. AutoGluon achieved better results in terms of RMSE, while Chronos-T5 performed better in minimizing MAE. Since our objective is to minimize MAE, we will use the Chronos-T5 model to generate the final forecast. However, the LSTM and TFT approaches also show promising results."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Final Forecast"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the cutoff date in UTC\n",
    "\n",
    "#add if needed\n",
    "#cutoff_date_utc = pd.Timestamp(\"2025-02-03 22:00\", tz=\"UTC\")\n",
    "\n",
    "# Load the data\n",
    "file_path = 'merged_data/data_collection/day_ahead_prices.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "#add if needed\n",
    "# Convert timestamp column to datetime (ensure it's in UTC)\n",
    "data['Date'] = pd.to_datetime(data['timestamp'], utc=True)\n",
    "\n",
    "# Filter data to include only timestamps **before** the cutoff date\n",
    "#data = data[data['Date'] <= cutoff_date_utc]\n",
    "\n",
    "# Set the index to Date\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Load the model\n",
    "finalmodeldir = \"juliushanusch/chronos-tiny-final-fine-tuned-day-ahead-prices\"\n",
    "model_type = \"Chronos\"\n",
    "model_name = finalmodeldir\n",
    "\n",
    "model = ChronosModel(model_name=model_name, model_type=model_type)\n",
    "model.model = model.custom_load(finalmodeldir)\n",
    "\n",
    "# Forecast based on the filtered dataset\n",
    "finalForecastChronos = model.run_prediction(data)\n",
    "\n",
    "# Convert forecast timestamps to Berlin time (CET/CEST)\n",
    "finalForecastChronos['timestamp'] = pd.to_datetime(finalForecastChronos['timestamp'], utc=True)\n",
    "finalForecastChronos['timestamp'] = finalForecastChronos['timestamp'].dt.tz_convert('Europe/Berlin')\n",
    "\n",
    "print(finalForecastChronos)\n",
    "\n",
    "# TODO: Write to file if needed\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "\n",
    "-> TODO wenn Ergebnisse da sind"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Summary & Future Work"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Self-Organization In Academic Projects\n",
    "\n",
    "Team Structure and Initial Organization:\n",
    "The BTW25 Data Science challenge presented a unique opportunity to work in a large-scale collaborative environment with twelve team members. To effectively manage this substantial team size, we implemented a strategic division into three specialized groups, each focusing on critical aspects of the project:\n",
    "\n",
    "- XAI and Data Preparation Group: Focused on explainable AI approaches and foundational data analysis\n",
    "- AutoGluon and Preprocessing Group: Concentrated on implementing solutions using AutoGluon and gathering data\n",
    "- Deep Learning Models Group: Dedicated to the development and application of complex neural network architectures\n",
    "\n",
    "Challenges of Interdependent Workflows:\n",
    "One of the primary challenges we faced was managing the interdependencies between these groups. Each team's output served as essential input for others, creating a complex web of dependencies. For example, the data fetching and preparation team's work directly impacted both the XAIs ability to explore the data and the deep learning team's capacity to develop their architectures. This interconnected structure meant that delays or changes in one group could create ripple effects throughout the entire project.\n",
    "\n",
    "Cross-Team Communication and Coordination:\n",
    "To address these challenges, we implemented an \"ambassador\" system. Members of the AutoGluon team were designated as visitors to other groups, attending their meetings and serving as communication bridges. This approach proved invaluable for several reasons:\n",
    "\n",
    "- Real-time awareness of progress and challenges across all teams\n",
    "- Immediate feedback on compatibility issues between different components\n",
    "- Rapid dissemination of important updates or changes\n",
    "- Prevention of duplicate efforts across groups\n",
    "\n",
    "Standardization and Technical Integration:\n",
    "A crucial aspect of our success was the implementation of strict technical standards:\n",
    "\n",
    "Data Format Guidelines:\n",
    "\n",
    "- Standardized CSV formats for time series data\n",
    "- Consistent datetime formatting across all datasets\n",
    "- Uniform naming conventions for features and target variables\n",
    "- Desired range of data\n",
    "- Clear separation of train/val/test splits\n",
    "\n",
    "Model Wrapper Standardization:\n",
    "\n",
    "- Common interface for all models regardless of underlying implementation\n",
    "- Standardized prediction output formats\n",
    "- Unified evaluation metrics and reporting structures\n",
    "\n",
    "Effective Project Management Practices\n",
    "To maintain coherence across the large team, we established several key management practices:\n",
    "\n",
    "- Regular all-hands meetings for high-level coordination\n",
    "- Dedicated communication channels for each subgroup\n",
    "\n",
    "Learning Outcomes and Best Practices\n",
    "This experience provided valuable insights into managing large-scale ML projects:\n",
    "\n",
    "- The importance of clear communication channels and protocols\n",
    "- The value of standardized interfaces between different components\n",
    "- The effectiveness of cross-team ambassadors in maintaining project coherence\n",
    "- The necessity of flexible yet structured organization in academic projects\n",
    "\n",
    "Impact on Project Success\n",
    "These organizational strategies significantly contributed to our project's success by:\n",
    "\n",
    "- Minimizing integration issues between different components\n",
    "- Reducing redundant work across teams\n",
    "- Enabling rapid problem identification and resolution\n",
    "- Fostering knowledge sharing across specialization boundaries\n",
    "- Creating a cohesive final product despite the complexity of multiple approaches\n",
    "\n",
    "The experience demonstrated that effective organization and communication structures are as crucial to project success as technical expertise, particularly in large-scale academic collaborations.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Future Work\n",
    "#### Weather Improvements\n",
    "\n",
    "While the current weather implementation provides a solid foundation, various measures could be implemented to further optimise the process:\n",
    "\n",
    "- The installation of a greater number of weather stations would facilitate a more detailed and regionally differentiated depiction of weather conditions.\n",
    "\n",
    "- In addition, calculating an average at the country level has the potential to enhance the representativeness of the basis. This average could then be weighted with the population density and the regionally available capacity of the individual power plant types in order to better model local energy generation and demand.\n",
    "\n",
    "- Furthermore, the incorporation of extreme weather events, such as heat waves, cold snaps, and storms, is imperative for the analysis and forecasting of energy generation and demand. Including data on these events could facilitate more precise prediction of unexpected fluctuations in energy production and demand. The analysis of long-term weather patterns, including seasonal fluctuations and the effects of climate change, has the potential to significantly enhance forecasting accuracy.\n",
    "\n",
    "- Hyperparameter optimization for Chronos.\n",
    "\n",
    "By expanding the analysed data set, weather implementation could be made more precise and flexible, allowing it to respond better to the requirements of a dynamic energy market."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Advancing LSTM Models\n",
    "\n",
    "Since the Encoder Decoder LSTM with attention has shown to be superior to LSTM models without attention, it might lead to a further improved forecasting accuracy to use an additional feature attention. This could help the model to focus only on important features rather than weighting each feature evenly.\n",
    "Also, LSTM models benefit strongly from a dataset with substained features. Therefore, finding even more meaningful features could support the model on learning correct forecasts. Further optimizing the model on the combination of features, the model size and other hyperparameters could possibly decrease the error of the final forecast."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Summary"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Bibliography\n",
    "|                                                  |                                                                                                                                                                                                                                                       |\n",
    "|--------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| [Ansari et al. 2024]                             | Abdul Fatir Ansari et al. 2024: Chronos: Learning the Language of Time Series https://doi.org/10.48550/arXiv.2403.07815                                                                                                                               |\n",
    "| [Bosch et al. 2023]                              | Bosch, S., Schlenker, F., Bohn, J., Kupies, S., & Schmidt, M. (2023). Deutschland–Pionierland der Energiewende. In Energie-Weltatlas: Transformation des Energiesystems in globaler Perspektive (pp. 81-91). Wiesbaden: Springer Fachmedien Wiesbaden |\n",
    "| [Contreras et al. 2003]                          | Contreras, J., Espinola, R., Nogales, F. J., & Conejo, A. J. (2003). ARIMA models to predict next-day electricity prices. IEEE transactions on power systems, 18(3), 1014-1020                                                                        |\n",
    "| [Dumancic 2024]                                  | Dumancic, M. (2024, March). Marktmacht in der Stromwirtschaft: Mehr Wettbewerb durch Zukunftstechnologien?. In Kartellrecht und Zukunftstechnologien (pp. 105-128). Nomos Verlagsgesellschaft mbH & Co. KG.                                           |\n",
    "| [Hein et al. 2020]                               | Hein, F., & Hermann, H. (2020). Agorameter–Dokumentation. Agora Energiewende: Berlin, Germany.                                                                                                                                                        |\n",
    "| [Horáček 2010]                                   | Horáček, P. (2010). Power balance control in electrical grids. Seminary Textbooks, Faculty of Electrical Engineering.                                                                                                                                 |\n",
    "| [Lago et al. 2018]                               | Lago, J., De Ridder, F., & De Schutter, B. (2018). Forecasting spot electricity prices: Deep learning approaches and empirical comparison of traditional algorithms. Applied Energy, 221, 386-405.                                                    |\n",
    "| [Nestle et al. 2009]                             | Nestle, D., Ringelstein, J., & Selzam, P. (2009). Integration dezentraler und erneuerbarer Energien durch variable Strompreise im liberalisierten Energiemarkt. uwf UmweltWirtschaftsForum, 17, 361-365.                                              |\n",
    "| [Niedermeier 2023]                               | Niedermeier, T. (2023). Auswertung der Stromerzeugung in Deutschland von 2015–2022 und Abgleich mit den Ausbauzielen des EEG 2023 (Doctoral dissertation, Hochschule für angewandte Wissenschaften München).                                          |\n",
    "| [Nunes et al. 2008]                              | Nunes, C., Pacheco, A., & Silva, T. (2008, May). Statistical models to predict electricity prices. In 2008 5th International Conference on the European Electricity Market (pp. 1-6). IEEE.                                                           |\n",
    "| [Ortner and Totschnig 2019]                      | Ortner, A., & Totschnig, G. (2019). The future relevance of electricity balancing markets in Europe-A 2030 case study. Energy Strategy Reviews, 24, 111-120.                                                                                          |\n",
    "| [Picasso et al. 2019]                            | Picasso et al., 2019: Technical analysis and sentiment embeddings for market trend prediction                                                                                                                                                         |\n",
    "| [Schumacher et al. 2015]                         | Schumacher, Ingrid, et al. \"Der strommarkt und die strompreisbildung.\" Strategien zur Strombeschaffung in Unternehmen: Energieeinkauf optimieren, Kosten senken (2015): 9-37.                                                                         |\n",
    "| [Shur et al. 2023]                               | Shchur, O., Turkmen, C., Erickson, N., Shen, H., Shirkov, A., Hu, T., Wang, T. (2023). AutoGluon-TimeSeries: AutoML for Probabilistic Time Series Forecasting. https://arxiv.org/abs/2308.05566                                                       |\n",
    "| [Steinmetz et al. 2022]                          | Steinmetz, H., Batzdorfer, V., Scherhag, J., & Bosnjak, M. (2022). The ZPID Lockdown Measures Dataset for Germany [Data set]. PsychArchives. https://doi.org/10.23668/PSYCHARCHIVES.6676                                                              |\n",
    "| [Hochreiter et al. 1997]                         | Hochreiter, Sepp & Schmidhuber, Jürgen. (1997). Long Short-Term Memory. Neural Computation. 9. 1735-1780. 10.1162/neco.1997.9.8.1735.                                                                                                                 |\n",
    "| [Schmidhuber et al. 2003]                        | Fakultät Informatik & Bengio, Y. & Frasconi, Paolo & Schmidhuber, Jürgen. (2003). Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies. A Field Guide to Dynamical Recurrent Neural Networks.                           |\n",
    "| [Bengio et al. 1993]                             | Y. Bengio, P. Frasconi and P. Simard, \"The problem of learning long-term dependencies in recurrent networks,\" IEEE International Conference on Neural Networks, San Francisco, CA, USA, 1993, pp. 1183-1188 vol.3, doi: 10.1109/ICNN.1993.298725.     |\n",
    "| [Parthipan et al. 2024]                          | Parthipan, R., Anand, M., Christensen, H. M., Hosking, J. S., & Wischik, D. J. (2024). Defining error accumulation in ML atmospheric simulators. arXiv preprint arXiv:2405.14714.                                                                     |\n",
    "| [Bahdanau et al. 2014]                           | Bahdanau, D. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.                                                                                                                           |\n",
    "| [Hochreiter et al. 1997]                         | Hochreiter, Sepp & Schmidhuber, Jürgen. (1997). Long Short-Term Memory. Neural Computation. 9. 1735-1780. 10.1162/neco.1997.9.8.1735.                                                                                                                 |\n",
    "| [Schmidhuber et al. 2003]                        | Informatik, Fakultit & Bengio, Y. & Frasconi, Paolo & Schmidhuber, Jfirgen. (2003). Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies. A Field Guide to Dynamical Recurrent Neural Networks.                         |\n",
    "| [Bengio et al. 1993]                             | Y. Bengio, P. Frasconi and P. Simard, \"The problem of learning long-term dependencies in recurrent networks,\" IEEE International Conference on Neural Networks, San Francisco, CA, USA, 1993, pp. 1183-1188 vol.3, doi: 10.1109/ICNN.1993.298725.     |\n",
    "| [Sutskever et al. 2014]                          | Sutskever, I. (2014). Sequence to Sequence Learning with Neural Networks. arXiv preprint arXiv:1409.3215.                                                                                                                                             |\n",
    "| [Zhao et al. 2020]                               | Zhao, J., Huang, F., Lv, J., Duan, Y., Qin, Z., Li, G., & Tian, G. (2020, November). Do RNN and LSTM have long memory?. In International Conference on Machine Learning (pp. 11365-11375). PMLR.                                                      |\n",
    "| [Kandadi et al. 2025]                            | Kandadi, T., & Shankarlingam, G. (2025). DRAWBACKS OF LSTM ALGORITHM: A CASE STUDY. Available at SSRN 5080605.                                                                                                                                        |\n",
    "| [Bahdanau et al. 2014]                           | Bahdanau, D. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.                                                                                                                           |\n",
    "| [Tschora et al. 2022]                            | Tschora, L., Pierre, E., Plantevit, M., & Robardet, C. (2022). Electricity price forecasting on the day-ahead market using machine learning. Applied Energy, 313, 118752.                                                                             |\n",
    "| [Vaswani et al. 2017]                            | Attention is all you need. Advances in Neural Information Processing Systems.                                                                                                                                                                         |\n",
    "| [Yun Bai et al. 2024]                            | Yun Bai et al., 2024: News and Load: A Quantitative Exploration of Natural Language Processing Applications for Forecasting Day-Ahead Electricity System Demand                                                                                       |\n",
    "| [Netztransparenz, Index-Ausgleichspreis, 2024]   | Netztransparenz.de, 50Hertz Transmission GmbH; Amprion GmbH; TenneT TSO GmbH; TransnetBW GmbH, 16.12.2024, URL: https://www.netztransparenz.de/de-de/Regelenergie/Ausgleichsenergiepreis/Index-Ausgleichsenergiepreis                                 |\n",
    "| [Smard, Großhandelspreise, 2024]                 | Smard.de, Bundesnetzagentur, 15.12.2024, URL: https://www.smard.de/page/home/wiki-article/446/562                                                                                                                                                     |\n",
    "| [Smard, Negative wholesale prices, 2025]         | Smard.de, Bundesnetzagentur, 13.02.2025, URL: https://www.smard.de/page/en/wiki-article/5884/105426                                                                                                                                                   |\n",
    "| [Smard user guide, 2024]                         | Smard.de, Bundesnetzagentur, 15.12.2024, URL: https://www.smard.de/resource/blob/205652/63fcff2c9813096fa2229d769da164ef/smard-user-guide-09-2021-data.pdf                                                                                           |\n",
    "| [Investing, Carbon Emissions Futures, 2024]      | Investing.com, Fusion Media Limited, 16.12.2024, URL: https://www.investing.com/commodities/carbon-emissions-historical-data                                                                                                                          |\n",
    "| [European Commission, EU ETS, 2024]              | commission.europa.eu, European Commission, 16.12.2024, URL: https://climate.ec.europa.eu/eu-action/eu-emissions-trading-system-eu-ets_en                                                                                                              |\n",
    "| [Finanztools, Inflationsraten Deutschland, 2025] | Finanz-tools.de, Klaudia Will, 12.02.2025, URL: https://www.finanz-tools.de/inflation/inflationsraten-deutschland                                                                                                                                     |"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Acknowledgement \n",
    "The authors gratefully acknowledge the computing time made available to them on the high-performance computer at the NHR Center of TU Dresden. This center is jointly supported by the Federal Ministry of Education and Research and the state governments participating in the NHR (www.nhr-verein.de/unsere-partner).\n",
    "\n",
    "We sincerely thank our supervisors, Jimmy Pöhlmann, Claudio Hartmann, and Wolfgang Lehner, for their guidance and support throughout the semester and the development of this report. Their expertise, feedback, and encouragement played an important role in shaping our work. We appreciate their time and effort in mentoring us through this process.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
