{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BTW 2025 Data Science Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "Forecasting market influences, supply, demand, and ultimately prices is of paramount importance for any entity operating within a market. \n",
    "This is particularly salient in the context of the electricity market, where the stability of the grid is contingent upon the precarious equilibrium of supply and demand, with any disruption to this balance having the potential to severely impact the grid's reliability.\n",
    "Consequently, the field of electricity price forecasting stands as a subject of considerable relevance for both market participants and academic researchers.\n",
    "Historically, these predictions have relied on statistical methods, as exemplified by Contreras et al. (2003) using ARIMA models to predict next day's prices or Nunes et al. (2008) using SARIMA and GARCH models with moving averages.\n",
    "In recent years, machine learning has become a prominent feature of these attempts. An illustration of this phenomenon can be found in the works of Lago et al. (2018), who reported substantial success in employing deep learning in comparison to conventional statistical methodologies when predicting the next days prices. \n",
    "Another recent example proving the merit of machine learning in electricity price forecasting was achieved by Tschora et al. (2022). In their research they evaluated several machine learning approaches and achieved state-of-the-art results. Furthermore, they were able to illustrate the impact of specific influences of the final price using xAI, enabling an informed selection of features and input data for price prediction. \n",
    "\n",
    "Inspired by the BTW 2025 Data Science Challenge and many such pioneers, the following report details the work of the Dresden group over the last semester, attempting to forecast the day-ahead-price via a large range of methods to forecast time series.\n",
    "These methods encompass a wide spectrum, ranging from xAI integrated with conventional forecasting techniques such as ARIMA and ETS, to advanced models including (X)LSTM and transformer-based models, culminating in highly optimized approaches enabled by AutoML, exemplified by AutoGluon. \n",
    "Nevertheless, it is imperative to acknowledge that no single approach can attain results that surpass the quality of the input data. The following work therefore also details the groups work in collecting, preparing and analyzing data pertaining to the German electricity prices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installs for real-time execution\n",
    "!pip install git+https://github.com/amazon-science/chronos-forecasting.git\n",
    "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install panda\n",
    "!pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autogluon\n",
    "!pip install statsmodels\n",
    "!pip install python-dotenv\n",
    "!pip install entsoe-py\n",
    "!pip install newsapi-python\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import final\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Base Model Class\n",
    "\"\"\"\n",
    "Base model structure.\n",
    "Every Forecasting model should inherit from this.\n",
    "\n",
    "Note: override all abstract methods and keep the final methods unchanged\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import final\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from chronos import ChronosPipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "class BaseModel(ABC):\n",
    "\n",
    "    def __init__(self, model_name: str, model_type: str):\n",
    "        \"\"\"Init and create model.\n",
    "\n",
    "        :param model_name: Name of your model\n",
    "        :param model_type: Type of your model e.g. LSTM\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model_type = model_type\n",
    "        self.model = None  # this is a placeholder for your model\n",
    "        self.__create_model()\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_model(self):\n",
    "        \"\"\"Define your own model under self.model.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, X_train: pd.DataFrame, y_train: pd.DataFrame,\n",
    "              X_val: pd.DataFrame = None, y_val: pd.DataFrame = None,\n",
    "              X_test: pd.DataFrame = None, y_test: pd.DataFrame = None) -> pd.DataFrame | None:\n",
    "        \"\"\"train the model on the training data.\n",
    "        test and validation data can be used only for evaluation (if available).\n",
    "\n",
    "        :param X_train: training features dataset\n",
    "        :param y_train: training target values\n",
    "        :param X_val: validation features' dataset\n",
    "        :param y_val: validation target values\n",
    "        :param X_test: testing features' dataset\n",
    "        :param y_test: testing target values\n",
    "        :return: training history (losses while training, if available else None) [epoch | train_loss | test_loss]\n",
    "        \"\"\"\n",
    "        # call the training loop/function of your model\n",
    "        # and return a history (if available, otherwise None)\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def run_prediction(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"run prediction on your defined model\n",
    "\n",
    "        :param X: features dataset\n",
    "        :return: prediction output, [timestamp | value]\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @final\n",
    "    def predict(self, X: pd.DataFrame, exp_dir: str = None) -> pd.DataFrame:\n",
    "        \"\"\"call this to run prediction\n",
    "\n",
    "        :param X: features dataset\n",
    "        :param exp_dir: dir to store prediction result\n",
    "        :return: prediction output, [timestamp | value]\n",
    "        \"\"\"\n",
    "        # run your custom prediction\n",
    "        prediction_results = self.__run_prediction(X)\n",
    "\n",
    "        # store if dir is provided\n",
    "        if exp_dir is not None:\n",
    "            prediction_results.to_csv(f'{exp_dir}\\\\{self.model_type}_{self.model_name}_prediction.csv')\n",
    "        return prediction_results\n",
    "\n",
    "    @abstractmethod\n",
    "    def custom_save(self, model: object, filename: str):\n",
    "        \"\"\"Use your own dataformat to save your model here\n",
    "\n",
    "        :param filename: filename or path\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def custom_load(self, filename: str) -> object:\n",
    "        \"\"\"Use your own dataformat to load your model here\n",
    "\n",
    "        :param filename: filename or path\n",
    "        :return: your loaded model\n",
    "        \"\"\"\n",
    "        # return model\n",
    "        ...\n",
    "\n",
    "    @final\n",
    "    def save(self, exp_dir: str):\n",
    "        \"\"\"call this to save self.model.\n",
    "\n",
    "        :param exp_dir: dir name or path to dir\n",
    "        \"\"\"\n",
    "        self.__custom_save(model=self.model, filename=f'{exp_dir}\\\\{self.model_type}_{self.model_name}')\n",
    "\n",
    "    @final\n",
    "    def load(self, exp_dir: str):\n",
    "        \"\"\"call this to load a retrained model\n",
    "\n",
    "        :param exp_dir: dir name or path to dir\n",
    "        \"\"\"\n",
    "        self.model = self.__custom_load(filename=f'{exp_dir}\\\\{self.model_type}_{self.model_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chronos import ChronosPipeline\n",
    "\n",
    "class ChronosModel(BaseModel):\n",
    "    def __init__(self, model_name: str, model_type: str):\n",
    "        \"\"\"Call the BaseModel constructor with the required arguments.\"\"\"\n",
    "        super().__init__(model_name, model_type)\n",
    "\n",
    "    def create_model(self):\n",
    "        self.model = None\n",
    "\n",
    "    \n",
    "    def run_prediction(self, X):\n",
    "\n",
    "        #define which column to be forecasted and forecast legth\n",
    "        target_column = \"day_ahead_prices_EURO\"\n",
    "        prediction_length = 24\n",
    "\n",
    "        \n",
    "        context = torch.tensor(X[target_column].values)[-512:]  # Limit context to last 512 samples\n",
    "        forecast = self.model.predict(context, prediction_length)\n",
    "        low, median, high = np.quantile(forecast[0].numpy(), [0.1, 0.5, 0.9], axis=0)\n",
    "\n",
    "        context_dates = X.index[-512:]\n",
    "        last_date = context_dates[-1]\n",
    "        forecast_index = pd.date_range(last_date + pd.Timedelta(hours=1), periods=prediction_length, freq=\"h\")\n",
    "\n",
    "        prediction_results = pd.DataFrame({\n",
    "            \"timestamp\": forecast_index,\n",
    "            \"forecasted_values\": median\n",
    "        })\n",
    "        \n",
    "        return prediction_results\n",
    "\n",
    "    \n",
    "    def custom_load(self, model_dir):\n",
    "        \n",
    "        # Load the model pipeline\n",
    "        pipeline = ChronosPipeline.from_pretrained(\n",
    "            model_dir,\n",
    "            device_map=(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "        self.model = pipeline  # Set the model to the loaded pipeline\n",
    "\n",
    "        return pipeline\n",
    "    \n",
    "    \n",
    "                \n",
    "    def custom_save(self, model = None, filename = None):\n",
    "        return\n",
    "\n",
    "    def train(self, X_train = None, y_train = None, X_val = None, y_val = None, X_test = None, y_test = None):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO Load Data\n",
    "\n",
    "\n",
    "\n",
    "# TODO All Models\n",
    "\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "class ETSModel(BaseModel):\n",
    "    def create_model(self):\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, X_train, y_train, X_val = None, y_val = None, X_test = None, y_test = None):\n",
    "        #time_series = X_train\n",
    "        if y_train.isnull().any():\n",
    "            print(\"Warning: Missing values in y_train are being automatically filled by the ETSModel.\")\n",
    "            y_train = y_train.ffill()\n",
    "        model = ExponentialSmoothing(y_train, seasonal_periods=24, trend=\"add\", seasonal=\"add\")\n",
    "        self.model = model.fit()\n",
    "        return None\n",
    "    \n",
    "    def run_prediction(self, X):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"The model is not trained. Call `train` before prediction.\")\n",
    "        forecast = self.model.forecast(len(X))\n",
    "        #print(forecast)\n",
    "        #print(X)\n",
    "        prediction_results = pd.DataFrame({\n",
    "            'timestamp': X['Date'],  # Use the 'Date' column in X for timestamps\n",
    "            'value': forecast.values\n",
    "        })\n",
    "        return prediction_results\n",
    "\n",
    "    def custom_save(self, model, filename):\n",
    "         with open(f\"{filename}.pkl\", \"wb\") as file:\n",
    "            pickle.dump(model, file)\n",
    "    \n",
    "    def custom_load(self, filename):\n",
    "        with open(f\"{filename}.pkl\", \"rb\") as file:\n",
    "            loaded_model = pickle.load(file)\n",
    "        return loaded_model\n",
    "    \n",
    "\n",
    "class MultivarLSTM(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers  # number of recurrent layers in the lstm\n",
    "        self.input_size = input_size  # number of input features\n",
    "        self.output_size = output_size  # output sequence length\n",
    "        self.hidden_size = hidden_size  # neurons in each lstm layer\n",
    "        # LSTM model\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=0.2,\n",
    "                            bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_size * 24 * 2, output_size)\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        # hidden state init\n",
    "        h_0 = Variable(torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size, device=device))\n",
    "        # cell state init\n",
    "        c_0 = Variable(torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size, device=device))\n",
    "        # propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0))  # (input, hidden, and internal state)\n",
    "        # reduce dimension to the required output sequence length\n",
    "        predictions = self.linear(output.reshape(output.size(0), output.size(1) * output.size(2)))\n",
    "        pred_out = self.activation(predictions)\n",
    "\n",
    "        return pred_out\n",
    "\n",
    "\n",
    "class MultivariateBiLSTM(BaseModel):\n",
    "    def __init__(self, features, target):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.input_length = 24\n",
    "        self.hidden_layer_size = 256\n",
    "        self.num_layers = 12\n",
    "        self.output_length = 24\n",
    "        # Check For GPU -> If available send model and data to it\n",
    "        self.device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        super(MultivariateBiLSTM, self).__init__(model_name='MultivarLSTM',\n",
    "                                                 model_type='MultivariateBidirectionalLSTM')\n",
    "\n",
    "    def create_model(self):\n",
    "        self.model = MultivarLSTM(num_layers=self.num_layers, hidden_size=self.hidden_layer_size,\n",
    "                                  input_size=len(self.features), output_size=self.output_length)\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def train(self, X_train: pd.DataFrame, y_train: pd.DataFrame,\n",
    "              X_val: pd.DataFrame = None, y_val: pd.DataFrame = None,\n",
    "              X_test: pd.DataFrame = None, y_test: pd.DataFrame = None,\n",
    "              n_epochs=500, batch_size=1024, learning_rate=0.001) -> pd.DataFrame | None:\n",
    "        self.target_scaler = MinMaxScaler()\n",
    "        self.feature_scaler = StandardScaler()\n",
    "\n",
    "        # select features and target columns\n",
    "        if len(self.features) > 0:\n",
    "            # use only selected features (of self.features not defined: use all columns as features)\n",
    "            X_train = X_train[self.features]\n",
    "            X_test = X_test[self.features]\n",
    "            X_val = X_val[self.features]\n",
    "        y_train = y_train[self.target].values\n",
    "        y_test = y_test[self.target].values\n",
    "        y_val = y_val[self.target].values\n",
    "\n",
    "        # fit scalar on training data\n",
    "        self.feature_scaler.fit(X_train)\n",
    "        self.target_scaler.fit(y_train.reshape(-1, 1))\n",
    "\n",
    "        X_train = self.feature_scaler.transform(X_train)\n",
    "        y_train = self.target_scaler.transform(y_train.reshape(-1, 1))\n",
    "        X_test = self.feature_scaler.transform(X_test)\n",
    "        y_test = self.target_scaler.transform(y_test.reshape(-1, 1))\n",
    "        X_val = self.feature_scaler.transform(X_val)\n",
    "        y_val = self.target_scaler.transform(y_val.reshape(-1, 1))\n",
    "\n",
    "        # convert dataset to tensors suitable for training the model\n",
    "        X_train_tensors = self.__prepare_feature_dataset(X_train)\n",
    "        y_train_tensors = self.__prepare_target_dataset(y_train)\n",
    "        X_test_tensors = self.__prepare_feature_dataset(X_test)\n",
    "        y_test_tensors = self.__prepare_target_dataset(y_test)\n",
    "        X_val_tensors = self.__prepare_feature_dataset(X_val)\n",
    "        y_val_tensors = self.__prepare_target_dataset(y_val)\n",
    "\n",
    "        history = self.__training_loop(n_epochs=n_epochs,\n",
    "                                       X_train=X_train_tensors,\n",
    "                                       y_train=y_train_tensors,\n",
    "                                       X_test=X_test_tensors,\n",
    "                                       y_test=y_test_tensors,\n",
    "                                       batch_size=batch_size,\n",
    "                                       learning_rate=learning_rate)\n",
    "\n",
    "    def __training_loop(self, n_epochs, X_train, y_train, X_test, y_test, batch_size, learning_rate):\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        loss_fn = torch.nn.MSELoss()  # mean-squared error for regression\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        train_history = {'epoch': [], 'train loss': [], 'test loss': []}\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            train_losses = []\n",
    "            self.model.train()\n",
    "            for seq, labels in train_loader:\n",
    "                outputs = self.model(seq)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_losses.append(loss.to('cpu').item())\n",
    "            # test loss\n",
    "            test_losses = []\n",
    "            self.model.eval()\n",
    "            for seq, labels in test_loader:\n",
    "                test_preds = self.model(seq)\n",
    "                test_loss = loss_fn(test_preds, labels)\n",
    "                test_losses.append(test_loss.to('cpu').item())\n",
    "            if epoch % 1 == 0:\n",
    "                print(\n",
    "                    f'Epoch {epoch + 1}:\\t|\\tTrain_Loss: {round(sum(train_losses) / len(train_losses), 5)}\\t|\\t'\n",
    "                    f'Test_Loss: {round(sum(test_losses) / len(test_losses), 5)}')\n",
    "                train_history['epoch'].append(epoch + 1)\n",
    "                train_history['train loss'].append((sum(train_losses) / len(train_losses)))\n",
    "                train_history['test loss'].append((sum(test_losses) / len(test_losses)))\n",
    "        train_history = pd.DataFrame(train_history).set_index('epoch')\n",
    "\n",
    "        return train_history\n",
    "\n",
    "    def run_prediction(self, X: pd.DataFrame, batch_size=1024) -> pd.DataFrame:\n",
    "        \"\"\"run prediction on your defined model.\n",
    "\n",
    "        :param X: features dataset\n",
    "        :return: prediction output, [timestamp | value]\n",
    "        \"\"\"\n",
    "        X = X.reset_index(names='timestamp')\n",
    "        timestamps = X['timestamp']\n",
    "        X = X.drop(['timestamp'], axis=1)\n",
    "\n",
    "        # scale features using the training scaler\n",
    "        X = self.feature_scaler.transform(X[self.features])\n",
    "        # prepare dataset\n",
    "        X_pred_tensors = self.__prepare_feature_dataset(X)\n",
    "        dataset = TensorDataset(X_pred_tensors)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        pred_list = []\n",
    "        # predict each batch\n",
    "        for seq in data_loader:\n",
    "            X_batch = seq[0]\n",
    "            predictions = self.model.forward(X_batch)\n",
    "            pred_list.append(predictions.to('cpu').detach().numpy())\n",
    "\n",
    "        pred_np = np.concatenate(pred_list)\n",
    "        pred_steps = pred_np[::self.output_length]\n",
    "        pred_shaped = np.reshape(pred_steps, pred_steps.shape[0] * pred_steps.shape[1]).reshape(-1, 1)\n",
    "        # rescale using training scaler and reshape into one continuous sequence\n",
    "        pred_sequence = self.target_scaler.inverse_transform(pred_shaped).reshape(-1)\n",
    "\n",
    "        df_result = pd.DataFrame({'timestamp': timestamps[:pred_sequence.shape[0]],\n",
    "                                  'day_ahead_price_predicted': pred_sequence})\n",
    "        return df_result\n",
    "\n",
    "    def __prepare_feature_dataset(self, X):\n",
    "        X_seq = self.__split_feature_sequences(features_seq=X)\n",
    "\n",
    "        X_tensor = Variable(torch.Tensor(X_seq))\n",
    "        X_tensor_format = torch.reshape(X_tensor, (X_tensor.shape[0], self.input_length, X_tensor.shape[2]))\n",
    "        X_tensor_format = X_tensor_format.to(self.device)\n",
    "\n",
    "        return X_tensor_format\n",
    "\n",
    "    def __prepare_target_dataset(self, y):\n",
    "        y_seq = self.__split_target_sequences(y)\n",
    "\n",
    "        y_tensor = Variable(torch.Tensor(y_seq))\n",
    "        y_tensor = y_tensor.to(self.device)\n",
    "\n",
    "        return y_tensor\n",
    "\n",
    "    def __split_feature_sequences(self, features_seq):\n",
    "        X = []  # instantiate X and y\n",
    "        for i in range(len(features_seq)):\n",
    "            # find the end of the sequence\n",
    "            end_ix = i + self.input_length\n",
    "            # check if we are beyond the dataset\n",
    "            if end_ix > len(features_seq):\n",
    "                break\n",
    "            # gather input and output of the pattern\n",
    "            seq_x = features_seq[i:end_ix]\n",
    "            X.append(seq_x)\n",
    "        return np.array(X)\n",
    "\n",
    "    def __split_target_sequences(self, target_seq):\n",
    "        y = []  # instantiate y\n",
    "        for i in range(len(target_seq)):\n",
    "            # find the end of the sequence\n",
    "            end_ix = i + self.input_length\n",
    "            # check if we are beyond the dataset\n",
    "            if end_ix > len(target_seq):\n",
    "                break\n",
    "            # gather input and output of the pattern\n",
    "            seq_y = target_seq[i:end_ix, -1]\n",
    "            y.append(seq_y)\n",
    "        return np.array(y)\n",
    "\n",
    "    def create_scalers(self, X_train, y_train):\n",
    "        self.target_scaler = MinMaxScaler()\n",
    "        self.feature_scaler = StandardScaler()\n",
    "\n",
    "        # select features and target columns\n",
    "        if len(self.features) > 0:\n",
    "            # use only selected features (of self.features not defined: use all columns as features)\n",
    "            X_train = X_train[self.features]\n",
    "        y_train = y_train[self.target].values\n",
    "\n",
    "        # fit scalar on training data\n",
    "        self.feature_scaler.fit(X_train)\n",
    "        self.target_scaler.fit(y_train.reshape(-1, 1))\n",
    "\n",
    "    def custom_load(self, filename: str) -> object:\n",
    "        \"\"\"Use your own dataformat to load your model here\n",
    "\n",
    "        :param filename: filename or path\n",
    "        :return: your loaded model\n",
    "        \"\"\"\n",
    "        self.model.load_state_dict(torch.load(filename))\n",
    "        self.model.eval()\n",
    "        return self.model\n",
    "\n",
    "    def custom_save(self, model: object, filename: str):\n",
    "        \"\"\"Use your own dataformat to save your model here\n",
    "\n",
    "        :param filename: filename or path\n",
    "        \"\"\"\n",
    "        torch.save(self.model.state_dict(), filename)\n",
    "        \n",
    "        \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 3, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"forward calculating the attention context and weights\n",
    "        :param hidden: (batch_size, 1, hidden_dim)\n",
    "        :param encoder_outputs: (batch_size, seq_len, hidden_dim)\n",
    "        :return: attention context and weights\n",
    "        \"\"\"\n",
    "        # Repeat decoder hidden state across sequence length\n",
    "        hidden = hidden.repeat(1, encoder_outputs.size(1), 1)  # (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # concatenate hidden and encoder outputs\n",
    "        combined = torch.cat((hidden, encoder_outputs), dim=2)  # (batch_size, seq_len, hidden_dim * 2)\n",
    "\n",
    "        # calculate attention scores\n",
    "        energy = torch.tanh(self.attn(combined))  # (batch_size, seq_len, hidden_dim)\n",
    "        scores = self.v(energy).squeeze(2)  # (batch_size, seq_len)\n",
    "\n",
    "        # apply softmax to get attention weights\n",
    "        attn_weights = torch.softmax(scores, dim=1)  # (batch_size, seq_len)\n",
    "\n",
    "        # Compute context vector as weighted sum of encoder outputs\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)  # (batch_size, 1, hidden_dim)\n",
    "\n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1, bidir=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=bidir)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(x.device)    # if bidir\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # output: (batch_size, seq_len, num_layer*hidden_dim)\n",
    "        # hidden: (num_layers*2, batch_size, hidden_dim)        -> *2 for num_layers if bidirectional=True\n",
    "        # cell: (num_layers*2, batch_size, hidden_dim)\n",
    "        output, (hidden, cell) = self.lstm(x, (h0, c0))\n",
    "        return output, hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim=1, hidden_dim=16, num_layers=1, num_heads=4, bidir=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.lstm = nn.LSTM(1 + (2 * hidden_dim), hidden_dim, self.num_layers, batch_first=True, bidirectional=bidir)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "\n",
    "        self.attention = Attention(hidden_dim)\n",
    "\n",
    "    def forward(self, x, hidden, cell, encoder_out):\n",
    "        # Compute attention\n",
    "        # context: (batch_size, 1, hidden_dim*num_layers)\n",
    "        # att_weights: (batch_size, seq_len)\n",
    "        context, att_weights = self.attention(hidden[-1].unsqueeze(1), encoder_out)\n",
    "\n",
    "        # Concatenate context vector and decoder input\n",
    "        # x: (batch_size, 1, hidden_dim + output_dim)       \n",
    "        x = torch.cat((x, context), dim=2)\n",
    "        # output: (batch_size, seq_len, num_layer*hidden_dim)\n",
    "        # hidden: (num_layers*2, batch_size, hidden_dim)        -> *2 for num_layers if bidirectional=True\n",
    "        # cell: (num_layers*2, batch_size, hidden_dim)\n",
    "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden, cell, att_weights\n",
    "\n",
    "\n",
    "class EncDecLSTM(nn.Module):\n",
    "    def __init__(self, encoder, decoder, target_length):\n",
    "        super(EncDecLSTM, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.target_length = target_length\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, dec_init=None):\n",
    "        \"\"\"Forward pass the input sequences, containing feature space through the\n",
    "        Encoder-Attention-Decoder model and output the predicted sequence\n",
    "\n",
    "        :param x: input sequences tensor\n",
    "        :param dec_init: encoder input price value\n",
    "        :return: prediction sequence\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length, input_dim = x.size()\n",
    "        encoder_output, hidden, cell = self.encoder(x)\n",
    "        if dec_init is not None:\n",
    "            decoder_input = dec_init\n",
    "        else:\n",
    "            decoder_input = torch.zeros(batch_size, 1, 1).to(x.device)\n",
    "\n",
    "        out = torch.zeros(batch_size, self.target_length, 1).to(x.device)\n",
    "\n",
    "        out, hidden, cell, attn_weights = self.decoder(decoder_input, hidden, cell, encoder_output)\n",
    "        out = out.squeeze(1)\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EncoderDecoderAttentionLSTM(BaseModel):\n",
    "    def __init__(self, target_length, features, target, hidden_size=64, num_layers=3):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.target = target\n",
    "        self.features = features\n",
    "        self.target_length = target_length\n",
    "        # Check For GPU -> If available send model and data to it\n",
    "        self.device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        super(EncoderDecoderAttentionLSTM, self).__init__(model_name='EncDecAttLSTM',\n",
    "                                                          model_type='EncoderDecoderAttentionLSTM')\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"Define your own model under self.model.\n",
    "        \"\"\"\n",
    "        num_heads = 1\n",
    "        input_size = len(self.features)\n",
    "        self.input_length = 24\n",
    "        # case seq2seq decoder: use output_size = self.output_length\n",
    "        # case autoregressive decoder: use output_size = 1\n",
    "        output_size = self.target_length\n",
    "\n",
    "        Enc = Encoder(input_dim=input_size, hidden_dim=self.hidden_size, num_layers=self.num_layers, bidir=True)\n",
    "        Dec = Decoder(output_dim=output_size, hidden_dim=self.hidden_size, num_layers=self.num_layers,\n",
    "                      num_heads=num_heads, bidir=True)\n",
    "        self.model = EncDecLSTM(encoder=Enc, decoder=Dec, target_length=self.target_length)\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def train(self, X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame = None,\n",
    "              y_val: pd.DataFrame = None, X_test: pd.DataFrame = None,\n",
    "              y_test: pd.DataFrame = None, n_epochs: int = 500, batch_size: int = 1024,\n",
    "              learning_rate: float = 0.001) -> pd.DataFrame | None:\n",
    "        \"\"\"train the model on the training data.\n",
    "        test and validation data can be used only for evaluation (if available).\n",
    "\n",
    "        :param X_train: training features dataset\n",
    "        :param y_train: training target values\n",
    "        :param X_val: validation features' dataset\n",
    "        :param y_val: validation target values\n",
    "        :param X_test: testing features' dataset\n",
    "        :param y_test: testing target values\n",
    "        :param n_epochs: number of training iterations\n",
    "        :param batch_size: size of each processed chunk of data in trainings loop\n",
    "        :param learning_rate: learning rate\n",
    "        :return: training history (losses while training, if available else None) [epoch | train_loss | test_loss]\n",
    "        \"\"\"\n",
    "\n",
    "        self.target_scaler = MinMaxScaler()\n",
    "        self.feature_scaler = StandardScaler()\n",
    "\n",
    "        # select features and target columns\n",
    "        if len(self.features) > 0:\n",
    "            # use only selected features (of self.features not defined: use all columns as features)\n",
    "            X_train = X_train[self.features]\n",
    "            X_test = X_test[self.features]\n",
    "            X_val = X_val[self.features]\n",
    "        y_train = y_train[self.target].values\n",
    "        y_test = y_test[self.target].values\n",
    "        y_val = y_val[self.target].values\n",
    "\n",
    "        # fit scalar on training data\n",
    "        self.feature_scaler.fit(X_train)\n",
    "        self.target_scaler.fit(y_train.reshape(-1, 1))\n",
    "\n",
    "        X_train = self.feature_scaler.transform(X_train)\n",
    "        y_train = self.target_scaler.transform(y_train.reshape(-1, 1))\n",
    "        X_test = self.feature_scaler.transform(X_test)\n",
    "        y_test = self.target_scaler.transform(y_test.reshape(-1, 1))\n",
    "        X_val = self.feature_scaler.transform(X_val)\n",
    "        y_val = self.target_scaler.transform(y_val.reshape(-1, 1))\n",
    "\n",
    "        # convert dataset to tensors suitable for training the model\n",
    "        X_train_tensors = self.__prepare_feature_dataset(X_train)\n",
    "        y_train_tensors = self.__prepare_target_dataset(y_train)\n",
    "        X_test_tensors = self.__prepare_feature_dataset(X_test)\n",
    "        y_test_tensors = self.__prepare_target_dataset(y_test)\n",
    "        X_val_tensors = self.__prepare_feature_dataset(X_val)\n",
    "        y_val_tensors = self.__prepare_target_dataset(y_val)\n",
    "\n",
    "        loss_fn = torch.nn.MSELoss()  # mean-squared error for regression\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        history = self.__training_loop(n_epochs=n_epochs,\n",
    "                                       optimiser=optimizer,\n",
    "                                       loss_fn=loss_fn,\n",
    "                                       X_train=X_train_tensors,\n",
    "                                       y_train=y_train_tensors,\n",
    "                                       X_test=X_test_tensors,\n",
    "                                       y_test=y_test_tensors,\n",
    "                                       batch_size=batch_size)\n",
    "        return history\n",
    "\n",
    "    def __training_loop(self, n_epochs, optimiser, loss_fn, X_train, y_train,\n",
    "                        X_test, y_test, batch_size):\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        train_history = {'epoch': [], 'train loss': [], 'test loss': []}\n",
    "\n",
    "        min_loss = 1000\n",
    "        model_state = self.model.state_dict()\n",
    "        for epoch in range(n_epochs):\n",
    "            train_losses = []\n",
    "            self.model.train()\n",
    "            timestep = 0\n",
    "            for seq, labels in train_loader:\n",
    "                decoder_input = seq[:, -1:, -1:]\n",
    "                outputs = self.model(seq, decoder_input)\n",
    "                optimiser.zero_grad()\n",
    "\n",
    "                loss = torch.sqrt(loss_fn(outputs, labels))\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "                train_losses.append(loss.to('cpu').item())\n",
    "                timestep += 1\n",
    "            # test loss\n",
    "            test_losses = []\n",
    "            self.model.eval()\n",
    "            # for seq, labels in tqdm(test_loader):\n",
    "            for seq, labels in test_loader:\n",
    "                decoder_input = seq[:, -1:, -1:]\n",
    "                test_preds = self.model(seq, decoder_input)\n",
    "\n",
    "                test_loss = torch.sqrt(loss_fn(test_preds, labels))\n",
    "                test_losses.append(test_loss.to('cpu').item())\n",
    "            if epoch % 1 == 0:\n",
    "                print(\n",
    "                    f'Epoch {epoch + 1}:\\t|\\tTrain_Loss: {round(sum(train_losses) / len(train_losses), 5)}\\t|\\t'\n",
    "                    f'Test_Loss: {round(sum(test_losses) / len(test_losses), 5)}')\n",
    "                train_history['epoch'].append(epoch + 1)\n",
    "                train_history['train loss'].append((sum(train_losses) / len(train_losses)))\n",
    "                train_history['test loss'].append((sum(test_losses) / len(test_losses)))\n",
    "            if min_loss > (sum(test_losses) / len(test_losses)):\n",
    "                model_state = self.model.state_dict()\n",
    "                min_loss = (sum(test_losses) / len(test_losses))\n",
    "\n",
    "        train_history = pd.DataFrame(train_history).set_index('epoch')\n",
    "\n",
    "        return train_history\n",
    "\n",
    "    def create_scalers(self, X_train, y_train):\n",
    "        self.target_scaler = MinMaxScaler()\n",
    "        self.feature_scaler = StandardScaler()\n",
    "\n",
    "        # select features and target columns\n",
    "        if len(self.features) > 0:\n",
    "            # use only selected features (of self.features not defined: use all columns as features)\n",
    "            X_train = X_train[self.features]\n",
    "        y_train = y_train[self.target].values\n",
    "\n",
    "        # fit scalar on training data\n",
    "        self.feature_scaler.fit(X_train)\n",
    "        self.target_scaler.fit(y_train.reshape(-1, 1))\n",
    "\n",
    "    def run_prediction(self, X: pd.DataFrame, batch_size=1024) -> pd.DataFrame:\n",
    "        \"\"\"run prediction on your defined model.\n",
    "\n",
    "        :param X: features dataset\n",
    "        :return: prediction output, [timestamp | value]\n",
    "        \"\"\"\n",
    "        pred_length = X.shape[0]\n",
    "        X = X.reset_index(names='timestamp')\n",
    "        timestamps = X['timestamp']\n",
    "        X = X.drop(['timestamp'], axis=1)\n",
    "\n",
    "        # scale features using the training scaler\n",
    "        X = self.feature_scaler.transform(X[self.features])\n",
    "        X_pred_tensors = self.__prepare_feature_dataset(X)\n",
    "        dataset = TensorDataset(X_pred_tensors)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        pred_list = []\n",
    "        for seq in data_loader:\n",
    "            X_batch = seq[0]\n",
    "            decoder_input = X_batch[:, -1:, -1:]\n",
    "            predictions = self.model.forward(X_batch, decoder_input)\n",
    "            pred_list.append(predictions.to('cpu').detach().numpy())\n",
    "        pred_np = np.concatenate(pred_list)\n",
    "        pred_steps = pred_np[::self.target_length]\n",
    "        pred_shaped = np.reshape(pred_steps, pred_steps.shape[0] * pred_steps.shape[1]).reshape(-1, 1)\n",
    "        # rescale using training scaler and reshape into one continuous sequence\n",
    "        pred_sequence = self.target_scaler.inverse_transform(pred_shaped).reshape(-1)\n",
    "\n",
    "        df_result = pd.DataFrame({'timestamp': timestamps[:pred_sequence.shape[0]],\n",
    "                                  'day_ahead_price_predicted': pred_sequence})\n",
    "        return df_result\n",
    "\n",
    "    def __prepare_feature_dataset(self, X):\n",
    "\n",
    "        X_seq = self.__split_feature_sequences(features_seq=X)\n",
    "\n",
    "        X_tensor = Variable(torch.Tensor(X_seq))\n",
    "        X_tensor_format = torch.reshape(X_tensor, (X_tensor.shape[0], self.input_length, X_tensor.shape[2]))\n",
    "        X_tensor_format = X_tensor_format.to(self.device)\n",
    "\n",
    "        return X_tensor_format\n",
    "\n",
    "    def __prepare_target_dataset(self, y):\n",
    "\n",
    "        y_seq = self.__split_target_sequences(y)\n",
    "\n",
    "        y_tensor = Variable(torch.Tensor(y_seq))\n",
    "        y_tensor = y_tensor.to(self.device)\n",
    "\n",
    "        return y_tensor\n",
    "\n",
    "    def __split_feature_sequences(self, features_seq):\n",
    "        X = []  # instantiate X\n",
    "        for i in range(len(features_seq)):\n",
    "            # find the end of the input, output sequence\n",
    "            end_ix = i + self.input_length\n",
    "            if end_ix > len(features_seq):\n",
    "                break\n",
    "            # gather input and output of the pattern\n",
    "            seq_x = features_seq[i:end_ix]\n",
    "            X.append(seq_x)\n",
    "        return np.asarray(X)\n",
    "\n",
    "    def __split_target_sequences(self, target_seq):\n",
    "        y = []  # instantiate y\n",
    "        for i in range(len(target_seq)):\n",
    "            # find the end of the input, output sequence\n",
    "            end_ix = i + self.input_length\n",
    "            if end_ix > len(target_seq):\n",
    "                break\n",
    "            # gather input and output of the pattern\n",
    "            seq_y = target_seq[i:end_ix, -1]\n",
    "            y.append(seq_y)\n",
    "        return np.asarray(y)\n",
    "\n",
    "    def custom_save(self, model: object, filename: str):\n",
    "        \"\"\"Use your own dataformat to save your model here\n",
    "\n",
    "        :param filename: filename or path\n",
    "        \"\"\"\n",
    "        torch.save(self.model.state_dict(), filename)\n",
    "        pass\n",
    "\n",
    "    def custom_load(self, filename: str) -> object:\n",
    "        \"\"\"Use your own dataformat to load your model here\n",
    "\n",
    "        :param filename: filename or path\n",
    "        :return: your loaded model\n",
    "        \"\"\"\n",
    "        self.model.load_state_dict(torch.load(filename))\n",
    "        self.model.eval()\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkMaker:\n",
    "    def __init__(self, export_dir):\n",
    "        self.export_dir = export_dir  # output for plots and benchmark .csv files\n",
    "\n",
    "        if not os.path.exists(self.export_dir):\n",
    "            # create the directory in case it does not already exist\n",
    "            os.makedirs(self.export_dir)\n",
    "\n",
    "        # load input data\n",
    "        self.data = None\n",
    "        self.model_names = []\n",
    "\n",
    "    def load_dataframes(self, predictions: dict, prices: pd.DataFrame):\n",
    "        \"\"\"loading datasets of predictions from different models\n",
    "\n",
    "        :param predictions: dict form: {model_name1: df1, model_name2: df2, ...}\n",
    "        :param prices: ground truth prices dataframe\n",
    "        \"\"\"\n",
    "        for k in predictions.keys():\n",
    "            predictions[k] = predictions[k].set_axis([str(k)], axis='columns')\n",
    "            self.model_names.append(str(k))\n",
    "        prices = prices.set_axis(['day_ahead_prices'], axis='columns')\n",
    "        self.align_dataframes(list(predictions.values()) + [prices])\n",
    "\n",
    "    def align_dataframes(self, dataframes):\n",
    "        \"\"\"Align multiple DataFrames with timestamp indices by merging them on their index.\n",
    "        \"\"\"\n",
    "        result_df = dataframes[0]\n",
    "        for df in dataframes[1:]:\n",
    "            result_df = result_df.join(df, how='outer')\n",
    "        self.data = result_df\n",
    "\n",
    "    def calc_errors(self):\n",
    "        no_nan = self.data.copy(deep=True).dropna(how='any')\n",
    "        gt_values = no_nan['day_ahead_prices'].values\n",
    "        for pred_model in self.model_names:\n",
    "            pred_values = no_nan[pred_model].values\n",
    "            self.data[str(pred_model) + '_RMSE'] = self.calc_rmse(gt_values, pred_values)\n",
    "            self.data[str(pred_model) + '_MAE'] = self.calc_mae(gt_values, pred_values)\n",
    "            self.data[str(pred_model) + '_MAPE'] = self.calc_mape(gt_values, pred_values)\n",
    "            self.data[str(pred_model) + '_SE'] = self.calc_squared_error(self.data['day_ahead_prices'].values,\n",
    "                                                                         self.data[pred_model].values)\n",
    "            self.data[str(pred_model) + '_AE'] = self.calc_absolute_error(self.data['day_ahead_prices'].values,\n",
    "                                                                          self.data[pred_model].values)\n",
    "\n",
    "    def calc_squared_error(self, actual_values, predicted_values):\n",
    "        se = (actual_values - predicted_values)**2\n",
    "        return se\n",
    "\n",
    "    def calc_absolute_error(self, actual_values, predicted_values):\n",
    "        ae = abs(actual_values - predicted_values)\n",
    "        return ae\n",
    "\n",
    "    def calc_rmse(self, actual_values, predicted_values):\n",
    "        mse = mean_squared_error(actual_values, predicted_values)\n",
    "        rmse = np.sqrt(mse)\n",
    "        return rmse\n",
    "\n",
    "    def calc_mae(self, actual_values, predicted_values):\n",
    "        mae = mean_absolute_error(actual_values, predicted_values)\n",
    "        return mae\n",
    "\n",
    "    def calc_mape(self, actual_values: np.ndarray, predicted_values: np.ndarray) -> float:\n",
    "        \"\"\"calculate mean average percentage error.\n",
    "        close to 0: good\n",
    "        close to 1: bad\n",
    "\n",
    "        :param actual_values: correct underlying values\n",
    "        :param predicted_values: forecasted values\n",
    "        :return: mape\"\"\"\n",
    "        mape = mean_absolute_percentage_error(actual_values, predicted_values)\n",
    "        return mape\n",
    "\n",
    "    def plot_rmse_per_hour(self):\n",
    "        for model in self.model_names:\n",
    "            plt.plot(self.data.index.values, np.sqrt(self.data[model + '_SE'].values), label=model)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title('RMSE per Hour')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        if self.export_dir is not None:\n",
    "            plt.savefig(self.export_dir + '\\\\' + 'hourly_rmse.png')\n",
    "        plt.show(block=True)\n",
    "\n",
    "    def plot_mae_per_hour(self):\n",
    "        for model in self.model_names:\n",
    "            plt.plot(self.data.index.values, self.data[model + '_AE'].values, label=model)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title('Absolute Error per Hour')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('Absolute Error')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        if self.export_dir is not None:\n",
    "            plt.savefig(self.export_dir + '\\\\' + 'hourly_mae.png')\n",
    "        plt.show(block=True)\n",
    "\n",
    "    def plot_rmse_per_day(self):\n",
    "        copy_df = self.data.reset_index(names='timestamp')\n",
    "        for model in self.model_names:\n",
    "            days_df = copy_df.groupby(copy_df.timestamp.dt.date).mean().reset_index(names='date')\n",
    "            plt.plot(days_df['timestamp'], np.sqrt(days_df[model + '_SE']), label=model)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title('RMSE per Day')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        if self.export_dir is not None:\n",
    "            plt.savefig(self.export_dir + '\\\\' + 'daily_rmse.png')\n",
    "        plt.show(block=True)\n",
    "\n",
    "    def plot_mae_per_day(self):\n",
    "        copy_df = self.data.reset_index(names='timestamp')\n",
    "        for model in self.model_names:\n",
    "            days_df = copy_df.groupby(copy_df.timestamp.dt.date).mean().reset_index(names='date')\n",
    "            plt.plot(days_df['timestamp'], np.sqrt(days_df[model + '_AE']), label=model)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title('MAE per Day')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        if self.export_dir is not None:\n",
    "            plt.savefig(self.export_dir + '\\\\' + 'daily_mae.png')\n",
    "        plt.show(block=True)\n",
    "\n",
    "    def plot_compare_predictions_hourly(self):\n",
    "        gt_values = self.data['day_ahead_prices'].values\n",
    "        timestamps = self.data.index.values\n",
    "        plt.plot(timestamps, gt_values, label='Actual Values')\n",
    "\n",
    "        for model in self.model_names:\n",
    "            pred_values = self.data[model].values\n",
    "            plt.plot(timestamps, pred_values, label=model)\n",
    "\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title('Model Predictions per Hour')\n",
    "        plt.ylabel('Day Ahead Price in ')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        if self.export_dir is not None:\n",
    "            plt.savefig(self.export_dir + '\\\\' + 'hourly_compare_predictions.png')\n",
    "        plt.show(block=True)\n",
    "\n",
    "    def plot_compare_predictions_daily(self):\n",
    "        copy_df = self.data.reset_index(names='timestamp')\n",
    "        days_df = copy_df.groupby(copy_df.timestamp.dt.date).mean().reset_index(\n",
    "            names='date')\n",
    "        timestamps = days_df['timestamp'].values\n",
    "\n",
    "        gt_values = days_df['day_ahead_prices'].values\n",
    "        plt.plot(timestamps, gt_values, label='Actual Values')\n",
    "\n",
    "        for model in self.model_names:\n",
    "            pred_values = days_df[model].values\n",
    "            plt.plot(timestamps, pred_values, label=model)\n",
    "\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title('Model Predictions per Day')\n",
    "        plt.ylabel('Day Ahead Price in ')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        if self.export_dir is not None:\n",
    "            plt.savefig(self.export_dir + '\\\\' + 'daily_compare_predictions.png')\n",
    "        plt.show(block=True)\n",
    "\n",
    "    def plot_compare_mae(self):\n",
    "        no_nan = self.data.copy(deep=True).dropna(how='any')\n",
    "        gt_values = no_nan['day_ahead_prices'].values\n",
    "\n",
    "        mae_values = []\n",
    "        for model in self.model_names:\n",
    "            pred_values = no_nan[model].values\n",
    "            mae = self.calc_mae(gt_values, pred_values)\n",
    "            mae_values.append(mae)\n",
    "        x = np.arange(len(self.model_names))\n",
    "\n",
    "        ax = plt.subplot(111)\n",
    "        my_cmap = plt.get_cmap(\"jet\")\n",
    "        colors = my_cmap(np.linspace(0, 1, len(self.model_names)))\n",
    "        bar = ax.bar(x, mae_values, width=0.4, align='center', label='MAE', color=colors)\n",
    "\n",
    "        def digit_label(rects):\n",
    "            for rect in rects:\n",
    "                h = rect.get_height()\n",
    "                ax.text(rect.get_x() + rect.get_width() / 2., 1.05 * h, f'{int(h)}',\n",
    "                        ha='center', va='bottom', color=rect.get_facecolor())\n",
    "        digit_label(bar)\n",
    "        plt.title('MAE per Model')\n",
    "        ax.set_ylabel('MAE')\n",
    "        ax.set_xlabel('Model')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(self.model_names)\n",
    "        ax.set_ylim([0, max(mae_values) + 5])\n",
    "        plt.tight_layout()\n",
    "        if self.export_dir is not None:\n",
    "            plt.savefig(self.export_dir + '\\\\' + 'compare_mae.png')\n",
    "        plt.show(block=True)\n",
    "\n",
    "    def plot_compare_rmse(self):\n",
    "        no_nan = self.data.copy(deep=True).dropna(how='any')\n",
    "        gt_values = no_nan['day_ahead_prices'].values\n",
    "\n",
    "        rmse_values = []\n",
    "        for model in self.model_names:\n",
    "            pred_values = no_nan[model].values\n",
    "            rmse = self.calc_rmse(gt_values, pred_values)\n",
    "            rmse_values.append(rmse)\n",
    "        x = np.arange(len(self.model_names))\n",
    "\n",
    "        ax = plt.subplot(111)\n",
    "        my_cmap = plt.get_cmap(\"jet\")\n",
    "        colors = my_cmap(np.linspace(0, 1, len(self.model_names)))\n",
    "        bar = ax.bar(x, rmse_values, width=0.4, align='center', label='RMSE', color=colors)\n",
    "\n",
    "        def digit_label(rects):\n",
    "            for rect in rects:\n",
    "                h = rect.get_height()\n",
    "                ax.text(rect.get_x() + rect.get_width() / 2., 1.05 * h, f'{int(h)}',\n",
    "                        ha='center', va='bottom', color=rect.get_facecolor())\n",
    "        digit_label(bar)\n",
    "        plt.title('RMSE per Model')\n",
    "        ax.set_ylabel('RMSE')\n",
    "        ax.set_xlabel('Model')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(self.model_names)\n",
    "        ax.set_ylim([0, max(rmse_values) + 5])\n",
    "        plt.tight_layout()\n",
    "        if self.export_dir is not None:\n",
    "            plt.savefig(self.export_dir + '\\\\' + 'compare_rmse.png')\n",
    "        plt.show(block=True)\n",
    "\n",
    "    def plot_compare_mape(self):\n",
    "        no_nan = self.data.copy(deep=True).dropna(how='any')\n",
    "        gt_values = no_nan['day_ahead_prices'].values\n",
    "\n",
    "        mape_values = []\n",
    "        for model in self.model_names:\n",
    "            pred_values = no_nan[model].values\n",
    "            mape = self.calc_mae(gt_values, pred_values)\n",
    "            mape_values.append(mape / abs(no_nan[model].max() - no_nan[model].min()) * 100)\n",
    "        x = np.arange(len(self.model_names))\n",
    "\n",
    "        ax = plt.subplot(111)\n",
    "        my_cmap = plt.get_cmap(\"jet\")\n",
    "        colors = my_cmap(np.linspace(0, 1, len(self.model_names)))\n",
    "        bar = ax.bar(x, mape_values, width=0.4, align='center', label='MAPE', color=colors)\n",
    "\n",
    "        def digit_label(rects):\n",
    "            for rect in rects:\n",
    "                h = rect.get_height()\n",
    "                ax.text(rect.get_x() + rect.get_width() / 2., 1.05 * h, f'{int(h)}',\n",
    "                        ha='center', va='bottom', color=rect.get_facecolor())\n",
    "        digit_label(bar)\n",
    "        plt.title('MAPE per Model')\n",
    "        ax.set_ylabel('MAPE in %')\n",
    "        ax.set_xlabel('Model')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(self.model_names)\n",
    "        ax.set_ylim([0, 100])\n",
    "        plt.tight_layout()\n",
    "        if self.export_dir is not None:\n",
    "            plt.savefig(self.export_dir + '\\\\' + 'compare_mape.png')\n",
    "        plt.show(block=True)\n",
    "\n",
    "    def plot_single_model(self, model_name: str = ''):\n",
    "        x = self.data.index.values\n",
    "        y1 = self.data[model_name].values\n",
    "        y2 = self.data[model_name + '_AE'].values\n",
    "        y3 = self.data[model_name + '_SE'].values\n",
    "\n",
    "        fig, ax1 = plt.subplots()\n",
    "\n",
    "        # plot prediction and absolute error with  scale\n",
    "        ax1.plot(x, y1, 'b', label='y1 (sin(x)')\n",
    "        ax1.set_xlabel('X-axis')\n",
    "        ax1.set_ylabel('y1', color='b')\n",
    "        ax1.tick_params('y', colors='b')\n",
    "\n",
    "        # plot RMSE with additional scale\n",
    "        ax2 = ax1.twinx()\n",
    "\n",
    "        ax2.plot(x, y2, 'g', label='y2 (exp(-x))')\n",
    "        ax2.set_ylabel('y2', color='g')\n",
    "        ax2.tick_params('y', colors='g')\n",
    "\n",
    "        ax3 = ax1.twinx()\n",
    "\n",
    "        ax3.plot(x, y3, 'r', label='y3 (100*cos(x))')\n",
    "        ax3.spines['right'].set_position(('outward', 60))\n",
    "        ax3.set_ylabel('y3', color='r')\n",
    "        ax3.tick_params('y', colors='r')\n",
    "\n",
    "        lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        lines3, labels3 = ax3.get_legend_handles_labels()\n",
    "        lines = lines1 + lines2 + lines3\n",
    "        labels = labels1 + labels2 + labels3\n",
    "        plt.legend(lines, labels, loc='upper right')\n",
    "\n",
    "        plt.title('Multiple Y-axis Scales')\n",
    "        plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "\n",
    "def chronosForecast(model_dir):\n",
    "    model_type = \"LSTM\"\n",
    "    model_name = \"Chronos\"\n",
    "\n",
    "    model = ChronosModel(model_name=model_name, model_type=model_type)\n",
    "    model.model = model._BaseModel__custom_load(model_dir)\n",
    "\n",
    "    # Load the data\n",
    "    file_path = '../final-submission/merged_data/allData.csv'\n",
    "    columns_to_read = ['Date', 'day_ahead_prices_EURO']\n",
    "\n",
    "    data = pd.read_csv(file_path, usecols=columns_to_read)\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.set_index('Date', inplace=True)\n",
    "    data.index = data.index.tz_localize(None)\n",
    "\n",
    "    # Define forecast date range\n",
    "    start_forecast_date = pd.Timestamp(\"2024-02-14 00:00:00\").tz_localize(None)\n",
    "    end_forecast_date = pd.Timestamp(\"2024-02-22 23:00:00\").tz_localize(None)\n",
    "    forecast_timestamps = pd.date_range(start=start_forecast_date, end=end_forecast_date, freq=\"h\")\n",
    "\n",
    "    # Slice the actual data for the forecast period\n",
    "    actual_data = data.loc[forecast_timestamps].reset_index()\n",
    "    actual_data.rename(columns={'index': 'timestamp', 'day_ahead_prices_EURO': 'actual_value'}, inplace=True)\n",
    "\n",
    "    # Collect forecasts\n",
    "    forecast_results = []\n",
    "\n",
    "    for forecast_date in pd.date_range(start_forecast_date, end_forecast_date, freq=\"D\"):\n",
    "        # Determine the context period (up to 1 hour before the forecast start date)\n",
    "        context_end_date = forecast_date - pd.Timedelta(hours=1)\n",
    "\n",
    "        # Slice the data to get the context window\n",
    "        context_data = data.loc[:context_end_date].iloc[-512:]  # Limit to the last 512 entries\n",
    "\n",
    "        # Run the forecast for the current day (24 hourly values)\n",
    "        forecast_result = model._BaseModel__run_prediction(context_data)\n",
    "\n",
    "        # Add timestamps to forecast results\n",
    "        forecast_result['timestamp'] = pd.date_range(\n",
    "            start=forecast_date,\n",
    "            periods=24,\n",
    "            freq=\"h\"\n",
    "        )\n",
    "        forecast_results.append(forecast_result)\n",
    "\n",
    "    # Combine all forecast results into a single DataFrame\n",
    "    forecast_df = pd.concat(forecast_results, ignore_index=True)\n",
    "\n",
    "    #print(forecast_df)\n",
    "\n",
    "    #print(actual_data)\n",
    "\n",
    "    # Merge forecasts with actual data to compute absolute error\n",
    "    combined_df = forecast_df.merge(actual_data, left_on='timestamp', right_on='timestamp', how='left')\n",
    "    combined_df['absolute_error'] = abs(combined_df['forecasted_values'] - combined_df['actual_value'])\n",
    "\n",
    "    # Drop unnecessary columns and return the result\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from newsapi import NewsApiClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import logging\n",
    "from typing import List, Dict, Union\n",
    "import pytz\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "class NewsEmbeddingPipeline:\n",
    "    def __init__(self, api_key: str, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline with NewsAPI credentials and embedding model.\n",
    "        \"\"\"\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API key cannot be empty\")\n",
    "            \n",
    "        self.newsapi = NewsApiClient(api_key=api_key)\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        logger.info(\"Pipeline initialized successfully\")\n",
    "\n",
    "    def fetch_news(\n",
    "        self,\n",
    "        query: str = \"energy OR electricity OR power market OR renewable\",\n",
    "        from_date: str = None,\n",
    "        to_date: str = None,\n",
    "        language: str = \"en\",\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Fetch news articles from NewsAPI with error handling and logging.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Fetching news with query: {query} from {from_date} to {to_date}\")\n",
    "            \n",
    "            # Ensure dates are properly formatted\n",
    "            if from_date:\n",
    "                datetime.strptime(from_date, '%Y-%m-%d')\n",
    "            if to_date:\n",
    "                datetime.strptime(to_date, '%Y-%m-%d')\n",
    "            \n",
    "            response = self.newsapi.get_everything(\n",
    "                q=query,\n",
    "                from_param=from_date,\n",
    "                to=to_date,\n",
    "                language=language,\n",
    "                sort_by=\"publishedAt\",\n",
    "                page_size=100\n",
    "            )\n",
    "            \n",
    "            articles = response.get(\"articles\", [])\n",
    "            logger.info(f\"Retrieved {len(articles)} articles\")\n",
    "            return articles\n",
    "\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()   \n",
    "            logger.error(f\"Error fetching news: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create embeddings for a list of texts.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Creating embeddings for {len(texts)} texts\")\n",
    "            embeddings = self.model.encode(texts)\n",
    "            # print number of dimensions in the embeddings\n",
    "            logger.info(f\"Embedding dimensions: {embeddings.shape[1]}\")\n",
    "            logger.info(\"Embeddings created successfully\")\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            logger.error(f\"Error creating embeddings: {str(e)}\")\n",
    "            return np.array([])\n",
    "\n",
    "    def process_articles(self, articles: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process articles and create embeddings.\n",
    "        \"\"\"\n",
    "        if not articles:\n",
    "            logger.warning(\"No articles to process\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            # Extract relevant information\n",
    "            processed_articles = []\n",
    "            for article in articles:\n",
    "                # Convert timestamp to UTC\n",
    "                timestamp = pd.to_datetime(article[\"publishedAt\"]).tz_convert('UTC')\n",
    "                # Round to nearest hour\n",
    "                timestamp_hourly = timestamp.round('H')\n",
    "                \n",
    "                processed_articles.append({\n",
    "                    \"timestamp\": timestamp_hourly,\n",
    "                    \"title\": article[\"title\"],\n",
    "                    \"description\": article[\"description\"],\n",
    "                    \"content\": article[\"content\"],\n",
    "                    \"source\": article[\"source\"][\"name\"],\n",
    "                    \"url\": article[\"url\"],\n",
    "                })\n",
    "\n",
    "            df = pd.DataFrame(processed_articles)\n",
    "            logger.info(f\"Processed {len(df)} articles into DataFrame\")\n",
    "\n",
    "            # Combine title and description for embedding\n",
    "            texts = [\n",
    "                f\"{row['title']} {row['description']}\"\n",
    "                if pd.notna(row[\"description\"])\n",
    "                else row[\"title\"]\n",
    "                for _, row in df.iterrows()\n",
    "            ]\n",
    "\n",
    "            # Create embeddings\n",
    "            embeddings = self.create_embeddings(texts)\n",
    "            \n",
    "            if len(embeddings) > 0:\n",
    "                # Store embeddings as numpy arrays for easier manipulation\n",
    "                df[\"embedding\"] = list(embeddings)\n",
    "                logger.info(\"Added embeddings to DataFrame\")\n",
    "            else:\n",
    "                logger.warning(\"No embeddings were created\")\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            logger.error(f\"Error processing articles: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def aggregate_embeddings(self, df: pd.DataFrame, aggregation_method: str = 'mean') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Aggregate embeddings for articles with the same timestamp.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with articles and embeddings\n",
    "            aggregation_method: 'mean' or 'weighted_mean'\n",
    "        \"\"\"\n",
    "        if df.empty:\n",
    "            return df\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Aggregating embeddings by timestamp\")\n",
    "            \n",
    "            # Group by timestamp\n",
    "            grouped = df.groupby('timestamp')\n",
    "            \n",
    "            aggregated_data = []\n",
    "            \n",
    "            for timestamp, group in grouped:\n",
    "                embeddings = np.stack(group['embedding'].values)\n",
    "                \n",
    "                if aggregation_method == 'mean':\n",
    "                    # Simple mean of embeddings\n",
    "                    combined_embedding = np.mean(embeddings, axis=0)\n",
    "                elif aggregation_method == 'weighted_mean':\n",
    "                    # You could implement different weighting schemes here\n",
    "                    # Example: weight by article length\n",
    "                    weights = [len(text) for text in group['content']]\n",
    "                    weights = np.array(weights) / np.sum(weights)\n",
    "                    combined_embedding = np.average(embeddings, axis=0, weights=weights)\n",
    "                \n",
    "                # Collect metadata\n",
    "                sources = list(group['source'].unique())\n",
    "                urls = list(group['url'])\n",
    "                \n",
    "                aggregated_data.append({\n",
    "                    'timestamp': timestamp,\n",
    "                    'embedding': combined_embedding,\n",
    "                    'num_articles': len(group),\n",
    "                    'sources': sources,\n",
    "                    'urls': urls,\n",
    "                })\n",
    "            \n",
    "            aggregated_df = pd.DataFrame(aggregated_data)\n",
    "            logger.info(f\"Aggregated {len(df)} articles into {len(aggregated_df)} timestamps\")\n",
    "            \n",
    "            return aggregated_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            logger.error(f\"Error aggregating embeddings: {str(e)}\")\n",
    "            return df\n",
    "\n",
    "    def collect_and_save_historical_data(\n",
    "        self, \n",
    "        days_back: int = 30, \n",
    "        save_path: str = \"news_embeddings_hourly.csv\",\n",
    "        aggregate: bool = True,\n",
    "        aggregation_method: str = 'mean'\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Collect historical news data day by day and save with continuous hourly timestamps.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Collecting historical data for past {days_back} days\")\n",
    "            \n",
    "            # End date should be yesterday (as today's data isn't available in free tier)\n",
    "            end_date = datetime.now(pytz.UTC).replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=1)\n",
    "            start_date = end_date - timedelta(days=days_back)\n",
    "            \n",
    "            logger.info(f\"Collecting data from {start_date} to {end_date}\")\n",
    "            all_data = []\n",
    "\n",
    "            # Process each day individually\n",
    "            for day_offset in range(days_back + 1):\n",
    "                current_date = start_date + timedelta(days=day_offset)\n",
    "                current_date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "                \n",
    "                logger.info(f\"Processing data for {current_date_str}\")\n",
    "                \n",
    "                # Fetch news for single day\n",
    "                articles = self.fetch_news(\n",
    "                    from_date=current_date_str,\n",
    "                    to_date=current_date_str\n",
    "                )\n",
    "                \n",
    "                if articles:\n",
    "                    # Process articles for this day\n",
    "                    daily_df = self.process_articles(articles)\n",
    "                    \n",
    "                    if not daily_df.empty:\n",
    "                        if aggregate:\n",
    "                            daily_df = self.aggregate_embeddings(daily_df, aggregation_method)\n",
    "                        all_data.append(daily_df)\n",
    "                \n",
    "                # Add a small delay to avoid rate limiting\n",
    "                time.sleep(1)\n",
    "            \n",
    "            # Combine all daily data if we have any\n",
    "            if all_data:\n",
    "                print(\"all_data\", all_data)\n",
    "                # Combine all data\n",
    "                combined_df = pd.concat(all_data, ignore_index=True)\n",
    "                \n",
    "                # Create continuous hourly index\n",
    "                full_hourly_index = pd.date_range(\n",
    "                    start=start_date,\n",
    "                    end=end_date,\n",
    "                    freq='h'  # Using 'h' instead of deprecated 'H'\n",
    "                )\n",
    "                \n",
    "                # Ensure timestamp is datetime index\n",
    "                combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp'])\n",
    "                combined_df.set_index('timestamp', inplace=True)\n",
    "                \n",
    "                # Reindex with full hourly range and forward fill\n",
    "                combined_df = combined_df.reindex(full_hourly_index)\n",
    "                \n",
    "                # Forward fill embeddings and other columns\n",
    "                combined_df['embedding'] = combined_df['embedding'].ffill()\n",
    "                combined_df['num_articles'] = combined_df['num_articles'].ffill().fillna(0).astype(int)\n",
    "                \n",
    "                # Handle list columns safely\n",
    "                combined_df['sources'] = combined_df['sources'].fillna(str([])).apply(lambda x: x if isinstance(x, list) else eval(x))\n",
    "                combined_df['urls'] = combined_df['urls'].fillna(str([])).apply(lambda x: x if isinstance(x, list) else eval(x))\n",
    "                \n",
    "                # Reset index to make timestamp a column again\n",
    "                combined_df.reset_index(names=['timestamp'], inplace=True)\n",
    "                \n",
    "                # Convert embeddings to lists for storage\n",
    "                combined_df['embedding'] = combined_df['embedding'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\n",
    "                \n",
    "                print(\"save_path\", save_path)\n",
    "                save_path = os.path.join(save_path, \"newsapi_embeddings.csv\")\n",
    "                # # Save combined data\n",
    "                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                combined_df.to_csv(save_path, index=False)\n",
    "                logger.info(f\"Saved data with continuous hourly timestamps to {save_path}\")\n",
    "                \n",
    "                # Log some statistics\n",
    "                logger.info(f\"Total hours covered: {len(combined_df)}\")\n",
    "                logger.info(f\"Hours with actual news: {combined_df['num_articles'].astype(bool).sum()}\")\n",
    "                \n",
    "                return combined_df\n",
    "            else:\n",
    "                # Create empty DataFrame with correct structure\n",
    "                full_hourly_index = pd.date_range(\n",
    "                    start=start_date,\n",
    "                    end=end_date,\n",
    "                    freq='h'\n",
    "                )\n",
    "                \n",
    "                empty_df = pd.DataFrame({\n",
    "                    'timestamp': full_hourly_index,\n",
    "                    'embedding': [[] for _ in range(len(full_hourly_index))],\n",
    "                    'num_articles': [0 for _ in range(len(full_hourly_index))],\n",
    "                    'sources': [[] for _ in range(len(full_hourly_index))],\n",
    "                    'urls': [[] for _ in range(len(full_hourly_index))]\n",
    "                })\n",
    "                \n",
    "                print(\"save_path\", save_path)\n",
    "                # Save empty DataFrame\n",
    "                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                empty_df.to_csv(save_path, index=False)\n",
    "                logger.info(f\"Saved empty DataFrame with continuous hourly timestamps to {save_path}\")\n",
    "                \n",
    "                return empty_df\n",
    "\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()   \n",
    "            logger.error(f\"Error collecting historical data: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "# guardian pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import logging\n",
    "import pytz\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class GuardianEmbeddingPipeline:\n",
    "    def __init__(self, api_key: str, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API key cannot be empty\")\n",
    "            \n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://content.guardianapis.com/search\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        logger.info(\"Pipeline initialized successfully\")\n",
    "\n",
    "    def fetch_news(\n",
    "        self,\n",
    "        query: str = \"energy OR electricity OR renewable OR weather\",\n",
    "        from_date: str = None,\n",
    "        to_date: str = None,\n",
    "        page_size: int = 50,\n",
    "        max_pages: int = 20\n",
    "    ) -> List[Dict]:\n",
    "        try:\n",
    "            logger.info(f\"Fetching news with query: {query} from {from_date} to {to_date}\")\n",
    "            \n",
    "            all_articles = []\n",
    "            current_page = 1\n",
    "            \n",
    "            while current_page <= max_pages:\n",
    "                params = {\n",
    "                    'q': query,\n",
    "                    'section': 'business|environment|technology|money',\n",
    "                    'from-date': from_date,\n",
    "                    'to-date': to_date,\n",
    "                    'page-size': page_size,\n",
    "                    'page': current_page,\n",
    "                    'api-key': self.api_key,\n",
    "                    'show-fields': 'all',\n",
    "                    'order-by': 'newest'\n",
    "                }\n",
    "                \n",
    "                response = requests.get(self.base_url, params=params)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                data = response.json()['response']\n",
    "                articles = data.get('results', [])\n",
    "                \n",
    "                if not articles:\n",
    "                    break\n",
    "                    \n",
    "                all_articles.extend(articles)\n",
    "                \n",
    "                if current_page >= data['pages']:\n",
    "                    break\n",
    "                    \n",
    "                current_page += 1\n",
    "                \n",
    "            logger.info(f\"Retrieved {len(all_articles)} articles\")\n",
    "            return all_articles\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Request error: {str(e)}\")\n",
    "            return []\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"JSON decode error: {str(e)}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching news: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def create_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Create embeddings for a list of texts.\"\"\"\n",
    "        if not texts:\n",
    "            logger.warning(\"No texts provided for embedding\")\n",
    "            return np.array([])\n",
    "            \n",
    "        try:\n",
    "            logger.info(f\"Creating embeddings for {len(texts)} texts\")\n",
    "            embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "            logger.info(f\"Embedding dimensions: {embeddings.shape[1]}\")\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating embeddings: {str(e)}\")\n",
    "            return np.array([])\n",
    "\n",
    "    def process_articles(self, articles: List[Dict]) -> pd.DataFrame:\n",
    "        if not articles:\n",
    "            logger.warning(\"No articles to process\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            processed_articles = []\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    timestamp = pd.to_datetime(article['webPublicationDate']).tz_convert('UTC')\n",
    "                    timestamp_hourly = timestamp.round('h')\n",
    "                    \n",
    "                    fields = article.get('fields', {})\n",
    "                    processed_articles.append({\n",
    "                        \"timestamp\": timestamp_hourly,\n",
    "                        \"title\": article.get('webTitle', ''),\n",
    "                        \"description\": fields.get('trailText', ''),\n",
    "                        \"content\": fields.get('bodyText', ''),\n",
    "                        \"source\": \"The Guardian\",\n",
    "                        \"url\": article.get('webUrl', ''),\n",
    "                        \"section\": article.get('sectionName', ''),\n",
    "                    })\n",
    "                except (KeyError, ValueError) as e:\n",
    "                    logger.warning(f\"Skipping malformed article: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            if not processed_articles:\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            df = pd.DataFrame(processed_articles)\n",
    "            \n",
    "            df['text_for_embedding'] = df.apply(\n",
    "                lambda row: f\"{row['title']} {row['description']}\" if pd.notna(row[\"description\"]) else row[\"title\"],\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            embeddings = self.create_embeddings(df['text_for_embedding'].tolist())\n",
    "            \n",
    "            if len(embeddings) > 0:\n",
    "                df[\"embedding\"] = list(embeddings)\n",
    "                df.drop('text_for_embedding', axis=1, inplace=True)\n",
    "                logger.info(\"Added embeddings to DataFrame\")\n",
    "            else:\n",
    "                logger.warning(\"No embeddings were created\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing articles: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def aggregate_embeddings(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if df.empty:\n",
    "            return df\n",
    "\n",
    "        try:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.sort_values('timestamp')\n",
    "            \n",
    "            aggregated = df.groupby('timestamp').agg({\n",
    "                'embedding': lambda x: np.mean([emb for emb in x if isinstance(emb, (np.ndarray, list))], axis=0),\n",
    "                'source': lambda x: list(set(x)),\n",
    "                'url': list,\n",
    "                'section': lambda x: list(set(x)),\n",
    "                'title': 'count'\n",
    "            }).reset_index()\n",
    "            \n",
    "            aggregated.rename(columns={'title': 'num_articles'}, inplace=True)\n",
    "            \n",
    "            logger.info(f\"Aggregated {len(df)} articles into {len(aggregated)} timestamps\")\n",
    "            return aggregated\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error aggregating embeddings: {str(e)}\")\n",
    "            return df\n",
    "\n",
    "    def collect_and_save_historical_data(\n",
    "        self, \n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        save_path: str = \"data/guardian_embeddings\",\n",
    "        query: str = \"energy OR electricity OR renewable OR weather\"\n",
    "    ) -> pd.DataFrame:\n",
    "        try:\n",
    "            start_dt = pd.to_datetime(start_date).tz_localize(pytz.UTC).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "            end_dt = pd.to_datetime(end_date).tz_localize(pytz.UTC).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "            \n",
    "            if start_dt > end_dt:\n",
    "                raise ValueError(\"Start date must be before end date\")\n",
    "                \n",
    "            logger.info(f\"Collecting data from {start_dt} to {end_dt}\")\n",
    "            \n",
    "            from_date = start_dt.strftime(\"%Y-%m-%d\")\n",
    "            to_date = end_dt.strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "            articles = self.fetch_news(from_date=from_date, to_date=to_date, query=query)\n",
    "            if articles:\n",
    "                df = self.process_articles(articles)\n",
    "                if not df.empty:\n",
    "                    aggregated_df = self.aggregate_embeddings(df)\n",
    "                else:\n",
    "                    logger.warning(\"No articles processed\")\n",
    "                    return pd.DataFrame()\n",
    "            else:\n",
    "                logger.warning(\"No articles fetched\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            full_hourly_index = pd.date_range(\n",
    "                start=start_dt,\n",
    "                end=end_dt,\n",
    "                freq='h',\n",
    "                tz='UTC'\n",
    "            )\n",
    "            \n",
    "            template_df = pd.DataFrame({\n",
    "                'timestamp': full_hourly_index,\n",
    "                'embedding': [None for _ in range(len(full_hourly_index))],\n",
    "                'source': [None for _ in range(len(full_hourly_index))],\n",
    "                'url': [None for _ in range(len(full_hourly_index))],\n",
    "                'section': [None for _ in range(len(full_hourly_index))],\n",
    "                'num_articles': [None for _ in range(len(full_hourly_index))]\n",
    "            })\n",
    "            \n",
    "            final_df = pd.merge(\n",
    "                template_df,\n",
    "                aggregated_df,\n",
    "                on='timestamp',\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            final_df['embedding'] = final_df['embedding_y'].fillna(final_df['embedding_x'])\n",
    "            final_df['source'] = final_df['source_y'].fillna(final_df['source_x'])\n",
    "            final_df['url'] = final_df['url_y'].fillna(final_df['url_x'])\n",
    "            final_df['section'] = final_df['section_y'].fillna(final_df['section_x'])\n",
    "            final_df['num_articles'] = final_df['num_articles_y'].fillna(final_df['num_articles_x'])\n",
    "            \n",
    "            final_df['embedding'] = final_df['embedding'].ffill()\n",
    "            final_df['source'] = final_df['source'].ffill().apply(lambda x: x if isinstance(x, list) else [])\n",
    "            final_df['url'] = final_df['url'].ffill().apply(lambda x: x if isinstance(x, list) else [])\n",
    "            final_df['section'] = final_df['section'].ffill().apply(lambda x: x if isinstance(x, list) else [])\n",
    "            final_df['num_articles'] = final_df['num_articles'].fillna(0).astype(int)\n",
    "            \n",
    "            final_df = final_df[['timestamp', 'embedding', 'source', 'url', 'section', 'num_articles']]\n",
    "            \n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            output_file = os.path.join(save_path, f\"guardian_embeddings.csv\")\n",
    "            \n",
    "            final_df['embedding'] = final_df['embedding'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\n",
    "            final_df.to_csv(output_file, index=False)\n",
    "            \n",
    "            logger.info(f\"Total hours: {len(final_df)}\")\n",
    "            logger.info(f\"Hours with articles: {(final_df['num_articles'] > 0).sum()}\")\n",
    "            \n",
    "            return final_df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error collecting historical data: {str(e)}\")\n",
    "            return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def expand_embeddings(df):\n",
    "    \"\"\"\n",
    "    Convert string embeddings to separate numeric columns\n",
    "    \"\"\"\n",
    "    if 'embedding' not in df.columns:\n",
    "        return df\n",
    "        \n",
    "    print(\"Converting string embeddings to numeric arrays...\")\n",
    "    # Convert string representation of list to actual list of floats\n",
    "    embeddings = df['embedding'].apply(eval)  # converts string representation to list\n",
    "    # Convert to numpy array for easier handling\n",
    "    embedding_array = np.vstack(embeddings)\n",
    "    # Create separate columns for each embedding dimension\n",
    "    embedding_cols = [f'embedding_{i}' for i in range(embedding_array.shape[1])]\n",
    "    embedding_df = pd.DataFrame(embedding_array, columns=embedding_cols, index=df.index)\n",
    "    \n",
    "    # Drop original embedding column and add expanded columns\n",
    "    df = df.drop(columns=['embedding'])\n",
    "    df = pd.concat([df, embedding_df], axis=1)\n",
    "    \n",
    "    print(f\"Expanded embeddings into {len(embedding_cols)} dimensions\")\n",
    "    return df\n",
    "\n",
    "def reduce_embedding_dimensionality(df, n_components=50):\n",
    "    \"\"\"\n",
    "    Reduce dimensionality of embeddings using PCA\n",
    "    \"\"\"\n",
    "    # Extract embedding columns\n",
    "    embedding_cols = [col for col in df.columns if col.startswith('embedding_')]\n",
    "    if not embedding_cols:\n",
    "        return df, None\n",
    "    \n",
    "    # Standardize the embeddings\n",
    "    scaler = StandardScaler()\n",
    "    embeddings_scaled = scaler.fit_transform(df[embedding_cols])\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    embeddings_reduced = pca.fit_transform(embeddings_scaled)\n",
    "    \n",
    "    # Create new dataframe with reduced embeddings\n",
    "    reduced_cols = [f'pca_embedding_{i}' for i in range(n_components)]\n",
    "    embeddings_df = pd.DataFrame(embeddings_reduced, columns=reduced_cols, index=df.index)\n",
    "    \n",
    "    # Drop original embeddings and add reduced ones\n",
    "    df_reduced = df.drop(columns=embedding_cols)\n",
    "    df_reduced = pd.concat([df_reduced, embeddings_df], axis=1)\n",
    "    \n",
    "    # Print variance explained\n",
    "    variance_explained = np.sum(pca.explained_variance_ratio_)\n",
    "    print(f\"Variance explained by {n_components} components: {variance_explained:.3f}\")\n",
    "    \n",
    "    return df_reduced, pca\n",
    "\n",
    "def prepare_data_simple(df, use_pca=False, n_pca_components=50):\n",
    "    \"\"\"\n",
    "    Simple data preparation focusing only on price and embeddings.\n",
    "    Option to use PCA or keep full dimensionality.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Drop metadata columns if they exist\n",
    "    metadata_cols = ['source', 'url', 'section', 'num_articles']\n",
    "    df = df.drop(columns=[col for col in metadata_cols if col in df.columns])\n",
    "    \n",
    "    # Create target variable (next day's price)\n",
    "    df['target'] = df['day_ahead_prices_EURO']\n",
    "    \n",
    "    # Expand embeddings from string to numeric columns\n",
    "    \n",
    "    # Optionally reduce dimensionality\n",
    "    pca = None\n",
    "    if use_pca:\n",
    "        df = expand_embeddings(df)\n",
    "        df, pca = reduce_embedding_dimensionality(df, n_components=n_pca_components)\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Define features\n",
    "    base_features = []\n",
    "    if use_pca:\n",
    "        embedding_features = [col for col in df.columns if col.startswith('pca_embedding_')]\n",
    "    else:\n",
    "        embedding_features = [col for col in df.columns if col.startswith('embedding_')]\n",
    "    features = base_features + embedding_features\n",
    "    \n",
    "    print(\"\\nFeature Summary:\")\n",
    "    print(f\"Total number of features: {len(features)}\")\n",
    "    print(f\"- Base price features: {len(base_features)}\")\n",
    "    print(f\"- Embedding features: {len(embedding_features)}\")\n",
    "    \n",
    "    df['item_id'] = 'price_series'\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']).dt.tz_localize(None)\n",
    "    \n",
    "    return df, features, pca\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prepare_embedding_data(guardian_embeddings, newsapi_embeddings, day_ahead_prices):\n",
    "    guardian_embeddings_day_ahead = pd.merge(guardian_embeddings, day_ahead_prices, on='timestamp', how='inner')\n",
    "    newsapi_embeddings_day_ahead = pd.merge(newsapi_embeddings, day_ahead_prices, on='timestamp', how='inner')\n",
    "\n",
    "    # cut day ahead prices to the same time frame as the embeddings\n",
    "    guardian_embeddings_start = guardian_embeddings_day_ahead['timestamp'].min()\n",
    "    guardian_embeddings_end = guardian_embeddings_day_ahead['timestamp'].max()\n",
    "    \n",
    "    \n",
    "    newsapi_embeddings_day_ahead = newsapi_embeddings_day_ahead[(newsapi_embeddings_day_ahead['timestamp'] >= guardian_embeddings_start) & (newsapi_embeddings_day_ahead['timestamp'] <= guardian_embeddings_end)]\n",
    "    \n",
    "    news_embeddings_start = newsapi_embeddings_day_ahead['timestamp'].min()\n",
    "    news_embeddings_end = newsapi_embeddings_day_ahead['timestamp'].max()\n",
    "\n",
    "    guardian_embeddings_day_ahead = guardian_embeddings_day_ahead[(guardian_embeddings_day_ahead['timestamp'] >= news_embeddings_start) & (guardian_embeddings_day_ahead['timestamp'] <= news_embeddings_end)]\n",
    "    \n",
    "    day_ahead_prices = day_ahead_prices[(day_ahead_prices['timestamp'] >= news_embeddings_start) & (day_ahead_prices['timestamp'] <= news_embeddings_end)]\n",
    "\n",
    "    # remove the last 24 rows\n",
    "    guardian_embeddings_day_ahead = guardian_embeddings_day_ahead[:-24]\n",
    "    newsapi_embeddings_day_ahead = newsapi_embeddings_day_ahead[:-24]\n",
    "    day_ahead_prices_for_prediction = day_ahead_prices.copy()[:-24]\n",
    "    ground_truth = day_ahead_prices.copy()[-24:]\n",
    "    \n",
    "    return guardian_embeddings_day_ahead, newsapi_embeddings_day_ahead, day_ahead_prices_for_prediction, ground_truth\n",
    "\n",
    "def predict_embedding_data(predictor_path, df):\n",
    "    predictor = TimeSeriesPredictor.load(predictor_path)\n",
    "    \n",
    "    df_embeddings, _, _ = prepare_data_simple(df)\n",
    "    \n",
    "    predict_data = TimeSeriesDataFrame(df_embeddings)\n",
    "    \n",
    "    predictions = predictor.predict(predict_data)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def visualize_embedding_model_results(predictions, ground_truth, label: str):\n",
    "    \n",
    "    predictions = predictions.reset_index()\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(predictions['mean'], ground_truth['day_ahead_prices_EURO']))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(ground_truth['timestamp'], ground_truth['day_ahead_prices_EURO'], label='Actual Price')\n",
    "    plt.plot(predictions['timestamp'], predictions['mean'], label='Predicted Price')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title(f'Actual vs. Predicted Prices {label} (RMSE: {rmse:.2f})')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Domain Knowledge \n",
    "### The German Energy market\n",
    "#### What markets exist within Germany? \n",
    "As explained by Schumacher et al (2015), in Germany, the price of electricity is primarily determined by the instantaneous relationship between energy consumption and energy production. Energy that has been produced is rarely stored in a quantity that is significant in relation to the prevailing market price. The market value may therefore fluctuate on a moment-by-moment basis in response to outages, consumption spikes, or production surges.\n",
    "This naturally introduces an element of unpredictability, which may not be conducive to the stability and predictability that businesses and customers require.\n",
    "Consequently, submarkets have emerged that offer longer-term contracts, enabling customers to procure electricity in advance for a period of up to six years, typically at a premium for these assurances. Submarkets offering electricity for shorter periods may also be established, with auctions taking place for the following day or even intra-day trading. As is the case with the majority of markets, the ability to effectively forecast various factors and, subsequently, prices is therefore of significant importance.\n",
    "It is important to note that only approximately 20% of electricity volume trading occurs on exchange markets. Nevertheless, even contracts that are negotiated directly between consumers and producers of electricity directly, frequently establish their prices in accordance with the prices set by the exchanges.\n",
    "\n",
    "#### Day-ahead prices\n",
    "As the main forecast target of the present work, the following subsection outlines the creation of the day-ahead price, as described in Nestle et al. (2009) and Schumacher et al. (2015).\n",
    "By midday, market participants submit their bids and offers, which include the quantity and delivery time for the following day. Based on the aforementioned bids and offers, a wholesale price for each hour of the forthcoming day is calculated.\n",
    "Ultimately, the price of electricity is determined by ranking the offers in descending order of price, with lower-priced generation given precedence until the demand is satisfied.\n",
    "The highest production price that is still accepted becomes the agreed-upon price, resulting in varying margins between producers. Evidently, lower-priced generation tends to be that, with low marginal costs.\n",
    "While renewable energy sources may require some installation costs, they do not have to include fuel prices in their calculations. Therefore, the quantity of such lower priced energy heavily influences market prices.\n",
    "\n",
    "#### Importance of prediction\n",
    "While these dynamic factors are important for providers and customers for economic planning as previously described, predicition and planning are also curucial for the grids integrity.\n",
    "Producers and consumers are grouped in balancing groups (Bilanzkreis) as described by Dumancic (2024), with transmission system operators balancing supply and demand using positive and negative balancing. Unplanned imbalance in the form of over or underproduction of electricity in such a balancing group is financially penalized.\n",
    "\n",
    "https://www.smard.de/page/en/wiki-article/5884/5840\n",
    "https://www.smard.de/page/en/wiki-article/5884/5840\n",
    "https://www.smard.de/page/en/wiki-article/6076/5976\n",
    "https://www.smard.de/page/en/wiki-article/6078/5852\n",
    "\n",
    "#### What influences the electricity price?\n",
    "As seen in Niedermayer (2023) and Hein et al. (2020), the installed net rated capacity of electricity in Germany is steadily increasing. According to Bosch et al. (2023) and Hein et al. (2020), main energy sources include fossil gas, lignite, coal, wind and photovoltaics. From the production side, main drivers of energy prices are therefore factors pertaining to these elements, such as fuel prices or weather conditions.\n",
    "\n",
    "#### Looking beyond the German market\n",
    "In a manner analogous to the operation of individual balancing groups, which are responsible for balancing the production and consumption of electricity to ensure the stability of the grid, electricity can be traded and balanced between countries. A substantial number of European countries are participants in the SMARD exchange system, as discussed by Ortner and Totschnig (2019). Consequently, the cost of electricity in Germany is influenced by the cross-border flow of electricity with neighboring countries and their respective energy consumption and production. \n",
    "\n",
    "https://www.smard.de/page/en/topic-article/212254/215382"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weatherdata\n",
    "The weather is a pivotal factor in the calculation of the day-ahead price, given its direct impact on supply and demand in the energy market. Meteorological conditions, such as temperatures, wind speeds and solar radiation, influence both energy consumption, e.g. for heating or cooling, and energy generation from renewable sources like wind and solar energy.\n",
    "\n",
    "To record the meteorological conditions in Germany, three representative weather stations were selected, covering the northern, central and southern regions. This geographical division facilitates the consideration of distinct climatic conditions in the respective regions, in conjunction with other data sources. \n",
    "\n",
    "The meteorological data for these stations is obtained from the OpenWeather platform of the German Weather Service (DWD). This involves the integration of historical and current weather data, with the values from 2015 onwards being merged into a dataframe. This extensive data set is then enriched with the current forecasts for the following days, thus enabling more accurate mapping of the day-ahead price.\n",
    "While the current weather implementation provides a solid foundation, various measures could be implemented to further optimise the process: \n",
    "\n",
    "- The installation of a greater number of weather stations would facilitate a more detailed and regionally differentiated depiction of weather conditions.\n",
    "\n",
    "- In addition, calculating an average at the country level has the potential to enhance the representativeness of the basis. This average could then be weighted with the population density and the regionally available capacity of the individual power plant types in order to better model local energy generation and demand.\n",
    "\n",
    "- Furthermore, the incorporation of extreme weather events, such as heat waves, cold snaps, and storms, is imperative for the analysis and forecasting of energy generation and demand. The incorporation of data on extreme weather events, such as heatwaves, cold snaps, or storms, could facilitate more precise prediction of unexpected fluctuations in energy production and demand.\n",
    "The analysis of long-term weather patterns, including seasonal fluctuations and the effects of climate change, has the potential to significantly enhance the accuracy of forecasting. \n",
    "\n",
    "By expanding the analysed data set, weather implementation could be made more precise and flexible, allowing it to respond better to the requirements of a dynamic energy market.\n",
    "\n",
    "### Entsoe Energy Market Data\n",
    "The ENTSO-E (European Network of Transmission System Operators for Electricity) Transparency Platform, launched in 2015, serves as Europe's central hub for electricity market data. Created to fulfill EU Regulation 543/2013, it collects and publishes data from over 42 transmission system operators across Europe. The platform transformed what was once a fragmented landscape of national data sources into a unified repository, providing crucial information about power generation, consumption, and cross-border flows. For market participants and analysts, this data source is invaluable as it offers insights into the fundamental drivers of electricity prices, including generation mix, grid constraints, and demand patterns.\n",
    "The German market underwent a significant structural change on October 1, 2018, when the joint German-Austrian-Luxembourg bidding zone (DE_AT_LU) was split. This necessitated implementing a data merging strategy to maintain continuous historical records, combining data from both the old (DE_AT_LU) and new (DE_LU) market configurations.\n",
    "\n",
    "### SMARD Electricity Market Data\n",
    "SMARD is an information platform for the German electricity market run by the federal power network agency \"Bundesnetzagentur\".\n",
    "On the SMARD website there are datasets containing electricity generation, balancing, consumption and commercial exchanges available for download, and real time data is updated with high frequency.\n",
    "Furthermore, background information about these datasets and the electricity is given, explaining the types and context of available data. This is especially important for datasets which went through different circumstances or determination methods within our selected period of time, as these changes might make some additional preprocessing necessary.\n",
    "SMARD presents a reliable data source due to it being a government backed service, and the wide variety of datasets downloadable freely.\n",
    "\n",
    "\n",
    "### Stock Market Resource Data\n",
    "Since our energy consumption cannot yet be fully satisfied with renewable energy sources, there is still a need for fossil-based energy production and nuclear power. To reflect this in our dataset, resource prices on the stock market need to be considered. Trading in the stock market is not limited to companies but also includes resources. Resources like oil, natural gas, coal, and uranium are important for energy production and were therefore chosen for examination. Examples include uranium for nuclear power plants and coal for coal plants. These resources do not have static pricing and are listed and traded on the stock market. Their prices fluctuate, and the daily closing price is used for the calculations. It could be speculated that hourly values might yield better results, but sources for these values could not be found and were therefore not considered. The stock market data also has the highest rate of missing values, as there are no values for weekends and bank holidays when the stock markets are closed.\n",
    "\n",
    "Initially, the idea was to include the prices of company stocks with high energy usage, but this was discarded as stock prices are not directly tied to energy prices. Additionally, selecting which companies to include would have been challenging, as it would require deciding whether to focus on global, European, or only German companies.\n",
    "\n",
    "### GDP and Inflation\n",
    "An example of an idea that was considered but did not end up in the final product is the inclusion of GDP (Gross Domestic Product) and inflation data in the calculations. The challenge identified with these datasets is that most sources for inflation and GDP only publish results annually, meaning only a single value per year would be available in our dataset. The impact of such a value would therefore be negligible. Additionally, a selection of countries would need to be made, and as with high energy usage companies, it would be unclear whether to use data from global, European, or a smaller set of countries.\n",
    "\n",
    "### Carbon Emission Futures\n",
    "\n",
    "Carbon emission futures data represents the market price of permits that companies must purchase to cover their carbon dioxide emissions under cap-and-trade systems such as the European Union Emissions Trading System (EU ETS). These permits, also known as emission allowances, are traded in financial markets, where contracts are bought and sold for delivery at a specific future date and with prices influenced by factors such as regulatory policies, market demand, and emission reduction targets. \n",
    "The maturity dates are set at quarterly intervals to provide market participants with the opportunity to manage their compliance obligations and trade allowances across various timeframes.\n",
    "\n",
    "The settlement date for a contract signifies the last day trading can occur, after which the allowances are delivered or settled financially, depending on the contract terms.\n",
    "Each allowance grants the holder the right to emit one metric ton of carbon dioxide (CO) or its equivalent in other greenhouse gases. The data provides a quantitative measure of the cost of carbon emissions, which is a significant factor for energy producers using fossil fuels.\n",
    "\n",
    "This data was incorporated into the analysis because carbon costs significantly influence the operational expenses of energy generation, especially for carbon-intensive sources such as coal and natural gas. Incorporating carbon emission futures as a feature allows the model to reflect the economic and regulatory pressures that shape energy production costs, capturing a critical aspect of market dynamics.\n",
    "The data was sourced from Investing.com and specifically utilizes the Carbon Emissions Futures Historical dataset. It consists of daily records spanning from 2015 to the present, offering a consistent and detailed historical dataset for analysis. With the use of this dataset the aim is to ensure that the model reflects the ongoing trends and fluctuations in carbon pricing, which are critical for accurately capturing the dynamics of energy markets.\n",
    "\n",
    "### Major social events\n",
    "\n",
    "Including major social events in the analysis and prediction of day-ahead energy prices in Germany is valuable because such events often lead to significant shifts in both energy demand and supply. Social events, such as public holidays, large-scale festivals, political events, strikes, or even sporting events, can influence the daily behavior of energy consumers, altering typical consumption patterns. These events often result in changes in energy demand due to variations in business activity, transportation, public services, and individual behaviors.\n",
    "\n",
    "By incorporating major social events as a feature, the model is trained to be able to account for these irregular, yet predictable, fluctuations in demand. The goal is to improve the accuracy of day-ahead price forecasting by capturing the non-seasonal, event-driven dynamics that can significantly impact energy markets.\n",
    "\n",
    "In this analysis, major social events such as Carnival, Oktoberfest, Berlinale, and the Super Bowl were included to enhance the models ability to account for shifts in energy demand driven by these high-visibility social activities in Germany. These events, while not occurring daily, can lead to significant variations in energy consumption, particularly in terms of electricity usage in public spaces, transportation, and entertainment venues. For example, during Carnival or Oktoberfest, there is often an uptick in energy demand in cities like Cologne or Munich, due to increased activity in hospitality and public services. Similarly, the Berlinale film festival or the Super Bowl can lead to higher energy consumption driven by larger gatherings and public viewing events.\n",
    "\n",
    "The specific dates of these events from 2015 to 2025 were manually derived based on historical records and scheduled occurrences. Each event was represented by a feature in the model with a binary value (1 or 0), indicating whether the event occurred on a given date.\n",
    "\n",
    "### Covid Pandemic Data\n",
    "\n",
    "The COVID-19 Pandemic was a very impactful and disruptive event in recent history.\n",
    "The pandemic and especially governmental containment and prevention measures had a high influence on human behaviour and the economy, therefore exerting a strong influence on energy demands.\n",
    "For instance the resulting shutdown of the economy implied a halt to production facilities and gastronomy, and the recommendation to not leave your home meant the private homes of people, who would otherwise be out for most of the day, had to be temperature regulated during day hours, whereas they might not have used their home AC or heating while at work.\n",
    "\n",
    "While certain social and economic repercussions of measures in the past are still evident today, the direct influence of political actions on energy demands is assumed to be limited to the time in which respective actions were active.\n",
    "These actions however differed in time and extent in the federal states, and a suitable dataset needs to represent that.\n",
    "Therefore, a dataset containing different types of political actions in the federal states and their corresponding time frames is needed.\n",
    "Such a dataset can be found as part of a publication containing daily distinctions of 16 types of political measures, each rated on a severity scale of 0-2 (\"free\" to \"fully restricted\"), and listed for each of the 16 federal states of Germany. [Steinmetz et al., 2022]\n",
    "\n",
    "This dataset required preprocessing to be adjusted for our use case. Political measures were therefore each valued with factors of 0, 1 or 2 to eliminate measures with no or unlikely relation to energy demand, for example the recommendation to keep a distance of 1.5m to other persons, or weigh them accordingly if a strong influence can be assumed, for example closure of kindergartens and daycare possibly forcing workers to stay at home.\n",
    "Furthermore, the weighted political actions were multiplied by the population percentage residing the according federal state, to adjust for the amount of people affected.\n",
    "On days with no active political measures affecting energy demands, or on days that were not part of the dataset, the value 0 was filled in.\n",
    "The resulting daily index number was then used to create an hourly dataset by filling in all 24 hours with the daily value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entsoe import EntsoeRawClient, EntsoePandasClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv \n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "\n",
    "## Entsoe\n",
    "\n",
    "ENTSOE_API_KEY = \"562a20c4-03b0-4ee6-a692-19d534b4393a\"\n",
    "client = EntsoePandasClient(api_key=ENTSOE_API_KEY)\n",
    "\n",
    "start = pd.Timestamp('20150101', tz='UTC')\n",
    "change_date = pd.Timestamp('20181001', tz='UTC')\n",
    "end = pd.Timestamp(datetime.datetime.now(), tz='UTC')\n",
    "\n",
    "print(os.getcwd())\n",
    "out_dir = 'merged_data/data_collection'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "country_code_old = 'DE_AT_LU'\n",
    "country_code_new = 'DE_LU'\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10), retry=retry_if_exception_type(Exception))\n",
    "def query_entsoe_data(query_func, country_code, start, end):\n",
    "    try:\n",
    "        df = query_func(country_code, start=start, end=end)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying data: {e}\")\n",
    "        raise\n",
    "    return df\n",
    "\n",
    "def merge_data(query_func):\n",
    "    data_old = query_entsoe_data(query_func, country_code_old, start, change_date)\n",
    "    \n",
    "    data_new = query_entsoe_data(query_func, country_code_new, change_date, end)\n",
    "\n",
    "    if not isinstance(data_old, pd.DataFrame):\n",
    "        data_old = data_old.to_frame()\n",
    "    if not isinstance(data_new, pd.DataFrame):\n",
    "        data_new = data_new.to_frame()\n",
    "    \n",
    "    if not data_old.empty and not data_new.empty:\n",
    "        if len(data_old.columns) != len(data_new.columns):\n",
    "            same_columns = list(set(data_old.columns) & set(data_new.columns))\n",
    "            data_old = data_old[same_columns]\n",
    "            data_new = data_new[same_columns]\n",
    "        else:\n",
    "            data_new.columns = data_old.columns\n",
    "    df_combined = pd.concat([data_old, data_new])\n",
    "    df_combined.index = df_combined.index.tz_convert('UTC')\n",
    "    return df_combined\n",
    "\n",
    "def save_df_with_timestamp(df, filename):\n",
    "    df_copy = df.copy()\n",
    "    df_copy.index.name = 'timestamp'\n",
    "    df_copy.to_csv(filename)\n",
    "\n",
    "# Day-ahead prices (EUR/MWh)\n",
    "day_ahead_prices = merge_data(client.query_day_ahead_prices)\n",
    "day_ahead_prices = day_ahead_prices.rename(columns={day_ahead_prices.columns[0]: 'day_ahead_prices_EURO'})\n",
    "save_df_with_timestamp(day_ahead_prices, f'{out_dir}/day_ahead_prices.csv')\n",
    "print('Day-ahead prices done')\n",
    "\n",
    "# Load forecast (MWh)\n",
    "load_forecast = merge_data(client.query_load_forecast)\n",
    "load_forecast = load_forecast.rename(columns={load_forecast.columns[0]: 'E_load_forecast_MWh'})\n",
    "save_df_with_timestamp(load_forecast, f'{out_dir}/load_forecast.csv')\n",
    "print('Load forecast done')\n",
    "\n",
    "# Generation forecast (MWh)\n",
    "generation_forecast = merge_data(client.query_generation_forecast)\n",
    "generation_forecast = generation_forecast.rename(columns={generation_forecast.columns[0]: 'E_generation_forecast_MWh'})\n",
    "save_df_with_timestamp(generation_forecast, f'{out_dir}/generation_forecast.csv')\n",
    "print('Generation forecast done')\n",
    "\n",
    "# Wind and solar forecasts (MWh)\n",
    "intraday_wind_solar_forecast = merge_data(client.query_intraday_wind_and_solar_forecast)\n",
    "for col in intraday_wind_solar_forecast.columns:\n",
    "    if 'Wind' in col:\n",
    "        intraday_wind_solar_forecast = intraday_wind_solar_forecast.rename(columns={col: 'E_wind_forecast_MWh'})\n",
    "    elif 'Solar' in col:\n",
    "        intraday_wind_solar_forecast = intraday_wind_solar_forecast.rename(columns={col: 'E_solar_forecast_MWh'})\n",
    "save_df_with_timestamp(intraday_wind_solar_forecast, f'{out_dir}/intraday_wind_solar_forecast.csv')\n",
    "print('Intraday wind and solar forecast done')\n",
    "\n",
    "# Day ahead wind and solar forecast (MWh)\n",
    "day_ahead_wind_solar_forecast = merge_data(client.query_wind_and_solar_forecast)\n",
    "for col in day_ahead_wind_solar_forecast.columns:\n",
    "    if 'Wind' in col:\n",
    "        day_ahead_wind_solar_forecast = day_ahead_wind_solar_forecast.rename(columns={col: 'E_wind_forecast_MWh'})\n",
    "    elif 'Solar' in col:\n",
    "        day_ahead_wind_solar_forecast = day_ahead_wind_solar_forecast.rename(columns={col: 'E_solar_forecast_MWh'})\n",
    "save_df_with_timestamp(day_ahead_wind_solar_forecast, f'{out_dir}/day_ahead_wind_solar_forecast.csv')\n",
    "print('Day ahead wind and solar forecast done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##News Embeddings\n",
    "NEWS_API_KEY=\"9b546c7456c147f5b45e9cfb00b0b445\"\n",
    "GUARDIAN_API_KEY=\"39250d58-e880-4584-b0b5-c7b2f1fe0317\"\n",
    "\n",
    "\n",
    "# First get the datetime object\n",
    "end_date_dt = datetime.now()\n",
    "# Calculate start date using datetime object\n",
    "start_date_dt = end_date_dt - timedelta(days=30)\n",
    "\n",
    "# Convert to strings only when needed\n",
    "end_date = end_date_dt.strftime(\"%Y-%m-%d\")\n",
    "start_date = start_date_dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "save_path = '../final-submission/merged_data/data_collection'\n",
    "\n",
    "guardian_pipeline = GuardianEmbeddingPipeline(GUARDIAN_API_KEY)\n",
    "guardian_df = guardian_pipeline.collect_and_save_historical_data(\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    save_path=save_path,\n",
    "    query=\"energy OR electricity OR renewable OR weather\"\n",
    ")\n",
    "\n",
    "news_pipeline = NewsEmbeddingPipeline(NEWS_API_KEY)\n",
    "news_df = news_pipeline.collect_and_save_historical_data(\n",
    "    days_back=30,\n",
    "    save_path=save_path,\n",
    "    aggregate=True,\n",
    "    aggregation_method='mean'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading zip file...\n",
      "Download successful. Unzipping...\n",
      "Successfully extracted to models/models/guardian_embeddings\n",
      "Downloading zip file...\n",
      "Failed to download file. Please check the URL.\n",
      "Downloading zip file...\n",
      "Failed to download file. Please check the URL.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "\n",
    "def download_and_unzip_github_file(local_path, url):\n",
    "    # GitHub raw content URL for the zip file\n",
    "    # Convert the normal GitHub URL to raw content URL\n",
    "    raw_url = url\n",
    "    \n",
    "    print(\"Downloading zip file...\")\n",
    "    response = requests.get(raw_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Create local directory if it doesn't exist\n",
    "        local_path = local_path\n",
    "        os.makedirs(local_path, exist_ok=True)\n",
    "        \n",
    "        print(\"Download successful. Unzipping...\")\n",
    "        # Create a BytesIO object from the downloaded content\n",
    "        zip_file = BytesIO(response.content)\n",
    "        \n",
    "        # Extract the contents\n",
    "        with ZipFile(zip_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(local_path)\n",
    "            \n",
    "        print(f\"Successfully extracted to {local_path}\")\n",
    "    else:\n",
    "        print(\"Failed to download file. Please check the URL.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_and_unzip_github_file(\"models/models/guardian_embeddings\", \"https://raw.githubusercontent.com/BTW25-Data-Science-Challenge/final-submission/refs/heads/main/models/models/guardian_embedding_model.zip\")\n",
    "    download_and_unzip_github_file(\"models/models/newsapi_embeddings\", \"https://raw.githubusercontent.com/BTW25-Data-Science-Challenge/final-submission/refs/heads/main/models/models/newsapi_embedding_model.zip\")\n",
    "    download_and_unzip_github_file(\"models/models/no_embeddings\", \"https://raw.githubusercontent.com/BTW25-Data-Science-Challenge/final-submission/refs/heads/main/models/models/no_embedding_model.zip)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "from base64 import b64decode\n",
    "\n",
    "def download_github_directory(api_url, local_path, token=None):\n",
    "    \"\"\"\n",
    "    Download files from a GitHub repository directory\n",
    "    \n",
    "    Args:\n",
    "        api_url (str): GitHub API URL for the repository contents\n",
    "        local_path (str): Local path to save the files\n",
    "        token (str, optional): GitHub personal access token\n",
    "    \"\"\"\n",
    "    # Create headers with token if provided\n",
    "    headers = {}\n",
    "    if token:\n",
    "        headers['Authorization'] = f'token {token}'\n",
    "    \n",
    "    # Make API request\n",
    "    response = requests.get(api_url, headers=headers)\n",
    "    print(f\"Status code: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(local_path, exist_ok=True)\n",
    "        \n",
    "        files = response.json()\n",
    "        if not isinstance(files, list):\n",
    "            files = [files]\n",
    "            \n",
    "        print(f\"\\nFound {len(files)} files/directories\")\n",
    "        \n",
    "        for file in files:\n",
    "            file_path = os.path.join(local_path, file['name'])\n",
    "            \n",
    "            if file['type'] == 'file':\n",
    "                # Get file download URL\n",
    "                download_url = file['download_url']\n",
    "                print(f\"Downloading: {file['name']}\")\n",
    "                \n",
    "                # Download the file\n",
    "                file_response = requests.get(download_url, headers=headers)\n",
    "                if file_response.status_code == 200:\n",
    "                    with open(file_path, 'wb') as f:\n",
    "                        f.write(file_response.content)\n",
    "                    print(f\"Successfully downloaded {file['name']}\")\n",
    "                else:\n",
    "                    print(f\"Failed to download {file['name']}\")\n",
    "            \n",
    "            elif file['type'] == 'dir':\n",
    "                # Recursively download directory\n",
    "                new_api_url = file['url']\n",
    "                download_github_directory(new_api_url, file_path, token)\n",
    "    else:\n",
    "        print(f\"Failed to access {api_url}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "\n",
    "# Usage\n",
    "# Replace with your repository details\n",
    "repo_owner = \"BTW25-Data-Science-Challenge\"\n",
    "repo_name = \"final-submission\"\n",
    "path_in_repo = \"models/models/guardian_embedding_model\"\n",
    "\n",
    "base_api_url = f\"https://api.github.com/repos/BTW25-Data-Science-Challenge/final-submission/tree/main/models/models/guardian_embedding_model\"\n",
    "base_local_path = \"models/models/test\"\n",
    "\n",
    "# Optional: Add your GitHub personal access token\n",
    "github_token = \"YOUR_GITHUB_TOKEN\"  # Replace with your token or remove if not needed\n",
    "\n",
    "download_github_directory(base_api_url, base_local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime as dt\n",
    "from functools import partial\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import datetime\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "import holidays\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from entsoe import EntsoePandasClient, EntsoeRawClient\n",
    "from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_exponential\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from newsapi import NewsApiClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "##Stockmarket\n",
    "url_oil = 'https://www.finanzen.net/rohstoffe/oelpreis'\n",
    "url_gas = 'https://www.finanzen.net/rohstoffe/erdgas-preis-natural-gas'\n",
    "url_coal = 'https://www.finanzen.net/rohstoffe/kohlepreis'\n",
    "url_uran = 'https://www.finanzen.net/rohstoffe/uranpreis'\n",
    "\n",
    "\n",
    "##Covid Lockdown Data\n",
    "\n",
    "FILE_URL = 'https://pada.psycharchives.org/bitstream/9ff033a9-4084-4d0e-87eb-aa963a1324a5'\n",
    "covid_df = pd.read_csv(FILE_URL, sep=\",\", header=[0])\n",
    "print(covid_df.head().iloc[:,:5])\n",
    "\n",
    "# dict with influence of measure (see readme)\n",
    "measure_influence = {\n",
    "    'leavehome': 1,\n",
    "    'dist': 0,\n",
    "    'msk': 1,\n",
    "    'shppng': 2,\n",
    "    'hcut': 2,\n",
    "    'ess_shps': 2,\n",
    "    'zoo': 0,\n",
    "    'demo': 0,\n",
    "    'school': 1,\n",
    "    'church': 0,\n",
    "    'onefriend': 0,\n",
    "    'morefriends': 0,\n",
    "    'plygrnd': 0,\n",
    "    'daycare': 2,\n",
    "    'trvl': 1,\n",
    "    'gastr': 2\n",
    "}\n",
    "# dict with state relative population of country\n",
    "state_percentages = {\n",
    "    'Baden-Wuerttemberg': 0.133924061,\n",
    "    'Bayern': 0.158676851,\n",
    "    'Berlin': 0.044670274,\n",
    "    'Brandenburg': 0.030491172,\n",
    "    'Bremen': 0.008169464,\n",
    "    'Hamburg': 0.022560236,\n",
    "    'Hessen': 0.075833,\n",
    "    'Mecklenburg-Vorpommern': 0.019245033,\n",
    "    'Niedersachsen': 0.096398323,\n",
    "    'Nordrhein-Westfalen': 0.214840756,\n",
    "    'Rheinland-Pfalz': 0.049301337,\n",
    "    'Saarland': 0.011744796,\n",
    "    'Sachsen': 0.048299274,\n",
    "    'Sachsen-Anhalt': 0.025752514,\n",
    "    'Schleswig-Holstein': 0.035026746,\n",
    "    'Thueringen': 0.025066162\n",
    "}\n",
    "\n",
    "## Smard\n",
    "\n",
    "#-------------translation for Balancing:------------------\n",
    "balancing_id={\n",
    "    #automatic frequency, tag=af\n",
    "    \"automatic_frequency\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Volume activated (+) [MWh] Calculated resolutions\":\"af_E_Volume_Activated_Plus_MWh\",\n",
    "        \"Volume activated (-) [MWh] Calculated resolutions\":\"af_E_Volume_Activated_Minus_MWh\",\n",
    "        \"Activation price (+) [/MWh] Calculated resolutions\":\"af_Activation_Price_Plus_EUR_MWh\",\n",
    "        \"Activation price (-) [/MWh] Calculated resolutions\":\"af_Activation_Price_Minus_EUR_MWh\",\n",
    "        \"Volume procured (+) [MW] Calculated resolutions\":\"af_E_Volume_Procured_Plus_MW\",\n",
    "        \"Volume procured (-) [MW] Calculated resolutions\":\"af_E_Volume_Procured_Minus_MW\",\n",
    "        \"Procurement price (+) [/MW] Calculated resolutions\":\"af_Procurement_Price_Plus_EUR_MW\",\n",
    "        \"Procurement price (-) [/MW] Calculated resolutions\":\"af_Procurement_Price_Minus_EUR_MW\",\n",
    "    },\n",
    "    #tag=mf\n",
    "    \"manual_frequency\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Volume activated (+) [MWh] Calculated resolutions\":\"mf_E_Volume_Activated_Plus_MWh\",\n",
    "        \"Volume activated (-) [MWh] Calculated resolutions\":\"mf_E_Volume_Activated_Minus_MWh\",\n",
    "        \"Activation price (+) [/MWh] Calculated resolutions\":\"mf_Activation_Price_Plus_EUR_MWh\",\n",
    "        \"Activation price (-) [/MWh] Calculated resolutions\":\"mf_Activation_Price_Minus_EUR_MWh\",\n",
    "        \"Volume procured (+) [MW] Calculated resolutions\":\"mf_E_Volume_Procured_Plus_MW\",\n",
    "        \"Volume procured (-) [MW] Calculated resolutions\":\"mf_E_Volume_Procured_Minus_MW\",\n",
    "        \"Procurement price (+) [/MW] Calculated resolutions\":\"mf_Procurement_Price_Plus_EUR_MW\",\n",
    "        \"Procurement price (-) [/MW] Calculated resolutions\":\"mf_Procurement_Price_Minus_EUR_MW\",\n",
    "    },\n",
    "     #balancing energy\n",
    "    \"balancing_energy\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Volume (+) [MWh] Calculated resolutions\":\"E_Volume_Calculated_Plus_MWh\",\n",
    "        \"Volume (-) [MWh] Calculated resolutions\":\"E_Volume_Calculated_Minus_MWh\",\n",
    "        \"Price [/MWh] Calculated resolutions\":\"Price_Calculated_EUR_MWh\",\n",
    "        \"Net income [] Calculated resolutions\":\"Net_Income_EUR\",\n",
    "    },\n",
    "    #costs\n",
    "    \"costs\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Balancing services [] Calculated resolutions\":\"Balancing_Services_Calculated_EUR\",\n",
    "        \"Network security [] Calculated resolutions\":\"Network_Security_Calculated_EUR\",\n",
    "        \"Countertrading [] Calculated resolutions\":\"Countertrading_Calculated_EUR\",\n",
    "    },\n",
    "    #frequency_containment_reserve\n",
    "    \"frequency_containment\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Volume procured [MW] Calculated resolutions\":\"E_Volume_Procured_Calculated_MW\",\n",
    "        \"Procurement price [/MW] Calculated resolutions\":\"Price_Procument_Calculated_EUR/MW\"\n",
    "    },\n",
    "    \"imported_balancing_services\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Austria [MWh] Calculated resolutions\":\"import_E_Austria_Calculated_MWh\",\n",
    "    },\n",
    "    \"exported_balancing_services\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Austria [MWh] Calculated resolutions\":\"export_E_Austria_Calculated_MWh\",\n",
    "    }         \n",
    "}    \n",
    "\n",
    "#actual consumption tag=actual\n",
    "electricity_consumption_id={\n",
    "    \"actual\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Total (grid load) [MWh] Calculated resolutions\":\"actual_E_Total_Gridload_MWh\",\n",
    "        \"Residual load [MWh] Calculated resolutions\":\"actual_E_Residual_Load_MWh\",\n",
    "        \"Hydro pumped storage [MWh] Calculated resolutions\":\"actual_E_Hydro_Pumped_Storage_MWh\",\n",
    "    },\n",
    "    #forecasted consumption tag=forecast\n",
    "    \"forecast\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Total (grid load) [MWh] Calculated resolutions\":\"forecast_E_Total_Gridload_MWh\",\n",
    "        \"Residual load [MWh] Calculated resolutions\":\"forecast_actual_E_Residual_Load_MWh\"\n",
    "    }\n",
    "}\n",
    "\n",
    "electricity_generation_id={\n",
    "    #actual generation\n",
    "    \"actual\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Biomass [MWh] Calculated resolutions\":\"actual_generation_E_Biomass_MWh\",\n",
    "        \"Hydropower [MWh] Calculated resolutions\":\"actual_generation_E_Hydropower_MWh\",\n",
    "        \"Wind offshore [MWh] Calculated resolutions\":\"actual_generation_E_Windoffshore_MWh\",\n",
    "        \"Wind onshore [MWh] Calculated resolutions\":\"actual_generation_E_Windonshore_MWh\",\n",
    "        \"Photovoltaics [MWh] Calculated resolutions\":\"actual_generation_E_Photovoltaics_MWh\",\n",
    "        \"Other renewable [MWh] Calculated resolutions\":\"actual_generation_E_OtherRenewable_MWh\",\n",
    "        \"Nuclear [MWh] Calculated resolutions\":\"actual_generation_E_Nuclear_MWh\",\n",
    "        \"Lignite [MWh] Calculated resolutions\":\"actual_generation_E_Lignite_MWh\",\n",
    "        \"Hard coal [MWh] Calculated resolutions\":\"actual_generation_E_HardCoal_MWh\",\n",
    "        \"Fossil gas [MWh] Calculated resolutions\":\"actual_generation_E_FossilGas_MWh\",\n",
    "        \"Hydro pumped storage [MWh] Calculated resolutions\":\"actual_generation_E_HydroPumpedStorage_MWh\",\n",
    "        \"Other conventional [MWh] Calculated resolutions\":\"actual_generation_E_OtherConventional_MWh\"\n",
    "    },\n",
    "    \n",
    "    #forecastet generation day ahead\n",
    "    \"forecast\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Biomass [MWh] Calculated resolutions\":\"forecast_generation_E_Biomass_MWh\",\n",
    "        \"Hydropower [MWh] Calculated resolutions\":\"forecast_generation_E_Hydropower_MWh\",\n",
    "        \"Wind offshore [MWh] Calculated resolutions\":\"forecast_generation_E_Windoffshore_MWh\",\n",
    "        \"Wind onshore [MWh] Calculated resolutions\":\"forecast_generation_E_Windonshore_MWh\",\n",
    "        \"Photovoltaics [MWh] Calculated resolutions\":\"forecast_generation_E_Photovoltaics_MWh\",\n",
    "        \"Other renewable [MWh] Calculated resolutions\":\"forecast_generation_E_OtherRenewable_MWh\",\n",
    "        \"Nuclear [MWh] Calculated resolutions\":\"forecast_generation_E_Nuclear_MWh\",\n",
    "        \"Lignite [MWh] Calculated resolutions\":\"forecast_generation_E_Lignite_MWh\",\n",
    "        \"Hard coal [MWh] Calculated resolutions\":\"forecast_generation_E_HardCoal_MWh\",\n",
    "        \"Fossil gas [MWh] Calculated resolutions\":\"forecast_generation_E_FossilGas_MWh\",\n",
    "        \"Hydro pumped storage [MWh] Calculated resolutions\":\"forecast_generation_E_HydroPumpedStorage_MWh\",\n",
    "        \"Other [MWh] Calculated resolutions\":\"forecast_generation_E_Other_MWh\",\n",
    "        \"Total [MWh] Original resolutions\":\"forecast_generation_E_Total_MWh\",\n",
    "        \"Photovoltaics and wind [MWh] Calculated resolutions\":\"forecast_generation_E_PhotovoltaicsAndWind_MWh\",\n",
    "        \"Other [MWh] Original resolutions\":\"forecast_generation_E_Original_MWh\"\n",
    "    },\n",
    "\n",
    "    #installed generation capacity\n",
    "    #key=instGenCapacity\n",
    "    \"installed_generation_capacity\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Biomass [MW] Calculated resolutions\":\"instGenCapacity_E_Biomass_MW\",\n",
    "        \"Hydropower [MW] Calculated resolutions\":\"instGenCapacity_E_Hydropower_MW\",\n",
    "        \"Wind offshore [MW] Calculated resolutions\":\"instGenCapacity_E_Windoffshore_MW\",\n",
    "        \"Wind onshore [MW] Calculated resolutions\":\"instGenCapacity_E_Windonshore_MW\",\n",
    "        \"Photovoltaics [MW] Calculated resolutions\":\"instGenCapacity_E_Photovoltaics_MW\",\n",
    "        \"Other renewable [MW] Calculated resolutions\":\"instGenCapacity_E_OtherRenewable_MW\",\n",
    "        \"Nuclear [MW] Calculated resolutions\":\"instGenCapacity_E_Nuclear_MW\",\n",
    "        \"Lignite [MW] Calculated resolutions\":\"instGenCapacity_E_Lignite_MW\",\n",
    "        \"Hard coal [MW] Calculated resolutions\":\"instGenCapacity_E_HardCoal_MW\",\n",
    "        \"Fossil gas [MW] Calculated resolutions\":\"instGenCapacity_E_FossilGas_MW\",\n",
    "        \"Hydro pumped storage [MW] Calculated resolutions\":\"instGenCapacity_E_HydroPumpedStorage_MW\",\n",
    "        \"Other conventional [MW] Calculated resolutions\":\"instGenCapacity_E_OtherConventional_MW\"\n",
    "    }\n",
    "}\n",
    "\n",
    "market_id={\n",
    "    #key=dayAhead\n",
    "    \"day_ahead_prices\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Germany/Luxembourg [/MWh] Original resolutions\":\"dayAhead_Price_GermanyAndLuxembourg_EUR_MWh\",\n",
    "        \" DE/LU neighbours [/MWh] Original resolutions\":\"dayAhead_Price_GermanyAndLuxembourgAverage_EUR_MWh\",\n",
    "        \"Belgium [/MWh] Original resolutions\":\"dayAhead_Price_Belgium_EUR_MWh\",\n",
    "        \"Denmark 1 [/MWh] Original resolutions\":\"dayAhead_Price_Denmark1_EUR_MWh\",\n",
    "        \"Denmark 2 [/MWh] Original resolutions\":\"dayAhead_Price_Denmark2_EUR_MWh\",\n",
    "        \"France [/MWh] Original resolutions\":\"dayAhead_Price_France_EUR_MWh\",\n",
    "        \"Netherlands [/MWh] Original resolutions\":\"dayAhead_Price_Netherlands_EUR_MWh\",\n",
    "        \"Norway 2 [/MWh] Original resolutions\":\"dayAhead_Price_Norway2_EUR_MWh\",\n",
    "        \"Austria [/MWh] Original resolutions\":\"dayAhead_Price_Austria_EUR_MWh\",\n",
    "        \"Poland [/MWh] Original resolutions\":\"dayAhead_Price_Poland_EUR_MWh\",\n",
    "        \"Sweden 4 [/MWh] Original resolutions\":\"dayAhead_Price_Sweden4_EUR_MWh\",\n",
    "        \"Switzerland [/MWh] Original resolutions\":\"dayAhead_Price_Switzerland_EUR_MWh\",\n",
    "        \"Czech Republic [/MWh] Original resolutions\":\"dayAhead_Price_CzechRepublic_EUR_MWh\",\n",
    "        \"DE/AT/LU [/MWh] Original resolutions\":\"dayAhead_Price_DE/AT/LU_EUR_MWh\",\n",
    "        \"Northern Italy [/MWh] Original resolutions\":\"dayAhead_Price_NothernItaly_EUR_MWh\",\n",
    "        \"Slovenia [/MWh] Original resolutions\":\"dayAhead_Price_Slovenia_EUR_MWh\",\n",
    "        \"Hungary [/MWh] Original resolutions\":\"dayAhead_Price_Hungary_EUR_MWh\"\n",
    "    },\n",
    "    \n",
    "    \"cross_border_physical\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Net export [MWh] Calculated resolutions\":\"E_NetExport_crossBorderPhysical_MWh\",\n",
    "        \"Netherlands (export) [MWh] Calculated resolutions\":\"E_NetherlandExport_corssBorderPhysical_MWh\",\n",
    "        \"Netherlands (import) [MWh] Calculated resolutions\":\"E_NetherlandImport_corssBorderPhysical_MW\",\n",
    "        \"Switzerland (export) [MWh] Calculated resolutions\":\"E_SwitzerlandExport_corssBorderPhysical_MWh\",\n",
    "        \"Switzerland (import) [MWh] Calculated resolutions\":\"E_SwitzerlandImport_corssBorderPhysical_MWh\",\n",
    "        \"Denmark (export) [MWh] Calculated resolutions\":\"E_DenmarkExport_corssBorderPhysical_MWh\",\n",
    "        \"Denmark (import) [MWh] Calculated resolutions\":\"E_Denmark_Import_corssBorderPhysical_MWh\",\n",
    "        \"Czech Republic (export) [MWh] Calculated resolutions\":\"E_CzechrepublicExport_corssBorderPhysical_MWh\",\n",
    "        \"Czech Republic (import) [MWh] Calculated resolutions\":\"E_CzechrepublicImport_corssBorderPhysical_MWh\",\n",
    "        \"Luxembourg (export) [MWh] Calculated resolutions\":\"E_LuxembourgExport_corssBorderPhysical_MWh\",\n",
    "        \"Luxembourg (import) [MWh] Calculated resolutions\":\"E_LuxembourgImport_corssBorderPhysical_MWh\",\n",
    "        \"Sweden (export) [MWh] Calculated resolutions\":\"E_SwedenExport_corssBorderPhysical_MWh\",\n",
    "        \"Sweden (import) [MWh] Calculated resolutions\":\"E_SwedenImportv_corssBorderPhysical_MWh\",\n",
    "        \"Austria (export) [MWh] Calculated resolutions\":\"E_AustriaExport_corssBorderPhysical_MWh\",\n",
    "        \"Austria (import) [MWh] Calculated resolutions\":\"E_AustriaImport_corssBorderPhysical_MWh\",\n",
    "        \"France (export) [MWh] Calculated resolutions\":\"E_FranceExport_corssBorderPhysical_MWh\",        \n",
    "        \"France (import) [MWh] Calculated resolutions\":\"E_FranceImport_corssBorderPhysical_MWh\",\n",
    "        \"Poland (export) [MWh] Calculated resolutions\":\"E_PolandExport_corssBorderPhysical_MWh\",\n",
    "        \"Poland (import) [MWh] Calculated resolutions\":\"E_PolandImport_corssBorderPhysical_MWh\",\n",
    "        \"Norway (export) [MWh] Calculated resolutions\":\"E_NorwayExport_corssBorderPhysical_MWh\",\n",
    "        \"Norway (import) [MWh] Calculated resolutions\":\"E_NorwayImport_corssBorderPhysical_MWh\",\n",
    "        \"Belgium (export) [MWh] Calculated resolutions\":\"E_BelgiumExport_corssBorderPhysical_MWh\",\n",
    "        \"Belgium (import) [MWh] Calculated resolutions\":\"E_BelgiumImport_corssBorderPhysical_MWh\",\n",
    "    },\n",
    "    \"scheudled_commercial_exchanges\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Net export [MWh] Calculated resolutions\":\"E_NetExport_MWh\",\n",
    "        \"Netherlands (export) [MWh] Calculated resolutions\":\"E_NetherlandExport_MWh\",\n",
    "        \"Netherlands (import) [MWh] Calculated resolutions\":\"E_NetherlandImport_MW\",\n",
    "        \"Switzerland (export) [MWh] Calculated resolutions\":\"E_SwitzerlandExport_MWh\",\n",
    "        \"Switzerland (import) [MWh] Calculated resolutions\":\"E_SwitzerlandImport_MWh\",\n",
    "        \"Denmark (export) [MWh] Calculated resolutions\":\"E_DenmarkExport_MWh\",\n",
    "        \"Denmark (import) [MWh] Calculated resolutions\":\"E_Denmark_Import_MWh\",\n",
    "        \"Czech Republic (export) [MWh] Calculated resolutions\":\"E_CzechrepublicExport_MWh\",\n",
    "        \"Czech Republic (import) [MWh] Calculated resolutions\":\"E_CzechrepublicImport_MWh\",\n",
    "        \"Luxembourg (export) [MWh] Calculated resolutions\":\"E_LuxembourgExport_MWh\",\n",
    "        \"Luxembourg (import) [MWh] Calculated resolutions\":\"E_LuxembourgImport_MWh\",\n",
    "        \"Sweden (export) [MWh] Calculated resolutions\":\"E_SwedenExport_MWh\",\n",
    "        \"Sweden (import) [MWh] Calculated resolutions\":\"E_SwedenImport_MWh\",\n",
    "        \"Austria (export) [MWh] Calculated resolutions\":\"E_AustriaExport_MWh\",\n",
    "        \"Austria (import) [MWh] Calculated resolutions\":\"E_AustriaImport_MWh\",\n",
    "        \"France (export) [MWh] Calculated resolutions\":\"E_FranceExport_MWh\",        \n",
    "        \"France (import) [MWh] Calculated resolutions\":\"E_FranceImport_MWh\",\n",
    "        \"Poland (export) [MWh] Calculated resolutions\":\"E_PolandExport_MWh\",\n",
    "        \"Poland (import) [MWh] Calculated resolutions\":\"E_PolandImport_MWh\",\n",
    "        \"Norway (export) [MWh] Calculated resolutions\":\"E_NorwayExport_MWh\",\n",
    "        \"Norway (import) [MWh] Calculated resolutions\":\"E_NorwayImport_MWh\",\n",
    "        \"Belgium (export) [MWh] Calculated resolutions\":\"E_BelgiumExport_MWh\",\n",
    "        \"Belgium (import) [MWh] Calculated resolutions\":\"E_BelgiumImport_MWh\",\n",
    "    }\n",
    "}\n",
    "\n",
    "##weather\n",
    "#Define stations\n",
    "combine_historicforecast_bool =False\n",
    "station_ids_r = [ \"01262\", \"01975\", \"02667\"]\n",
    "station_ids_f = [ \"10870\", \"10147\", \"10513\"]\n",
    "station_place = [ \"Muenchen\", \"Hamburg\", \"KoelnBonn\" ]\n",
    "#folderstructure\n",
    "output_folder = \"./merged_data/scripts/weather/\"\n",
    "station_folder = \"./merged_data/scripts/weather/stations\"\n",
    "computing_folder = \"./merged_data/scripts/weather/computing_folder\"\n",
    "stations_combined = \"./merged_data/scripts/weather/stations_combined\"\n",
    "data_collection_folder=\"../final-submission/merged_data/data_collection\"\n",
    "forecas_folder=\"../final-submission/merged_data/forecast\"\n",
    "#Basis-URL for dwd-data\n",
    "base_url_review = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/\"\n",
    "url_forecast = \"https://dwd.api.proxy.bund.dev/v30/stationOverviewExtended\"\n",
    "#collums to remove   \n",
    "columns_remove_clouds = [\"STATIONS_ID\",\"eor\", \"QN_8\",\"V_N_I\"]\n",
    "columns_remove_pressure = [\"STATIONS_ID\",\"eor\", \"QN_8\"]\n",
    "columns_remove_sun = [\"STATIONS_ID\",\"eor\", \"QN_7\"]\n",
    "columns_remove_temp = [\"STATIONS_ID\",\"QN_9\", \"eor\"]\n",
    "columns_remove_wind = [\"STATIONS_ID\",\"eor\", \"QN_3\"]\n",
    "columns_remove_precipitation = [\"STATIONS_ID\",\"eor\", \"QN_8\", \"WRTR\", \"RS_IND\"]\n",
    "\n",
    "columns_remove_forecast = ['isDay','dewPoint2m']\n",
    "#URL-endings for historical data\n",
    "data_types = {\n",
    "    \"temperature_historical\": \"air_temperature/historical/\",\n",
    "    \"temperature_recent\": \"air_temperature/recent/\",\n",
    "    \"cloudiness_historical\": \"cloudiness/historical/\",\n",
    "    \"cloudiness_recent\": \"cloudiness/recent/\",\n",
    "    \"pressure_historical\": \"pressure/historical/\",\n",
    "    \"pressure_recent\": \"pressure/recent/\",\n",
    "    \"sun_historical\": \"sun/historical/\",\n",
    "    \"sun_recent\": \"sun/recent/\",\n",
    "    \"wind_historical\": \"wind/historical/\",\n",
    "    \"wind_recent\": \"wind/recent/\",\n",
    "    \"precipitation_recent\": \"precipitation/recent/\",\n",
    "    \"precipitation_historical\": \"precipitation/historical/\",\n",
    "}\n",
    "#header for API\n",
    "headers_weather = {\n",
    "    \"accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##Stockmarket\n",
    "##Stockmarket\n",
    "\n",
    "def directory_exists(filepath):\n",
    "    directory = os.path.dirname(filepath)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# gets data from finanzen.net with the given url, the filename and resource have to be put in, it updates an already existing file, to not use selenium\n",
    "def get_Data(url, filename, resource, before):\n",
    "\n",
    "    #ellaborate header needed, otherwise finanzen.net will give an access denied error\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Referer': 'https://www.finanzen.net',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    }\n",
    "\n",
    "    session = requests.Session()\n",
    "    response = session.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    #used website inspection to find the right table from the website\n",
    "    table = soup.find('table', class_='table table--content-right')\n",
    "\n",
    "    if table:\n",
    "        headers = [th.get_text(strip=True) for th in table.find_all('th')]\n",
    "\n",
    "        #we only need schluss and date, the other columns are irrelevant\n",
    "        datum_index = headers.index('Datum')\n",
    "        schlusskurs_index = headers.index('Schlusskurs')\n",
    "        rows = table.find_all('tr')[1:] \n",
    "        extracted_data = []\n",
    "\n",
    "        for row in rows:\n",
    "            columns = row.find_all('td')\n",
    "            if len(columns) > max(datum_index, schlusskurs_index): \n",
    "                datum = columns[datum_index].get_text(strip=True)\n",
    "                schlusskurs = columns[schlusskurs_index].get_text(strip=True)\n",
    "                schlusskurs = schlusskurs.replace(',', '.')\n",
    "                extracted_data.append({'Date': datum, resource: schlusskurs})\n",
    "\n",
    "        df = pd.DataFrame(extracted_data)\n",
    "\n",
    "    else:\n",
    "        print(\"Table not found\")\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%d.%m.%Y')\n",
    "\n",
    "    old = pd.read_csv(before)\n",
    "    old['Date'] = pd.to_datetime(old['Date'], format='%Y-%m-%d')  \n",
    "\n",
    "    df_filtered = df[~df['Date'].isin(old['Date'])]\n",
    "\n",
    "    if not df_filtered.empty:\n",
    "        old = pd.concat([old, df_filtered], ignore_index=True)\n",
    "\n",
    "    old['Date'] = pd.to_datetime(old['Date'], format='%Y-%m-%d')\n",
    "\n",
    "    # data is in the wrong order, reverses it\n",
    "    old = old.sort_values(by='Date')\n",
    "\n",
    "    # Save the updated and sorted data to a new CSV file\n",
    "    old.to_csv(filename, index=False)\n",
    "    old.to_csv(before, index=False)\n",
    "\n",
    "    print(\"Data saved as\", filename)\n",
    "\n",
    "#the data is missing hour, as it is only daily, fills weekend gaps also\n",
    "def fill_missing_hours(csv):\n",
    "    df = pd.read_csv(csv)\n",
    "\n",
    "    value_Name = df.columns[1]\n",
    "\n",
    "    # Manually parse the 'date' column using the correct format (DD.MM.YY)\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "    df['Date'] = df['Date'].dt.normalize()\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # start 2015\n",
    "    full_hourly_range = pd.date_range(start='01.01.2015', end=df.index.max() + pd.Timedelta(days=1), freq='h')[:-1]\n",
    "\n",
    "    # put prefered null value here\n",
    "    df_full = df.reindex(full_hourly_range, fill_value=pd.NA)\n",
    "    df_full.reset_index(inplace=True)\n",
    "    df_full.rename(columns={'index': 'Date'}, inplace=True)\n",
    "    df_full[value_Name] = df_full.groupby(df_full['Date'].dt.floor('D'))[value_Name].transform(lambda group: group.ffill().bfill())\n",
    "\n",
    "    # fills emptys\n",
    "    df_full.fillna({value_Name:np.nan}, inplace=True)\n",
    "    df_full.to_csv(csv, index=False)\n",
    "    print('Missing Hours Filled: ', csv)\n",
    "\n",
    "##Entsoe\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10), retry=retry_if_exception_type(Exception))\n",
    "def query_entsoe_data(query_func, country_code, start, end):\n",
    "    try:\n",
    "        df = query_func(country_code, start=start, end=end)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying data: {e}\")\n",
    "        raise\n",
    "    return df\n",
    "\n",
    "def merge_data(query_func):\n",
    "    data_old = query_entsoe_data(query_func, country_code_old, start, change_date)\n",
    "    \n",
    "    data_new = query_entsoe_data(query_func, country_code_new, change_date, end)\n",
    "\n",
    "    if not isinstance(data_old, pd.DataFrame):\n",
    "        data_old = data_old.to_frame()\n",
    "    if not isinstance(data_new, pd.DataFrame):\n",
    "        data_new = data_new.to_frame()\n",
    "    \n",
    "    if not data_old.empty and not data_new.empty:\n",
    "        if len(data_old.columns) != len(data_new.columns):\n",
    "            same_columns = list(set(data_old.columns) & set(data_new.columns))\n",
    "            data_old = data_old[same_columns]\n",
    "            data_new = data_new[same_columns]\n",
    "        else:\n",
    "            data_new.columns = data_old.columns\n",
    "    df_combined = pd.concat([data_old, data_new])\n",
    "    df_combined.index = df_combined.index.tz_convert('UTC')\n",
    "    return df_combined\n",
    "\n",
    "def save_df_with_timestamp(df, filename):\n",
    "    df_copy = df.copy()\n",
    "    df_copy.index.name = 'timestamp'\n",
    "    df_copy.to_csv(filename)\n",
    "\n",
    "\n",
    "##Covid Lockdown Data\n",
    "def evaluate_date(request_date):\n",
    "    if request_date in list(covid_df):\n",
    "        truncated_covid_df = covid_df[['state', 'Measure ', request_date]]\n",
    "        sum_value = 0\n",
    "        for index, row in truncated_covid_df.iterrows():\n",
    "            if row.isnull().values.any(): continue  # if any value in row is missing\n",
    "            if measure_influence[row['Measure ']] == 0: continue  # if measure has no influence\n",
    "            sum_value += ((int(row[request_date]) / 5) + 0.6) * state_percentages[row['state']] * measure_influence[\n",
    "                row['Measure ']]  # see readme documentation\n",
    "        return sum_value\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "##smard\n",
    "def main():\n",
    "\n",
    "    output_path = sys.argv[1]\n",
    "\n",
    "    dict_ids = [balancing_id[\"automatic_frequency\"],\n",
    "                balancing_id[\"balancing_energy\"],\n",
    "                balancing_id[\"costs\"],\n",
    "                balancing_id[\"exported_balancing_services\"],\n",
    "                balancing_id[\"frequency_containment\"],\n",
    "                balancing_id[\"imported_balancing_services\"],\n",
    "                balancing_id[\"manual_frequency\"],\n",
    "                electricity_consumption_id[\"actual\"],\n",
    "                electricity_consumption_id[\"forecast\"],\n",
    "                electricity_generation_id[\"actual\"],\n",
    "                electricity_generation_id[\"forecast\"],\n",
    "                market_id[\"cross_border_physical\"],\n",
    "                market_id[\"scheudled_commercial_exchanges\"],\n",
    "                market_id[\"day_ahead_prices\"]    \n",
    "    ]\n",
    "    \n",
    "    final_df = None\n",
    "\n",
    "    for i in range(3):\n",
    "        working_df = download(i)\n",
    "        working_df = new_format(working_df, dict_ids[i])\n",
    "\n",
    "        #if i > 0:\n",
    "        working_df=working_df.drop(working_df.columns[1],axis=1)\n",
    "        #only called once\n",
    "        if final_df is None:\n",
    "            final_df = working_df\n",
    "        else:\n",
    "            final_df = pd.merge(final_df, working_df, on=working_df.columns[0], how='outer')\n",
    "            #final_df = pd.merge(final_df, working_df, on=working_df.columns[0], how='inner', copy=True)\n",
    "    \n",
    "    final_df=final_df[final_df.duplicated(keep=False) == False]\n",
    "\n",
    "    final_df.to_csv(output_path, sep=',', index=False)\n",
    "\n",
    "    #use gzip to compress .csv outputfile to <file_out>.gz\n",
    "    path_object = Path(output_path)\n",
    "    output_pathgz = path_object.with_suffix('.gz')\n",
    "    final_df.to_csv(output_pathgz, sep=',', index=False, compression='gzip')\n",
    "\n",
    "\n",
    "def download_and_merge_multiple_csv(module_ids):\n",
    "    steps = [\"1420066800000\",\"1600000000000\",str(int(datetime.datetime.today().timestamp()))+'000']\n",
    "    csvfiles = []\n",
    "    for timestamp_from, timestamp_to in zip(steps,steps[1:]):\n",
    "        response = requests.post('https://www.smard.de/nip-download-manager/nip/download/market-data',\n",
    "                                 data='{\"request_form\":[{\"format\":\"CSV\",\"moduleIds\":'+module_ids+',\"region\":\"DE\",\"timestamp_from\":'+timestamp_from+',\"timestamp_to\":'+timestamp_to+',\"type\":\"discrete\",\"language\":\"en\",\"resolution\":\"hour\"}]}')\n",
    "        csvfiles.append(response.content.decode('utf-8-sig'))\n",
    "    csvfile_data = csvfiles[0] + csvfiles[1][csvfiles[1].index('\\n'):]\n",
    "    return csvfile_data\n",
    "\n",
    "\n",
    "def download(download_id):\n",
    "    #14 different files\n",
    "    match download_id:\n",
    "        # AUTOMATIC FREQUENCY RESTORATION\n",
    "        case 0:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[18004368,18004369,18004370,18004351,18004371,18004372,18004373,18004374]')\n",
    "        # BALANCING ENERGY\n",
    "        case 1:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[15004383,15004384,15004382,15004390]')\n",
    "        # COSTS\n",
    "        case 2:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[16004391,16000419,16000418]')\n",
    "        # EXPORTED BALANCING SERVICES\n",
    "        case 3:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[20004385]')\n",
    "        #FREQUENCY CONTAINMENT RESERVE\n",
    "        case 4:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[17004363, 17004367]')\n",
    "        # IMPORTED BALANCING SERVICES\n",
    "        case 5:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[21004386]')\n",
    "        # MANUAL FREQUENCY RESTORATION RESERVE\n",
    "        case 6:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[19004377,19004375,19004376,19004352,19004378,19004379,19004380,19004381]')\n",
    "\n",
    "        #electricity consumption, actual\n",
    "        case 7:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[5000410,5004387,5004359]')\n",
    "        #forecast consumption\n",
    "        case 8:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[6000411,6004362]')\n",
    "        #electricity generation actual\n",
    "        case 9:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[1001224,1004066,1004067,1004068,1001223,1004069,1004071,1004070,1001226,1001228,1001227,1001225]')\n",
    "        #electricity generation forecast\n",
    "        case 10:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[2000122,2005097,2000715,2003791,2000123,2000125]')\n",
    "        #MARKET\n",
    "        # CROSSBORDER FLOWS\n",
    "        case 11:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[31004963,31004736,31004737,31004740,31004741,31004988,31004990,31004992,31004994,31004738,31004742,31004743,31004744,31004880,31004881,31004882,31004883,31004884,31004885,31004886,31004887,31004888,31004739]')\n",
    "        # CROSSBORDER SCHEDULED FLOWS\n",
    "        case 12:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[22004629,22004722,22004724,22004404,22004409,22004545,22004546,22004548,22004550,22004551,22004552,22004405,22004547,22004403,22004406,22004407,22004408,22004410,22004412,22004549,22004553,22004998,22004712]')\n",
    "        # DAYAHEAD\n",
    "        case 13:\n",
    "            csvfile_data = download_and_merge_multiple_csv('[8004169,8004170,8000251,8005078,8000252,8000253,8000254,8000255,8000256,8000257,8000258,8000259,8000260,8000261,8000262,8004996,8004997]')\n",
    "\n",
    "    download_df = pd.read_csv(StringIO(csvfile_data), sep=\";\", header=[0], na_values='-', low_memory=False)\n",
    "    return download_df\n",
    "\n",
    "\n",
    "def new_format(df, my_dict):\n",
    "        \n",
    "    #use fitting dict to rename table head\n",
    "    df.rename(columns=my_dict, inplace=True)\n",
    "    \n",
    "    #change Datetime_format; replace '-' with np.nan\n",
    "    df['Start_Date'] = pd.to_datetime(df['Start_Date'])\n",
    "    df['End_Date'] = pd.to_datetime(df['End_Date'])\n",
    "    df.replace(\"-\",np.nan, inplace=True)\n",
    "\n",
    "    #remove , seperator for thousand\n",
    "    df.replace(\",\",\"\", inplace=True, regex=True)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def my_merge(fin_df, work_df, i):\n",
    "\n",
    "    #if i > 0:\n",
    "        #work_df=work_df.drop(work_df.columns[1],axis=1)\n",
    "    work_df=work_df.drop(work_df.columns['End_Date'],axis=1)\n",
    "    #fin_df = pd.merge(fin_df, work_df, on=work_df.columns[0], how='inner', copy=True)\n",
    "    fin_df = pd.merge(fin_df, work_df, on=work_df.columns[0], how='outer')\n",
    "\n",
    "\n",
    "##weather\n",
    "\n",
    "#Definitions of funktions for weather\n",
    "def combine_historic(station_r, place): \n",
    "  #combine data\n",
    "  try:\n",
    "    file_r = os.path.join(station_folder, station_r, f\"{station_r}_data_combined.csv\")\n",
    "    \n",
    "    #read data\n",
    "    df_r = pd.read_csv(file_r)\n",
    "    combined_df=df_r\n",
    "    output_file = os.path.join(stations_combined, f\"{place}_review.csv\")\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Comibe: {station_r} -> {output_file}\")\n",
    "\n",
    "  except FileNotFoundError as e:\n",
    "    print(f\"File not found: {e}\")\n",
    "  except Exception as e:\n",
    "    print(f\"Error while computing{station_r}: {e}\")\n",
    "def combine_all_stations():\n",
    "  files = [f for f in os.listdir(stations_combined) if f.endswith('.csv')]\n",
    "\n",
    "  #rename collums to station name\n",
    "  for file in files:\n",
    "    file_path = os.path.join(stations_combined, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    #extract filename\n",
    "    file_name = os.path.splitext(file)[0]\n",
    "    columname=[df.columns[0]] + [f'{col}_{file_name}' for col in df.columns[1:]]\n",
    "    df.columns = columname\n",
    "    print(f'Renamend collums for {file_name}')\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "  #combine dataframes  \n",
    "  all_data_frames = []\n",
    "  for file in files:\n",
    "    file_path = os.path.join(stations_combined, file)  \n",
    "    \n",
    "    #load data and add to list\n",
    "    try:\n",
    "      df = pd.read_csv(file_path, delimiter=\",\", parse_dates=[\"date\"], date_format=\"%Y%m%d%H\")\n",
    "      all_data_frames.append(df)\n",
    "      print(f\"Add data from: {file_path}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Error while loading {file}: {e}\")\n",
    "  \n",
    "  #if loaded -> combine\n",
    "  if all_data_frames:\n",
    "    combined_data = all_data_frames[0]\n",
    "    for df in all_data_frames[1:]:\n",
    "      df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")                \n",
    "      combined_data = pd.merge(combined_data, df, on=[  \"date\"], how=\"outer\")\n",
    "    combined_data = combined_data.sort_values(by=[  \"date\"]).drop_duplicates(subset=[  \"date\"], keep='last')\n",
    "\n",
    "  #save\n",
    "  final_filename = os.path.join(data_collection_folder, f\"weather.csv\")\n",
    "  combined_data.to_csv(final_filename, index=False)\n",
    "  print(f\"Combined data saved: {final_filename}\")\n",
    "def start_combine_historic():\n",
    "    max_workers = min(os.cpu_count(), len(station_ids_r))  \n",
    "    print(f\"Start cmbination of {len(station_ids_r)} stations with {max_workers} threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_station = {executor.submit(combine_historic, station_r, place): (station_r,  place) for station_r,  place in zip(station_ids_r,  station_place) }    \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()\n",
    "                print(f\"All data combined for {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error while combination of station {station_id}: {e}\")\n",
    "def combine_forecast():\n",
    "\n",
    "  files = [f for f in os.listdir(station_folder) if f.endswith('.csv')]\n",
    "\n",
    "  #rename collumns to stationname\n",
    "  for file in files:\n",
    "    file_path = os.path.join(station_folder, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    #extract filename\n",
    "    file_name = os.path.splitext(file)[0]\n",
    "    columname=[df.columns[0]] + [f'{col}_{file_name}' for col in df.columns[1:]]\n",
    "    df.columns = columname\n",
    "    print(f'renamed collums for {file_name}')\n",
    "    df.to_csv(file_path, index=False)\n",
    " \n",
    "  all_data_frames = []\n",
    "  for file in files:\n",
    "    file_path = os.path.join(station_folder, file)  \n",
    "    \n",
    "    #load data and add to list\n",
    "    try:\n",
    "      df = pd.read_csv(file_path, delimiter=\",\", parse_dates=[\"date\"], date_format=\"%Y%m%d%H\")\n",
    "      all_data_frames.append(df)\n",
    "      print(f\"data added from {file_path}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Error while loading file {file}: {e}\")\n",
    "  \n",
    "   \n",
    "  if all_data_frames:\n",
    "    combined_data = all_data_frames[0]\n",
    "    for df in all_data_frames[1:]:\n",
    "      df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "      combined_data = pd.merge(combined_data, df, on=[  \"date\"], how=\"outer\")\n",
    "    combined_data = combined_data.sort_values(by=[  \"date\"]).drop_duplicates(subset=[  \"date\"], keep='last')\n",
    "\n",
    "\n",
    "  final_filename = os.path.join(forecas_folder, f\"weather_forecast.csv\")\n",
    "  combined_data.to_csv(final_filename, index=False)\n",
    "  print(f\"Saved combined forecast: {final_filename}\")\n",
    "def create_folder():\n",
    "  os.makedirs(computing_folder, exist_ok=True)\n",
    "  os.makedirs(stations_combined, exist_ok=True)\n",
    "  for station in station_ids_r:\n",
    "    output_folder_station = os.path.join(computing_folder, station)\n",
    "    os.makedirs(output_folder_station, exist_ok=True)\n",
    "    station_folder =os.path.join(output_folder,'stations',station)\n",
    "    os.makedirs(station_folder, exist_ok=True)\n",
    "\n",
    "#function to load forecast\n",
    "def station_folderget_weather_data_for_station_review(station_id):\n",
    "    output_filepath = os.path.join(computing_folder,station_id)\n",
    "    print(f\"storage location  {output_filepath}, computing_folder {computing_folder}, station_id {station_id}\")    \n",
    "    for data_type, endpoint in data_types.items():\n",
    "        url = base_url_review + endpoint\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        #lookup zip-file\n",
    "        for line in response.text.splitlines():\n",
    "            if station_id in line and \"zip\" in line:\n",
    "                filename = re.search(r'href=\"(.*?)\"', line).group(1)\n",
    "                file_url = url + filename\n",
    "                \n",
    "                print(f\"Download of: {file_url}\")\n",
    "                file_response = requests.get(file_url)\n",
    "                file_response.raise_for_status()\n",
    "\n",
    "                with zipfile.ZipFile(io.BytesIO(file_response.content)) as z:\n",
    "                    if data_type == \"cloudiness_historical\" or data_type == \"cloudiness_recent\":\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_n_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    elif data_type == \"pressure_historical\" or data_type == \"pressure_recent\":\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_p0_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    elif data_type == \"sun_historical\" or data_type == \"sun_recent\":\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_sd_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    elif data_type == \"wind_historical\" or data_type == \"wind_recent\":\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_ff_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    elif data_type == \"precipitation_historical\" or data_type == \"precipitation_recent\":\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_rr_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    else:\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_tu_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    \n",
    "                    if not txt_files:\n",
    "                        print(f\"No TXT file in the expected format for station {station_id} found.\")\n",
    "                        continue  \n",
    "\n",
    "                    txt_filename = txt_files[0]\n",
    "                    with z.open(txt_filename) as f:\n",
    "                        try:\n",
    "                            df = pd.read_csv(f, sep=\";\", encoding=\"utf-8\")\n",
    "                            if df.empty:\n",
    "                                print(f\"Warning: The file {txt_filename} is empty.\")\n",
    "                            else:\n",
    "                                print(\"Data loaded for:\", txt_filename)\n",
    "                                if data_type == \"temperature_historical\":\n",
    "                                    new_filename = f\"temp_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"temperature_recent\":\n",
    "                                    new_filename = f\"temp_{station_id}_recent.txt\"\n",
    "                                elif data_type == \"cloudiness_historical\":\n",
    "                                    new_filename = f\"clouds_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"cloudiness_recent\":\n",
    "                                    new_filename = f\"clouds_{station_id}_recent.txt\"\n",
    "                                elif data_type == \"pressure_historical\":\n",
    "                                    new_filename = f\"pressure_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"pressure_recent\":\n",
    "                                    new_filename = f\"pressure_{station_id}_recent.txt\"\n",
    "                                elif data_type == \"sun_historical\":\n",
    "                                    new_filename = f\"sun_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"sun_recent\":\n",
    "                                    new_filename = f\"sun_{station_id}_recent.txt\"\n",
    "                                elif data_type == \"wind_historical\":\n",
    "                                    new_filename = f\"wind_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"wind_recent\":\n",
    "                                    new_filename = f\"wind_{station_id}_recent.txt\"      \n",
    "                                elif data_type == \"precipitation_historical\":\n",
    "                                    new_filename = f\"precipitation_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"precipitation_recent\":\n",
    "                                    new_filename = f\"precipitation_{station_id}_recent.txt\"\n",
    "                                output_filename = os.path.join(output_filepath, new_filename)                                \n",
    "                                df.to_csv(output_filename, sep=\";\", encoding=\"utf-8\", index=False)\n",
    "                                print(f\" Saved weather-file as: {output_filepath}\")   \n",
    "                                print(f\" Saved file as: {os.path.abspath(output_filepath)}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error while loading file {txt_filename}: {e}\")\n",
    "    cut_historic_bevor_2015(station_id)\n",
    "\n",
    "def download_weather_data_for_all_stations_review(station_ids):\n",
    "    max_workers = min(os.cpu_count(), len(station_ids))   \n",
    "    print(f\"Start doanload of {len(station_ids)} stations with {max_workers} threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_station = {executor.submit(station_folderget_weather_data_for_station_review, station_id): station_id for station_id in station_ids}        \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()\n",
    "                print(f\"download succeded for{station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error while downloading {station_id}: {e}\")\n",
    "def cut_historic_bevor_2015(station_id):\n",
    "    computing_folder_station = os.path.join(computing_folder, station_id)\n",
    "    station_files = [f for f in os.listdir(computing_folder_station) if re.match(r'(.+)_hist\\.txt', f)]    \n",
    "    for file in station_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        with open(file_path, 'r') as infile:\n",
    "            lines = infile.readlines()\n",
    "        \n",
    "        filtered_lines = []\n",
    "        for line in lines[:1]:\n",
    "            filtered_lines.append(line)\n",
    "        for line in lines[1:]:\n",
    "            columns = line.strip().split(';')\n",
    "            if len(columns) > 1:  \n",
    "                mess_datum = columns[1]\n",
    "                year = int(mess_datum[:4])                \n",
    "                if year >= 2015:\n",
    "                    filtered_lines.append(line)\n",
    "\n",
    "        with open(file_path, 'w') as outfile:\n",
    "            outfile.writelines(filtered_lines)\n",
    "        print(f\"Historically shortened until 2015: {file}\")\n",
    "    remove_columns_review(station_id)\n",
    "\n",
    "def start_cut_historic_bevor_2015(station_ids):\n",
    "    max_workers = min(os.cpu_count(), len(station_ids))   \n",
    "    print(f\"start shortening till 2015 for {len(station_ids)} stations with {max_workers} threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_station = {executor.submit(cut_historic_bevor_2015, station_id): station_id for station_id in station_ids}        \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()\n",
    "                print(f\"Files shortend to 2015 for {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error while shortening files to 2015 for {station_id}: {e}\")\n",
    "\n",
    "def remove_columns_review(station_id):\n",
    "    print('Start Remove Columns')\n",
    "    computing_folder_station =os.path.join(computing_folder, station_id)\n",
    "    temp_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"temp_\") and f.endswith(\".txt\")]\n",
    "    clouds_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"clouds_\") and f.endswith(\".txt\")]\n",
    "    pressure_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"pressure_\") and f.endswith(\".txt\")]\n",
    "    sun_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"sun_\") and f.endswith(\".txt\")]\n",
    "    wind_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"wind_\") and f.endswith(\".txt\")]\n",
    "    precipitation_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"precipitation_\") and f.endswith(\".txt\")]\n",
    "    \n",
    "    for file in clouds_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True)\n",
    "            df = df.drop(columns=[col for col in columns_remove_clouds if col in df.columns])\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"Colums removed from{file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error while processing{file}: {e}\")\n",
    "    \n",
    "    for file in pressure_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True)\n",
    "            df = df.drop(columns=[col for col in columns_remove_pressure if col in df.columns])\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"removed collums from {file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error while prcessing file: {file}: {e}\")\n",
    "\n",
    "    for file in sun_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True) \n",
    "            df = df.drop(columns=[col for col in columns_remove_sun if col in df.columns])\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"removed collums from {file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error while prcessing file: {file}: {e}\")\n",
    "\n",
    "    for file in temp_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True)            \n",
    "            df = df.drop(columns=[col for col in columns_remove_temp if col in df.columns])\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"removed collums from {file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error while prcessing file: {file}: {e}\")\n",
    "\n",
    "    for file in wind_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True) \n",
    "            df = df.drop(columns=[col for col in columns_remove_wind if col in df.columns]) \n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"removed collums from {file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error while prcessing file: {file}: {e}\")\n",
    "\n",
    "    for file in precipitation_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True)\n",
    "            df = df.drop(columns=[col for col in columns_remove_precipitation if col in df.columns])\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"removed collums from {file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error while prcessing file: {file}: {e}\")\n",
    "    combine_historic_recent(station_id)\n",
    "\n",
    "def start_remove_columns_review(station_ids):\n",
    "    max_workers = min(os.cpu_count(), len(station_ids))   \n",
    "    print(f\"Start remove collumns {len(station_ids)} stations with {max_workers} threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_station = {executor.submit(remove_columns_review, station_id): station_id for station_id in station_ids}        \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()  \n",
    "                print(f\"Collumns deleted for {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error while deletion of collumns {station_id}: {e}\")\n",
    "def combine_historic_recent(station_id):\n",
    "    computing_folder_station = os.path.join(computing_folder, station_id)\n",
    "    station_files = [f for f in os.listdir(computing_folder_station) if re.match(r'(.+)_' + station_id + r'_(hist|recent)\\.txt', f)]\n",
    "    file_pairs = {}\n",
    "    for file in station_files:\n",
    "        match = re.match(r'(.+)_' + station_id + r'_(hist|recent)\\.txt', file)\n",
    "        if match:\n",
    "            wettertyp, period = match.groups()\n",
    "            key = f\"{wettertyp}_{station_id}\"\n",
    "            if key not in file_pairs:\n",
    "                file_pairs[key] = {}\n",
    "            file_pairs[key][period] = os.path.join(computing_folder_station, file)\n",
    "\n",
    "    #combine historic an current data\n",
    "    for key, file_pair in file_pairs.items():\n",
    "        if 'hist' in file_pair and 'recent' in file_pair:\n",
    "            hist_df = pd.read_csv(file_pair['hist'], delimiter=\";\")\n",
    "            recent_df = pd.read_csv(file_pair['recent'], delimiter=\";\")\n",
    "            hist_df[\"MESS_DATUM\"] = pd.to_datetime(hist_df[\"MESS_DATUM\"], format=\"%Y%m%d%H\", errors=\"coerce\")\n",
    "            recent_df[\"MESS_DATUM\"] = pd.to_datetime(recent_df[\"MESS_DATUM\"], format=\"%Y%m%d%H\", errors=\"coerce\")\n",
    "\n",
    "            \n",
    "            combined_df = pd.concat([hist_df, recent_df]).drop_duplicates(subset=[\"MESS_DATUM\"], keep='last')\n",
    "            combined_df = combined_df.sort_values(by=[\"MESS_DATUM\"])\n",
    "            combined_df[\"MESS_DATUM\"] = combined_df[\"MESS_DATUM\"].dt.strftime(\"%Y%m%d%H\")\n",
    "\n",
    "            combined_filename = os.path.join(computing_folder_station, f\"{key}_combined.txt\")\n",
    "            combined_df.to_csv(combined_filename, sep=\";\", index=False)\n",
    "            print(f\"Combined data saved: {combined_filename}\")\n",
    "        else:\n",
    "            print(f\"Missing file for {key}\")\n",
    "    combine_all_station_data_review(station_id)\n",
    "\n",
    "def start_combine_historic_recent(station_ids):\n",
    "    max_workers = min(os.cpu_count(), len(station_ids))   \n",
    "    print(f\"Start combination of historic and current data for {len(station_ids)} stations with {max_workers} threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_station = {executor.submit(combine_historic_recent, station_id): station_id for station_id in station_ids}        \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()\n",
    "                print(f\"combined historic and current data for {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error while combining historic and current data {station_id}: {e}\")\n",
    "def combine_all_station_data_review(station_id):\n",
    "    computing_folder_station = os.path.join(computing_folder, station_id)\n",
    "    station_folder_station = os.path.join(station_folder, station_id) \n",
    "    combined_files = [f for f in os.listdir(computing_folder_station) if f.endswith(f\"_{station_id}_combined.txt\")]\n",
    "    all_data_frames = []\n",
    "    for file in combined_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", parse_dates=[\"MESS_DATUM\"], date_format=\"%Y%m%d%H\")\n",
    "            all_data_frames.append(df)\n",
    "            print(f\"data added from {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error while loading file {file}: {e}\")\n",
    "    if all_data_frames:\n",
    "        combined_data = all_data_frames[0]\n",
    "        for df in all_data_frames[1:]:\n",
    "            df[\"MESS_DATUM\"] = pd.to_datetime(df[\"MESS_DATUM\"], format=\"%Y%m%d%H\", errors=\"coerce\")\n",
    "            combined_data = pd.merge(combined_data, df, on=[  \"MESS_DATUM\"], how=\"outer\")\n",
    "        combined_data = combined_data.sort_values(by=[  \"MESS_DATUM\"]).drop_duplicates(subset=[  \"MESS_DATUM\"], keep='last')\n",
    "        combined_data[\"MESS_DATUM\"] = combined_data[\"MESS_DATUM\"].dt.strftime(\"%Y%m%d%H\")\n",
    "        \n",
    "        # change header\n",
    "        header_mapping = {\n",
    "            \"STATIONS_ID\": \"STATIONS_ID\",\n",
    "            \"MESS_DATUM\": \"date\",\n",
    "            \"V_N_I\": \"Wolken_Interp\",\n",
    "            \"V_N\": \"clouds\",\n",
    "            \"P\": \"stationPressure_hPa\",\n",
    "            \"P0\": \"surfacePressure_hPa\",\n",
    "            \"SD_SO\": \"sunshine_min\",\n",
    "            \"TT_TU\": \"T_temperature_C\",\n",
    "            \"RF_TU\": \"humidity_Percent\",\n",
    "            \"F\": \"wind_speed_ms\",\n",
    "            \"D\": \"wind_direction_degree\",\n",
    "            \"R1\": \"precipitationTotal_mm\",\n",
    "            \"RS_IND\": \"precipitation_indicator\"\n",
    "\n",
    "        }\n",
    "    \n",
    "        combined_data.rename(columns=header_mapping, inplace=True)\n",
    "        final_filename = os.path.join(station_folder_station, f\"{station_id}_data_combined.csv\")\n",
    "        combined_data.to_csv(final_filename, sep=\",\", index=False)\n",
    "        print(f\"All data combined for station {station_id} saved in: {final_filename}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"No combined data for station {station_id} found.\")\n",
    "def start_combine_all_station_data_review(station_ids):\n",
    "    max_workers = min(os.cpu_count(), len(station_ids))   \n",
    "    print(f\"Start of combination of data for {len(station_ids)} stations with {max_workers} threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_station = {executor.submit(combine_all_station_data_review, station_id): station_id for station_id in station_ids}        \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()\n",
    "                print(f\"Files combined for {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error while combination of station {station_id}: {e}\")\n",
    "#Weather forecastfunktions:\n",
    "def get_weather_data_for_station_forecast(station_id, station_place):\n",
    "    params = {\n",
    "        \"stationIds\": station_id\n",
    "    }\n",
    "    #prepare request\n",
    "    request = requests.Request(\"GET\", url_forecast, headers=headers_weather, params=params)\n",
    "    prepared_request = request.prepare()\n",
    "    \n",
    "    response = requests.Session().send(prepared_request)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        filename = os.path.join(computing_folder, f\"weather_forecast_{station_place}.json\")\n",
    "        with open(filename, \"w\") as file:\n",
    "            json.dump(data, file, indent=4)\n",
    "        print(f\"Forecast was saved in {filename}\")\n",
    "        with open(filename) as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        for station_id, station_data in data.items():\n",
    "            forecast_data = station_data[\"forecast1\"]\n",
    "            start_time = forecast_data[\"start\"]\n",
    "            time_step = forecast_data[\"timeStep\"]\n",
    "\n",
    "            date = [dt.utcfromtimestamp((start_time + i * time_step) / 1000) for i in range(len(forecast_data[\"temperature\"]))]\n",
    "            \n",
    "            variables = {\n",
    "                \"T_temperature_C\": forecast_data.get(\"temperature\", []),\n",
    "                \"T_temperature_standarddeviation_C\": forecast_data.get(\"temperatureStd\", []),\n",
    "                \"precipitationTotal_mm\": forecast_data.get(\"precipitationTotal\", []),\n",
    "                \"sunshine_min\": forecast_data.get(\"sunshine\", []),\n",
    "                \"dewPoint2m\": forecast_data.get(\"dewPoint2m\", []),\n",
    "                \"surfacePressure_hPa\": forecast_data.get(\"surfacePressure\", []),\n",
    "                \"humidity_Percent\": forecast_data.get(\"humidity\", []),\n",
    "                \"isDay_bool\": forecast_data.get(\"isDay\", [])\n",
    "            }\n",
    "            max_length = max(len(date), *(len(values) for values in variables.values()))\n",
    "            date.extend([None] * (max_length - len(date)))\n",
    "            for key, values in variables.items():\n",
    "                variables[key].extend([None] * (max_length - len(values)))\n",
    "            df = pd.DataFrame({\n",
    "                \"date\": date,\n",
    "                **variables\n",
    "            })             \n",
    "            df[\"T_temperature_C\"] = df[\"T_temperature_C\"].apply(lambda x: x / 10 if pd.notnull(x) else x)\n",
    "            df[\"T_temperature_standarddeviation_C\"] = df[\"T_temperature_standarddeviation_C\"].apply(lambda x: x / 10 if pd.notnull(x) else x)\n",
    "            df[\"surfacePressure_hPa\"] = df[\"surfacePressure_hPa\"].apply(lambda x: x / 10 if pd.notnull(x) else x)\n",
    "            df[\"humidity_Percent\"] = df[\"humidity_Percent\"].apply(lambda x: x / 10 if pd.notnull(x) else x)\n",
    "            df[\"date\"] = df[\"date\"].apply(lambda x: x.strftime(\"%Y%m%d%H\"))\n",
    "            df.to_csv(os.path.join(station_folder, f\"weather_forecast_{station_place}.csv\"), index=False)\n",
    "            print(f\"Weather prediction in weather_forecast_{station_place}.csv convertet\")\n",
    "    else:\n",
    "        print(f\"Error while request {response.status_code}\")\n",
    "def download_weatherforecast_data_for_all_stations_forecast(station_ids, station_places):\n",
    "    for (station_id , station_place) in zip(station_ids, station_places):\n",
    "        print(f\"Start download of Station {station_id}...\")\n",
    "        get_weather_data_for_station_forecast(station_id, station_place)\n",
    "        print()\n",
    "def remove_columns_forecast():\n",
    "    print(\"Start removing columns\")\n",
    "    forecast_files = [f for f in os.listdir(station_folder) if f.startswith(\"weather_forecast_\")]  \n",
    "    print(f\"File: {forecast_files}...\")\n",
    "    for file in forecast_files:\n",
    "        print(f\"Start removing columns for {file}...\")\n",
    "        file_path = os.path.join(station_folder, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\",\", skipinitialspace=True)  \n",
    "            print(f\"Columns in dataframe: {list(df.columns)}\") \n",
    "            df = df.drop(columns=[col for col in columns_remove_forecast if col in df.columns])\n",
    "            df.to_csv(file_path, sep=\",\", index=False)\n",
    "            print(f\"removed collums from {file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error while prcessing file: {file}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "##Stockmarket\n",
    "get_Data(url_oil, '../final-submission/merged_data/data_collection/oilWti.csv', 'Oil WTI', \"../final-submission/merged_data/data_collection/oilWtiOld.csv\")\n",
    "get_Data(url_gas, '../final-submission/merged_data/data_collection/naturalGas.csv', 'Natural Gas', \"../final-submission/merged_data/data_collection/naturalGasOld.csv\")\n",
    "get_Data(url_coal, '../final-submission/merged_data/data_collection/coal.csv', 'Coal', \"../final-submission/merged_data/data_collection/CoalOld.csv\")\n",
    "get_Data(url_uran, '../final-submission/merged_data/data_collection/uran.csv', 'Uran', '../final-submission/merged_data/data_collection/uranOld.csv')\n",
    "\n",
    "fill_missing_hours('../final-submission/merged_data/data_collection/oilWti.csv')\n",
    "fill_missing_hours('../final-submission/merged_data/data_collection/naturalGas.csv')\n",
    "fill_missing_hours('../final-submission/merged_data/data_collection/coal.csv')\n",
    "fill_missing_hours('../final-submission/merged_data/data_collection/uran.csv')\n",
    "\n",
    "df1 = pd.read_csv('../final-submission/merged_data/data_collection/oilWti.csv')\n",
    "df2 = pd.read_csv('../final-submission/merged_data/data_collection/naturalGas.csv')\n",
    "df3 = pd.read_csv('../final-submission/merged_data/data_collection/coal.csv')\n",
    "df35 = pd.read_csv('../final-submission/merged_data/data_collection/uran.csv')\n",
    "\n",
    "merged_df = pd.merge(df1, df2, on='Date', how='outer')\n",
    "merged_df = pd.merge(merged_df, df3, on='Date', how='outer')\n",
    "merged_df = pd.merge(merged_df, df35, on='Date', how='outer')\n",
    "\n",
    "merged_df.to_csv('../final-submission/merged_data/data_collection/merged_data.csv', index=False)\n",
    "\n",
    "print(\"CSV files have been merged and saved.\")\n",
    "\n",
    "end_time = time.time()\n",
    "verstrichene_zeit = end_time - start_time\n",
    "print(f'Ausfhrungszeit nach Stockmarket: {verstrichene_zeit} Sekunden')\n",
    "\n",
    "##entsoe\n",
    "\n",
    "start_time_entsoe = time.time()\n",
    "df4 = pd.read_csv('../final-submission/merged_data/data_collection/day_ahead_prices.csv')\n",
    "df4.drop(df4.columns[2], axis=1, inplace=True)\n",
    "df4.columns.values[1] = 'day_ahead_prices_EURO'\n",
    "df5 = pd.read_csv('../final-submission/merged_data/data_collection/load_forecast.csv')\n",
    "df5.drop(df5.columns[2], axis=1, inplace=True)\n",
    "df6 = pd.read_csv('../final-submission/merged_data/data_collection/generation_forecast.csv')\n",
    "df6.drop(df6.columns[2], axis=1, inplace=True)\n",
    "df7 = pd.read_csv('../final-submission/merged_data/data_collection/intraday_wind_solar_forecast.csv')\n",
    "df7.drop(df7.columns[4], axis=1, inplace=True)\n",
    "df8 = pd.read_csv('../final-submission/merged_data/data_collection/day_ahead_wind_solar_forecast.csv')\n",
    "df8.drop(df8.columns[4], axis=1, inplace=True)\n",
    "\n",
    "merged_df2 = pd.merge(df5, df4, on='Date', how='outer')\n",
    "merged_df2 = pd.merge(merged_df2, df6, on='Date', how='outer')\n",
    "merged_df2 = pd.merge(merged_df2, df7, on='Date', how='outer')\n",
    "merged_df2 = pd.merge(merged_df2, df8, on='Date', how='outer')\n",
    "\n",
    "merged_df2.to_csv('../final-submission/merged_data/data_collection/merged_data2.csv', index=False)\n",
    "\n",
    "df = pd.read_csv('../final-submission/merged_data/data_collection/merged_data2.csv')\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df_filtered = df[df['Date'].dt.minute == 0]\n",
    "df_filtered['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_filtered.to_csv('../final-submission/merged_data/data_collection/merged_data3.csv', index=False)\n",
    "\n",
    "end_time_entsoe = time.time()\n",
    "verstrichene_zeit_entsoe = end_time_entsoe - start_time_entsoe\n",
    "print(f'Ausfhrungszeit nach Merge vor save als csv: {verstrichene_zeit_entsoe} Sekunden')\n",
    "\n",
    "merged_df.to_csv(f'{out_dir}/merged_data_multi_2.csv', index=False)\n",
    "\n",
    "end_time_entsoe = time.time()\n",
    "verstrichene_zeit_entsoe = end_time_entsoe - start_time_entsoe\n",
    "print(f'Ausfhrungszeit nach Merge_to_csv: {verstrichene_zeit_entsoe} Sekunden')\n",
    "\n",
    "end_time_entsoe = time.time()\n",
    "verstrichene_zeit_entsoe = end_time_entsoe - start_time_entsoe\n",
    "print(f'Ausfhrungszeit komplett: {verstrichene_zeit_entsoe} Sekunden')\n",
    "end_time = time.time()\n",
    "verstrichene_zeit = end_time - start_time\n",
    "print(f'Ausfhrungszeit nach Entsoe: {verstrichene_zeit} Sekunden')\n",
    "\n",
    "##Covid Lockdown Data\n",
    "# generate and populate dataframe with all dates from 2015-1-1 - today\n",
    "from datetime import date, timedelta\n",
    "\n",
    "working_dt = date(2015, 1, 1)\n",
    "end_dt = date(date.today().year, date.today().month, date.today().day)\n",
    "delta = timedelta(days=1)\n",
    "\n",
    "data_rows = []\n",
    "\n",
    "# populate df\n",
    "while working_dt <= end_dt:\n",
    "    factor = evaluate_date(working_dt.isoformat())\n",
    "    date = working_dt.isoformat()\n",
    "    for hour in range(24):\n",
    "        timestamp = pd.Timestamp(working_dt.isoformat()) + pd.Timedelta(hours=hour)\n",
    "        data_rows.append({'Date': timestamp, 'Covid factor': factor})  # Add to rows list\n",
    "    working_dt += delta\n",
    "\n",
    "covid_factors_df = pd.DataFrame(data_rows)\n",
    "print(covid_factors_df.head)\n",
    "\n",
    "covid_factors_df.to_csv('../final-submission/merged_data/data_collection/covid.csv', index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "verstrichene_zeit = end_time - start_time\n",
    "print(f'Ausfhrungszeit nach Covidzahlen: {verstrichene_zeit} Sekunden')\n",
    "\n",
    "##Smard\n",
    "output_path = '../final-submission/merged_data/data_collection/smard.csv'\n",
    "\n",
    "dict_ids = [balancing_id[\"automatic_frequency\"],\n",
    "            balancing_id[\"balancing_energy\"],\n",
    "            balancing_id[\"costs\"],\n",
    "            balancing_id[\"exported_balancing_services\"],\n",
    "            balancing_id[\"frequency_containment\"],\n",
    "            balancing_id[\"imported_balancing_services\"],\n",
    "            balancing_id[\"manual_frequency\"],\n",
    "            electricity_consumption_id[\"actual\"],\n",
    "            electricity_consumption_id[\"forecast\"],\n",
    "            electricity_generation_id[\"actual\"],\n",
    "            electricity_generation_id[\"forecast\"],\n",
    "            market_id[\"cross_border_physical\"],\n",
    "            market_id[\"scheudled_commercial_exchanges\"],\n",
    "            market_id[\"day_ahead_prices\"]    \n",
    "    ]\n",
    "    \n",
    "final_df = None\n",
    "\n",
    "for i in range(13):\n",
    "    working_df = download(i)\n",
    "    working_df = new_format(working_df, dict_ids[i])\n",
    "\n",
    "    if i > 0:\n",
    "       working_df=working_df.drop(working_df.columns[1],axis=1)\n",
    "        #only called once\n",
    "    if final_df is None:\n",
    "            final_df = working_df\n",
    "    else:\n",
    "        final_df = pd.merge(final_df, working_df, on=working_df.columns[0], how='inner', copy=True)\n",
    "    \n",
    "final_df.to_csv(output_path, sep=',', index=False)\n",
    "\n",
    "#use gzip to compress .csv outputfile to <file_out>.gz\n",
    "path_object = Path(output_path)\n",
    "output_pathgz = path_object.with_suffix('.gz')\n",
    "final_df.to_csv(output_pathgz, sep=',', index=False, compression='gzip')\n",
    "\n",
    "end_time = time.time()\n",
    "verstrichene_zeit = end_time - start_time\n",
    "print(f'Ausfhrungszeit nach Smard: {verstrichene_zeit} Sekunden')\n",
    "\n",
    "##weather\n",
    "start_time_w = time.time()\n",
    "create_folder()\n",
    "download_weather_data_for_all_stations_review(station_ids_r)\n",
    "\n",
    "end_time_w = time.time()\n",
    "verstrichene_zeit = end_time_w - start_time_w\n",
    "print(f'Ausfhrungszeit Wetter: {verstrichene_zeit} Sekunden')\n",
    "download_weatherforecast_data_for_all_stations_forecast(station_ids_f, station_place)\n",
    "remove_columns_forecast()\n",
    "\n",
    "end_time_w = time.time()\n",
    "verstrichene_zeit = end_time_w - start_time_w\n",
    "print(f'Ausfhrungszeit Wetter: {verstrichene_zeit} Sekunden')\n",
    "\n",
    "start_combine_historic()\n",
    "enend_time_wd = time.time()\n",
    "verstrichene_zeit = end_time_w - start_time_w\n",
    "print(f'Ausfhrungszeit Wetter: {verstrichene_zeit} Sekunden')\n",
    "combine_all_stations()\n",
    "combine_forecast()\n",
    "\n",
    "end_time_w = time.time()\n",
    "verstrichene_zeit = end_time_w - start_time_w\n",
    "print(f'Ausfhrungszeit Wetter: {verstrichene_zeit} Sekunden')\n",
    "\n",
    "end_time = time.time()\n",
    "verstrichene_zeit = end_time - start_time\n",
    "print(f'Ausfhrungszeit nach dem Wetter: {verstrichene_zeit} Sekunden')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##Zusammenfassung\n",
    "df_res = pd.read_csv('../final-submission/merged_data/data_collection/merged_data.csv')\n",
    "df_ens = pd.read_csv('../final-submission/merged_data/data_collection/merged_data3.csv')\n",
    "df_smard = pd.read_csv('../final-submission/merged_data/data_collection/smard.csv')\n",
    "df_smard = df_smard.rename(columns={'Start_Date': 'Date'})\n",
    "df_smard.to_csv('../final-submission/merged_data/data_collection/smard.csv', index=False)\n",
    "df_smard = pd.read_csv('../final-submission/merged_data/data_collection/smard.csv')\n",
    "print(df_smard.head())\n",
    "df_smard['Date'] = pd.to_datetime(df_smard['Date'])\n",
    "df_filteredSmard = df_smard[df_smard['Date'].dt.minute == 0]\n",
    "df_filteredSmard['Date'] = pd.to_datetime(df_filteredSmard['Date']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_filteredSmard.to_csv('../final-submission/merged_data/data_collection/smard.csv', index=False)\n",
    "df_smard = pd.read_csv('../final-submission/merged_data/data_collection/smard.csv')\n",
    "\n",
    "df_weather = pd.read_csv('../final-submission/merged_data/data_collection/weather.csv')\n",
    "df_weather = df_weather.rename(columns={'date': 'Date'})\n",
    "df_covid = pd.read_csv('../final-submission/merged_data/data_collection/covid.csv')\n",
    "df_social = pd.read_csv('../final-submission/merged_data/data_collection/major_social_events.csv')\n",
    "df_carbon = pd.read_csv('../final-submission/merged_data/data_collection/carbon_price_forward_filled.csv')\n",
    "\n",
    "merge_big = pd.merge(df_ens, df_res, on='Date', how='outer')\n",
    "merge_big = pd.merge(merge_big, df_smard, on='Date', how='outer')\n",
    "merge_big = pd.merge(merge_big, df_social, on='Date', how='outer')\n",
    "merge_big = pd.merge(merge_big, df_carbon, on='Date', how='outer')\n",
    "merge_big = pd.merge(merge_big, df_weather, on='Date', how='outer')\n",
    "merge_big = pd.merge(merge_big, df_covid, on='Date', how='outer')\n",
    "\n",
    "#add weekdays and Holidays\n",
    "merge_big['Date'] = pd.to_datetime(merge_big['Date'])\n",
    "merge_big['month'] = merge_big['Date'].dt.month\n",
    "merge_big['weekday'] = merge_big['Date'].dt.weekday  # 0=Montag, 6=Sonntag\n",
    "merge_big['week_of_year'] = merge_big['Date'].dt.isocalendar().week\n",
    "merge_big['is_weekend'] = merge_big['weekday'].isin([5, 6])\n",
    "german_holidays = holidays.Germany(years=range(merge_big['Date'].dt.year.min(),\n",
    "                                               merge_big['Date'].dt.year.max() + 1))\n",
    "merge_big['date'] = merge_big['Date'].dt.date\n",
    "merge_big['is_holiday'] = merge_big['date'].isin(german_holidays)\n",
    "merge_big = merge_big.loc[:, ~merge_big.columns.str.endswith('_y')]\n",
    "merge_big.columns =merge_big.columns.str.replace('_x$', '', regex=True)\n",
    "\n",
    "merge_big.to_csv('../final-submission/merged_data/allData.csv', index=False)\n",
    "\n",
    "print(\"CSV files have been merged and saved.\")\n",
    "end_time = time.time()\n",
    "verstrichene_zeit = end_time - start_time\n",
    "print(f'Ausfhrungszeit komplett: {verstrichene_zeit} Sekunden')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Merging\n",
    "\n",
    "As the data was provided, different parts of the team procured their own share of the data. This approach allowed us to gather more data, but it also meant that, in the end, the different datasets needed to be merged. The most important challenge turned out to be the varying time periods for which different data points were available. For example, some values could be updated every 15 minutes, while others were only available once per day. It was decided that hourly values would be used, and data points that could only provide one value per day would use that value for every hour of the given day. At the same time, there were days where some data points had no values, and in those cases, the corresponding value would be left empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMARD-Data Preprocessing\n",
    "The dataset of choice contains many different variables provided by smard.de, but to actually use the data, a few important steps of preprocessing are needed.\n",
    "At first, the names of the columns have to be changed to differentiate them. The reason for that lies in the way the data is provided; e.g., 'price actual' and 'price forecast' are not stored in the same table and have to be downloaded separately but are both initially just named 'price'.\n",
    "Therefore, the actual name is only implied by the download source, and after a merge of the different tables, it would lead to a name collision.\n",
    "To solve this issue, every column name gets extended with an explicit prefix.\n",
    "\n",
    "Another required preprocessing step, which is needed due to the nature of the model implementation, is to deal with \"not a number\" (short, NaN) values.\n",
    "\n",
    "The two main options to do that are:\n",
    "- Fill NaN with the integer value 0\n",
    "- Fill NaN with the mean of the corresponding column\n",
    "The impact of both methods is compared in section <ins>-> Data Analysis</ins>\n",
    "\n",
    "Dealing with DateTime type values can also be a challenge because, e.g., linear regression cannot work with the data type out of the box. A workaround for this problem is to encode the different segments, like days, hours, etc., to integer values, each representing one part of the time format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMARD-Data:\n",
    "An important step in the process of finding the right features to train the model is to examine the correlation between the variables in the given dataset. Since there are many different features, a heatmap plot is a good way to display them all while still being a practical overview.\n",
    "\n",
    "To get a more precise feature selection, the Python library pandas provides a .corr() function which can be used to get the values satisfying a chosen condition, e.g., getting the ten most correlating values relating to the 'price'.\n",
    "\n",
    "##### Heatmap:\n",
    "<img src=\"src/linreg_heatmap.png\" alt=\"Alt text\" title=\"Title\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation of sample value for the complete timeframe:\n",
    "\n",
    "<img src=\"src/linreg_corr_full.png\" alt=\"Alt text\" title=\"Title\" width=\"800\" />\n",
    "\n",
    "#### Correlation of sample value for a smaller, selected timeframe:\n",
    "\n",
    "<img src=\"src/linreg_corr_slice.png\" alt=\"Alt text\" title=\"Title\" width=\"800\" />\n",
    "\n",
    "The results show that the choice between filling with zero or filling with the mean does not impact the results in a significant way. Rather interesting is the difference between the usage of a timeslice containing fewer empty values. Therefore, the priority should lay on using less but complete data instead of a dataset with big gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data \n",
    "\n",
    "For analysis purposes, the separate columns of the final data frame were checked for missing data, or more accurately, empty entries. These empty entries can have various causes. For example, in the stock market data, there are no values for weekends or holidays. Other columns, like Superbowl or Holidays, have no empty entries as these events are known in advance. This check was done to prevent empty columns in the data frame. In the end, the columns with the highest percentage of missing values were, as expected, the stock market data, with missing values ranging from nearly 50% for uranium to almost 31% for coal. Fortunately, the other columns were more complete, with data points for special occasions and calendar observations (e.g., day of the week) having 0% missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AutoGluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal Fusion Transformer\n",
    "\n",
    "One of the benefits of the TFT model for time series forecasting is the fact that it allows for interpretation mostly in the form of feature importance for both encoder and decoder. Moreover, given its embedded multi-head attention mechanism, the level of attention as a function of the time index for the pastime horizon can also be analyzed.\n",
    "\n",
    "##### Attention\n",
    "\n",
    "<img src=\"src/attention.png\" alt=\"Alt text\" title=\"Title\" width=\"500\" />\n",
    " It can be observed that most of the model attention takes place at the morning and evening times of the past horizon (typically higher electricity prices), as well as in the midday (typically lowest electricity prices).\n",
    "\n",
    "##### Encoder importance\n",
    "\n",
    "<img src=\"src/encoder_variables_importance.png\" alt=\"Alt text\" title=\"Title\" width=\"500\" />\n",
    "\n",
    "Intuitively, electricity prices themselves represent the most important variable to be considered by the model at the time of prediction, namely when encoding the input data. Moreover, considering the fact that besides renewable sources, coal has the largest generation capacity in Germany, the price of this fossil fuel constitutes the second-largest importance for variable encoding.\n",
    "\n",
    "##### Decoder importance\n",
    " <img src=\"src/decoder_variables_importance.png\" alt=\"Alt text\" title=\"Title\" width=\"500\" />\n",
    " Finally, at the decoding stage, most importance is regarded towards natural gas prices probably given its higher volatity in comparison with coal prices. Moreover, as is typical for system electricity demand, the time of day has also a high importance at the time of forecast generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization & Story Telling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Linear regression is a simple machine learning algorithm used to predict a target given a set of variables/features and is based on the following formula:\n",
    "\n",
    "\\begin{equation}\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "- dependent variable (target):\n",
    "\\begin{equation}y\\end{equation}\n",
    "\n",
    "- independent variables (predictor variables):\n",
    "\\begin{equation}\n",
    "(x_1, x_2, \\dots, x_n)\n",
    "\\end{equation}\n",
    "\n",
    "- Intercept:\n",
    "\\begin{equation}\n",
    "\\beta_0 \n",
    "\\end{equation}\n",
    "\n",
    "- Coefficients of the independent variables\n",
    "\\begin{equation}\n",
    "(\\beta_1, \\beta_2, \\dots, \\beta_n)\n",
    "\\end{equation}\n",
    "\n",
    "- Error (residuum)\n",
    "\\begin{equation}\n",
    "\\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "PROS:\n",
    "- typically useful method for time series forecasting\n",
    "- easy to implement\n",
    "- Output can be interpreted; influence of predictor variables\n",
    "- Very high computational speed, can be trained and used for predictions on basic and affordable hardware in a short amount of time, even on large datasets; no high-performance computing cluster needed\n",
    "\n",
    "CONS:\n",
    "- Heavily depends on the correlation of y and X, small room to improve (if the result is bad, not much can be done)\n",
    "- Results can be strongly influenced by outliers in a negative way, which makes the prediction less accurate and less reliable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An LSTM (Long Short-Term Memory) is a type of RNN (Recurrent Neural Network) designed to address the limitations of traditional RNNs, particularly in handling long-term dependencies in sequential data. Unlike standard RNNs, which suffer from issues like vanishing or exploding gradients. When trying to learn relationships over extended sequences, LSTMs incorporate mechanisms that allow them to retain information over long periods. This makes them especially well-suited for tasks like time series forecasting, where understanding patterns and dependencies over time is crucial.\n",
    "To also incorparate multiple input features which might contribute to the day ahead prices, a  multivariate LSTM is used. After training the model the prediction is done using autoregression,  \n",
    "which involves using previous time steps in the sequence as input features to predict the next single value. This process is then repeated by also incorparating the predicted value and until the requested number of steps ahead is reached. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing for LSTM Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are using LSTM models for forecasting, the dataset format needs to fulfill certain conditions. The feature and target sequences needs to be continuous in time. Meening the absence of nan values. Therefore several preprocessing steps are applyed to enshure this condition is satisfied.\n",
    "\n",
    "1. Every feature, that contain at least one consecutive sequence of nan-values longer that 168hours or 7days are dropped from the dataset.\n",
    "2. For every feature that is not dropped by step (1), the nan-values are going to be replaced.\n",
    "\n",
    "This follows the assumption that it is hard to replace nan-values, if the nan sequences whose contain them are getting to long. The replacing is then done by training a XGBoost Regressor model which predict the missing values. This model is trained on the features of the dataset which will not be replaced and the feature containing the nan-values as target. Thats done for every such feature one after another.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When only using categorical encoded features for time-related features like the month, day of week or hour they show no cyclicity. However there is clearly cyclic behaviour contained in them. It makes sense that the connection between two consecutive hours like 23h and 0h is stronger than between 0h and 5h. The same applies to month and day of the week as well.\n",
    "To incorparate this cyclic behavior of time-related features they where encoded using trigonometric functions. For this each such feature gets transformed into two: one sine and one cosine feature.\n",
    "(Code source: https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "def sin_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "\n",
    "\n",
    "def cos_transformer(period):\n",
    "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Decoder LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoregression can struggle with error accumulation as well as limited context when predicting multiple time steps ahead. Seq2Seq (Sequence-to-sequence) models overcome this by encoding the entire fixed length input sequence into a context vector using a Encoder LSTM and pass it to the Decoder LSTM which decodes it to generate the full target sequence. This allows for better handling of long-term dependencies and multi-step forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorparate Bahdanau Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq models without attention faces challenges like limited context representation and difficulty in capturing long-range dependencies. Instead of relying on a single fixed-length context vector, Bahdanau attention computes a weighted combination of input sequence elements for each output time step. \n",
    "When using the attention mechanism, the context vector is created by combining the final hidden state of the encoder with the attention weights, while without attention only the encoders final hidden state is used. This context vector then gets processed by the decoder to generate the output sequence.\n",
    "This allows the model to focus on the most relevant parts of the input sequence when predicting each target value. By leveraging this mechanism, the model can better handle complex temporal relationships and improve forecasting accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chronos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chronos is a framework for pre-trained probabilistic time series models introduced in March 2024 by Ansari et al. It tokenizes time series values into a fixed vocabulary through scaling and quantization and trains transformer-based language models on these tokens using the cross-entropy loss function. These context tokens are then used for the (pre)training. Chronos is designed without time-series-specific architecture, resulting in a minimalistic yet effective approach. The framework achieved remarkable results in in-domain experiments and demonstrated competitive zero-shot performance, comparable to models specifically trained on similar tasks.\n",
    "\n",
    "The developers of Chronos provided a GitHub Repository (https://github.com/amazon-science/chronos-forecasting/tree/main) which allows to use the pretrained models for forecasting and fine-tune models on additional data. Chronos is built on Google's T5 architecture, which has gained significant popularity in recent years. However, a key difference between Chronos and the original T5 architecture is the reduced vocabulary size, which results in varying parameter counts. Chronos offers five distinct models, with sizes ranging from 8 million to 710 million parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this work, the pretrained Chronos-T5 (Tiny) model was utilized as a benchmark to assess and improve its performance by fine-tuning it on our own data. Specifically, we focused on fine-tuning the model with the day-ahead electricity prices from ENTSO-E as a domain-specific dataset. The tiny model was chosen for its practicality, as it can be fine-tuned and utilized for forecasting tasks even on a standard laptop. To gain deeper insights into the impact of the dataset size and training steps on model performance, we conducted fine-tuning experiments in four distinct ways.\n",
    "\n",
    "The day-ahead-prices for energy has experienced heightened volatility in recent years, driven by geopolitical and economic disruptions such as the Ukraine war. To evaluate the impact of dataset characteristics on model performance, we divided the data into two subsets. The first dataset contains day-ahead prices from January 2022 to December 2023, and the second one data spanning from January 2015 to December 2023. The smaller dataset focuses primarily on recent, highly volatile market conditions, reflecting current dynamics. In contrast, the larger dataset spans a longer historical period, capturing a broader range of market scenarios. This approach enables a direct comparison to determine whether the smaller, more focused dataset enhances adaptability to recent volatility or if the larger dataset provides a more comprehensive foundation due to its diversity.\n",
    "\n",
    "The developers of Chronos fine-tuned their Chronos-T5 (Small) model with 1000 training steps and achieved remarkable results. We were interested if even more fine-tuning steps could increase the models performance. Therefore, for both datasets, the fine-tuning was executed with two configurations of training steps1,000 and 10,000 steps. This comprehensive setup aimed to explore the potential of domain-specific fine-tuning in enhancing the model's capabilities.\n",
    "\n",
    "Chronos models are non-deterministic models. Therefore an evaluation which of the fine-tuned models performs the best should be done on a large test-set. To determine which configuration performs best for the challenge of predicting 24-hour day-ahead energy prices, we conducted forecasts for a full year, from the beginning of December 2023 to the end of November 2024. For each forecasted day, the context data consisted of the most recent 512 hourly day-ahead prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are computes as following. For each 24 values of one day we calculated the root mean squared error and the absolute error. For each day the mean of these values are calculated and over the whole year the mean is created again. The yearly Results of the Chronos-T5 (Tiny) model and its fine-tuned versions are presented in the table below.\n",
    "\n",
    "|Chronos-T5 (Tiny)    |1. Zero-Shot  |2. Fine-Tuned Data: 2015 Steps: 1000|3. Fine-Tuned Data: 2015 Steps: 10000|4. Fine-Tuned Data: 2022 Steps: 1000|5. Fine-Tuned Data: 2022 Steps: 10000|\n",
    "|------------|------------|------------|------------|------------|------------|\n",
    "| RMSE              | 25.817  | 22.739    | 23.501 | 24.552 | 26.413 |\n",
    "| RMSE %   | 117.48% | 120.92%   | 146.55% | 128.25% | 137.28% |\n",
    "| MAE               | 20.127  | 17.254    | 17.969 | 18.59 | 20.263 |\n",
    "| MAE %    | 2637.80% | 2484.01% | 3311.03% | 3168.22% | 4038.85% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results reveal several key insights. First, not all fine-tuned models outperform the benchmark. We can also observe that the MAE percentage and RMSE percentage values are unexpectedly high. This occurs because these metrics are dominated by a few days, where the actual day-ahead price is close to zero or even less than zero, resulting in extremely high values. Therefore, for the evaluation, we should focus on the RMSE and MAE values instead. Additionally, models fine-tuned on the larger dataset, which includes data from 2015, tend to perform better than those fine-tuned on the smaller, more recent dataset. This may be due to the fact that, although the energy market is currently highly volatile, less volatile days dominate the market, making the larger dataset, with its broader range of scenarios, more beneficial. Furthermore, the models trained with 1,000 steps, rather than 10,000, tend to perform better. This could indicate that overfitting occurred with the longer training duration.\n",
    "\n",
    "It is clear that the model which was fine-tuned on the large dataset with 1000 training steps performs the best. This model has the lowest RMSE and MAE. On average the difference between the predicted hourly day-ahead price and its actual value is 17.254 Euros.\n",
    "\n",
    "Interesting is that model three and four have still a better performance in the MAE and RMSE, but its corresponding percentage value are outperformed by the benchmark. A reason might be that with the pretraining the overall error is reduced, but the models are getting worse in predicting extreme scenarios like energy prices above 500 Euros and prices below zero euros.\n",
    "\n",
    "Model five is outperformed by the benchmark in all metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing tiny fine-tuned model - model two - has been uploaded to Hugging Face and is available for use. A comparison of model one with model two is visualized in the 2 following graphs. Each model forecasts the period from 02.14.2024-02.23.2024 which includes the date for the challenge - the 18th of February - just one year earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = \"amazon/chronos-t5-tiny\"\n",
    "forecast1 = chronosForecast(model1)\n",
    "\n",
    "model2 = \"juliushanusch/chronos-tiny-fine-tuned-day-ahead-prices\"\n",
    "forecast2 = chronosForecast(model2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the figure with two subplots (1 row, 2 columns)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot for model1 (forecast1)\n",
    "axs[0].plot(forecast1['timestamp'], forecast1['actual_value'], label='Actual', color='blue', linestyle='-', linewidth=2)\n",
    "axs[0].plot(forecast1['timestamp'], forecast1['forecasted_values'], label='Forecasted', color='orange', linestyle='--', linewidth=2)\n",
    "axs[0].plot(forecast1['timestamp'], forecast1['absolute_error'], label='Absolute Error', color='red', linestyle=':', linewidth=2)\n",
    "\n",
    "axs[0].set_xlabel('Timestamp')\n",
    "axs[0].set_ylabel('Value')\n",
    "axs[0].set_title(f'Actual vs Forecasted Values with Absolute Error \\n Model: {model1}')\n",
    "axs[0].legend()\n",
    "axs[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot for model2 (forecast2)\n",
    "axs[1].plot(forecast2['timestamp'], forecast2['actual_value'], label='Actual', color='blue', linestyle='-', linewidth=2)\n",
    "axs[1].plot(forecast2['timestamp'], forecast2['forecasted_values'], label='Forecasted', color='orange', linestyle='--', linewidth=2)\n",
    "axs[1].plot(forecast2['timestamp'], forecast2['absolute_error'], label='Absolute Error', color='red', linestyle=':', linewidth=2)\n",
    "\n",
    "axs[1].set_xlabel('Timestamp')\n",
    "axs[1].set_ylabel('Value')\n",
    "axs[1].set_title(f'Actual vs Forecasted Values with Absolute Error \\n Model: {model2}')\n",
    "axs[1].legend()\n",
    "axs[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust the layout to make sure there is no overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, several sizes of Chronos models are available. In addition to the Chronos-T5 (Tiny) model, we evaluated the performance of the Chronos-T5 (Large) model. For this model, we applied the same fine-tuning steps as we did for the Chronos-T5 (Tiny) model.\n",
    "The Chronos-T5 (Large) model contains 710 million parameters and is therefore expected to perform better than the Chronos-T5 (Tiny) model. However, this improved performance comes at the cost of significantly higher computational requirements, both in terms of training and forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The yearly Results of the Chronos-T5 (Large) model and its fine-tuned versions are presented in the table below.\n",
    "\n",
    "|Chronos-T5 (Large)    |1. Zero-Shot  |2. Fine-Tuned Data: 2015 Steps: 1000|3. Fine-Tuned Data: 2015 Steps: 10000|4. Fine-Tuned Data: 2022 Steps: 1000|5. Fine-Tuned Data: 2022 Steps: 10000|\n",
    "|------------|------------|------------|------------|------------|------------|\n",
    "| RMSE | 22.450 | 21.052 | 22.072 | 22.744 | 22.304 |\n",
    "| RMSE % | 111.78% | 124.34% | 142.17% | 122.61% | 110.01% |\n",
    "| MAE | 17.142| 15.850| 16.747 | 17.183 | 16.952 |\n",
    "| MAE % | 2395.71% | 2577.38% | 3054.47% | 3089.52% | 3153.29% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the large model performs better compared to the tiny models. We observe similar results in fine-tuning as seen with the tiny models. Once again, the models trained on the smaller dataset outperform those trained on the larger dataset. Interestingly, the non-fine-tuned Chronos-T5 (Large) model performs quite similarly to the Chronos-T5 (Tiny) model fine-tuned on the larger dataset with 1,000 training steps.\n",
    "\n",
    "Notably, Model five shows better performance relative to the other large models compared to its corresponding tiny model. It even surpasses the benchmark and other models in one of the metrics (RMSE as a percentage).\n",
    "\n",
    "With the goal of minimizing the difference between actual and forecasted day-ahead prices, model two emerges as the best-performing model overall. It achieves an average difference of just 15.85 Euros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we uploaded the best large fine-tuned model to Hugging Face - model two. The comparison of the large fine-tuned model number two with model number one is visualized in the graphs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = \"amazon/chronos-t5-large\"\n",
    "forecast3 = chronosForecast(model1)\n",
    "\n",
    "model4 = \"juliushanusch/chronos-large-fine-tuned-day-ahead-prices\"\n",
    "forecast4 = chronosForecast(model2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "# Set up the figure with two subplots (1 row, 2 columns)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot for model1 (forecast1)\n",
    "axs[0].plot(forecast3['timestamp'], forecast3['actual_value'], label='Actual', color='blue', linestyle='-', linewidth=2)\n",
    "axs[0].plot(forecast3['timestamp'], forecast3['forecasted_values'], label='Forecasted', color='orange', linestyle='--', linewidth=2)\n",
    "axs[0].plot(forecast3['timestamp'], forecast3['absolute_error'], label='Absolute Error', color='red', linestyle=':', linewidth=2)\n",
    "\n",
    "axs[0].set_xlabel('Timestamp')\n",
    "axs[0].set_ylabel('Value')\n",
    "axs[0].set_title(f'Actual vs Forecasted Values with Absolute Error \\n Model: {model3}')\n",
    "axs[0].legend()\n",
    "axs[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot for model2 (forecast2)\n",
    "axs[1].plot(forecast4['timestamp'], forecast4['actual_value'], label='Actual', color='blue', linestyle='-', linewidth=2)\n",
    "axs[1].plot(forecast4['timestamp'], forecast4['forecasted_values'], label='Forecasted', color='orange', linestyle='--', linewidth=2)\n",
    "axs[1].plot(forecast4['timestamp'], forecast4['absolute_error'], label='Absolute Error', color='red', linestyle=':', linewidth=2)\n",
    "\n",
    "axs[1].set_xlabel('Timestamp')\n",
    "axs[1].set_ylabel('Value')\n",
    "axs[1].set_title(f'Actual vs Forecasted Values with Absolute Error \\n Model: {model4}')\n",
    "axs[1].legend()\n",
    "axs[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust the layout to make sure there is no overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the final benchmark section of this report, the best performing tiny model and the best performing large model will be compared against other models developed and discussed throughout the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrated that fine-tuning the models can significantly improve the performance of certain models. Our investigation revealed that the performance of fine-tuned models depends on hyperparameters, such as the dataset used and the number of training steps. To further enhance the Chronos models, hyperparameter optimization could be a promising approach. By performing such optimization, it may be possible to identify the best-performing configuration for a specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Fusion Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main concepts\n",
    "\n",
    "Temporal Fusion Transformers (TFT) are a novel framework designed to address the complexities of multi-horizon time series forecasting. First introduced by Lim et al. in 2019 [Lim et al. 2019], TFT bridges the gap between high prediction accuracy and interpretability, making it uniquely suited for real-world applications where both performance and understanding are important. Traditional time series models often struggle to accommodate diverse data types or fail to provide insights into their decision-making processes. TFT resolves these challenges by integrating modern deep learning techniques with interpretable mechanisms.\n",
    "\n",
    "The model is particularly well-suited for datasets with mixed input types, such as historical data, future known covariates, and static features that do not vary with time. TFT excels in various domains, including finance, energy demand prediction, healthcare forecasting, and supply chain optimization, where data is abundant but understanding the temporal relationships and influences is critical. Its architecture is modular, enabling flexibility and extensibility while maintaining transparency in the decision-making process.\n",
    "\n",
    "#### Model Architechture\n",
    "\n",
    "The TFT architecture comprises several components that work together to extract meaningful patterns from complex temporal data. Below is a detailed explanation of its key building blocks:\n",
    "\n",
    "1. Variable Selection Layers:\n",
    "Variable selection layers dynamically identify and prioritize relevant features from the input data at each time step. This mechanism allows the model to adapt to changing conditions and reduces noise from irrelevant inputs, improving both accuracy and interpretability.\n",
    "\n",
    "2. Gating Mechanisms:\n",
    "To prevent overfitting and enhance robustness, gating mechanisms selectively control the flow of information through the network. By suppressing irrelevant or redundant components, these gates ensure that only the most informative features contribute to the forecast.\n",
    "\n",
    "3. Sequence-to-Sequence Layer:\n",
    "TFT employs recurrent layers, such as Long Short-Term Memory (LSTM) networks, to model local temporal dependencies. These layers process sequential data and extract short-term trends, capturing temporal relationships that are critical for accurate forecasting.\n",
    "\n",
    "4. Static Covariate Encoders:\n",
    "Static covariate encoders handle time-invariant features, such as demographic information or geographical context, that provide essential context for the forecasts. These encoders integrate static information into the temporal modeling process, ensuring consistency throughout the prediction horizon.\n",
    "\n",
    "5. Interpretable Multi-Head Attention Layers:\n",
    "Attention mechanisms allow the model to focus on specific time steps that are most influential for the forecast. By assigning different weights to historical and future inputs, TFT provides insights into the long-term dependencies and temporal dynamics underlying the predictions. This component is especially valuable for the model interpretability, as it highlights which parts of the data significantly impact the results.\n",
    "\n",
    "6. Fully Connected Output Layers:\n",
    "The final layers aggregate information from the previous components and produce the multi-step forecasts. These outputs can be tailored for specific objectives, such as point predictions or probabilistic forecasts, depending on the application.\n",
    "\n",
    "A depiction of all model components and its interactions is shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](src/tft.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation in the forecasting of day-ahead electricity prices\n",
    "\n",
    "Given its flexibility to consider multiple types of covariate information, which foremost represent surrogates of supply and demand (i.e. weather forecasts, holidays, generation capacities and outages), TFT is specially well suited for the forecasting of day-ahead electricity prices. For this aim, the following variables are included in the model:\n",
    "\n",
    "| Data variable                  | Variable type                        | Units             | Source           | Comments                                                                                              |\n",
    "|:-------------------------------|:-------------------------------------|:------------------|:-----------------|:-----------------------------------------------------------------------------------------------------|\n",
    "| Electricity price              | target predicted variable            | EUR / MWh         | ENTSO-E          |                                                                                                      |\n",
    "| Day ahead forecasted system load | time-varying known real input        | MW                | ENTSO-E          | Values are released hours before day ahead gate closure                                              |\n",
    "| Actual system load             | time-varying unknown real input      | MW                | ENTSO-E          |                                                                                                      |\n",
    "| Generation capacities          | time-varying known real input        | MW                | ENTSO-E          | Capacities include known generation and production outages                                           |\n",
    "| Fossil-Fuel prices             | time-varying known real input        | EUR / MWh         | Eikon datastream |                                                                                                      |\n",
    "| Sunshine hours                 | time-varying unknown real input      | minutes per hour  | DWD              | Stations considered: Brocken, Mnchen Flughafen, Hamburg-Fuhlsbttel, Kln/Bonn, Leipzig/Halle       |\n",
    "| Wind speed                     | time-varying unknown real input      | m/s               | DWD              | Stations considered: Brocken, Mnchen Flughafen, Hamburg-Fuhlsbttel, Kln/Bonn, Leipzig/Halle       |\n",
    "| Holidays                       | time-varying known categorical input | NA                | NA               | Holiday considering its name and each federal state                                                  |\n",
    "| Date features                  | time-varying known real input        | NA                | NA               | Encoded as sine and cosine of hour of day, day of week and month of year                              |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data considered, up to date our model has been trained considering the timeframe from October 2018 (stablishment of DE_LU as bidding zone) up to November 2024 (inclusive). Furthermore, day-ahead forecasting tests have been conducted for all days of december 2024 achieving an average MAPE of 60.25 % and an average  MAE of 32.66 EUR/MWh. In this regard, extreme values for electricity prices took place, specially given very low and high values for wind energy generation, which along with solar generation, are indirectly considered as unknown real inputs in this model. Below such forecast results are depicted:\n",
    "\n",
    "![alt text](src/max_mae.png \"Title\")\n",
    "![alt text](src/max_mape.png \"Title\")\n",
    "\n",
    "It can be seen for instance, that for the 12th of December 2024, very high electricity prices where experienced given a relative standstill in wind and solar generation (so called Dunkelflaute). In turn, for the 21st of December, large amounts of renewable generation took place, thus driving electricity prices to quite low values.\n",
    "\n",
    "Interestingly, the TFT model achieved relatively very good prediction results for several days of this month. Below are two of thes cases:\n",
    "\n",
    "![alt text](src/min_mae.png \"Title\")\n",
    "![alt text](src/min_mape.png \"Title\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoGluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our prediction of the day-ahead prices, we also used the open-source AutoML library AutoGluon. AutoGluon has three modes of operation: Tabular, Multimodal, and Time Series. In our predictions, Time Series Forecasting is used, as it can forecast future values of multiple time series when given previous data. A special feature AutoGluon provides its users is that multiple models are trained simultaneously to predict values, and at the end, the forecast from the best model is the output. This means that AutoGluon offers its users the ability to test multiple models and optimize the results without much prior knowledge or manual testing.\n",
    "\n",
    "We provided AutoGluon with our data frame and divided our features into three categories: static data, known covariates, and past covariates.\n",
    "\n",
    "- Static values: Data points like location that do not change over time; there is no such data point in our data frame.\n",
    "\n",
    "- Known covariates: Data points that are known for the future, e.g., day of the week, weather forecast, and holidays/special occasions.\n",
    "\n",
    "- Past covariates: Data points that are not known in the future, e.g., stock market values, cross-border energy flows.\n",
    "\n",
    "As we have collected data from 2015 until now for around 150 data points, it is an enormous amount of data and presents many challenges for processing. Before running with the whole data frame, AutoGluon checks the data for various issues, e.g., too many NULL values or incompatible data types. Since we had already checked our data beforehand, these checks passed without problems. Another test is correlation between data points. If two data points are too closely correlated, AutoGluon removes one from the calculations to avoid spending unnecessary computation time. AG also automatically checks if certain models require longer training time than allocated by the user. In such cases, AG automatically skips training that model and moves on to the next.\n",
    "\n",
    "As mentioned above, AutoGluon trains many models but can also train models specifically chosen by the user. These models range from naive forecasts (which set the forecast to the last observed value) to Temporal Fusion Models and Chronos Models mentioned above. Since these models do not need to be implemented separately, no decision needs to be made in advance, and models can simply be chosen for their results at the end. AG also provides a forecast using a Weighted Ensemble Model, which takes the best-performing models from the training and combines them for an even better forecast. Additionally, AG offers users the opportunity to use different presets and hyperparameters for each model. Presets range from medium_quality to best_quality, where the duration of training increases significantly. Hyperparameters can be chosen for each model separately, or AG can use default values.\n",
    "\n",
    "To evaluate these models, AG offers various metrics from which the user can choose: SQL, WGL, MAE, MAPE, MASE, MSE, RMSE, RMSLE, RMSSE, SMAPE, and WAPE. For our purposes, we selected MSE (mean squared error) and MAPE (mean absolute percentage error). We also compared the training duration for the different presets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Autogluon code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "from autogluon.timeseries import TimeSeriesPredictor # type: ignore\n",
    "from autogluon.common import space # type: ignore\n",
    "from datetime import datetime, timedelta\n",
    "import holidays  # type: ignore #Bibliothek fr Feiertage\n",
    "import os\n",
    "from matplotlib import pyplot as plt # type: ignore\n",
    "import seaborn as sns # type: ignore\n",
    "import time\n",
    "import tensorflow as tf # type: ignore\n",
    "import csv\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import shutil\n",
    "from sklearn.metrics import *\n",
    "\n",
    "def train_autogluon(set_preset,set_time_limit,dataset):\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.inter_op_parallelism_threads = 32\n",
    "    session = tf.compat.v1.Session(config=config)\n",
    "    print(f\"Zeitlimit:{set_time_limit}\")\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    print(f\"Preset: {set_preset}\")\n",
    "    #Zeitstempel beim Start des Skripts erstellen\n",
    "    start_time_script = time.time()\n",
    "    script_start_time = datetime.now().strftime('%Y.%m.%d-%H.%M')\n",
    "    output_folder = f\"/data/horse/ws/fewa833b-time-series-forecast/AutoGluon/Felix/AG-Preset/{script_start_time}_{set_preset}_{dataset}\"\n",
    "    os.makedirs(output_folder, exist_ok=True)  #Ordner erstellen, falls er nicht existiert\n",
    "\n",
    "    python_file = '/data/horse/ws/fewa833b-time-series-forecast/AutoGluon/Felix/AG-Preset/autogluon_train_all.py'\n",
    "    destination_file = os.path.join(output_folder, 'autogluon_train_all.py')\n",
    "    shutil.copy(python_file, destination_file)\n",
    "    \n",
    "    #CSV-Datei laden\n",
    "    if dataset=='all':\n",
    "        file_path = \"/data/horse/ws/fewa833b-time-series-forecast/AutoGluon/Felix/allData.csv\"\n",
    "        loaded_dataframe = \"AllData.csv\"\n",
    "        data = pd.read_csv(file_path)\n",
    "    elif dataset=='less':\n",
    "        file_path = \"/data/horse/ws/fewa833b-time-series-forecast/AutoGluon/Felix/lessData.csv\"\n",
    "        loaded_dataframe = \"lessData.csv\"        \n",
    "        data = pd.read_csv(file_path)\n",
    "        print(data.head())\n",
    "    elif dataset=='dayhead':\n",
    "        file_path = \"/data/horse/ws/fewa833b-time-series-forecast/AutoGluon/Felix/allData.csv\"\n",
    "        loaded_dataframe = \"Dayahead only\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        data = df[[\"Date\", \"day_ahead_prices_EURO\"]]\n",
    "    else:\n",
    "        print(f\"Error: No valid 'dataset': {dataset}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(data.columns)\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.rename(columns={'Date': 'timestamp'}, inplace=True)\n",
    "    #Daten berprfen\n",
    "    print(\"Datenbersicht:\")\n",
    "    print(data.head())\n",
    "\n",
    "    #Correlation Calculation\n",
    "    data_copy = data.copy()\n",
    "    data_copy = data_copy.select_dtypes(include=[np.number])\n",
    "    correlation_matrix = data_copy.corr()\n",
    "    print(correlation_matrix)\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title(\"Correlation Matrix of Extra Features\")\n",
    "    plt.savefig(f\"{output_folder}/correlation.png\")\n",
    "\n",
    "    corr_matrix = correlation_matrix \n",
    "    threshold = 0.95\n",
    "    high_corr_pairs = []\n",
    "    for col in corr_matrix.columns: # Loop through the correlation matrix to find pairs with correlation above the threshold\n",
    "        high_corr = corr_matrix.index[corr_matrix[col] > threshold].tolist()\n",
    "        high_corr = [x for x in high_corr if x != col]\n",
    "        for pair in high_corr:\n",
    "            high_corr_pairs.append((col, pair, corr_matrix[col][pair]))\n",
    "\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Column 1', 'Column 2', 'Correlation'])\n",
    "\n",
    "    high_corr_df.to_csv(f\"{output_folder}/high_correlation_pairs.csv\", index=False)\n",
    "    print(\"High correlation pairs saved to 'high_correlation_pairs.csv'\")\n",
    "\n",
    "    missing_percentage = data.isnull().mean() * 100\n",
    "    print(\"Missing values percentage per column:\")\n",
    "    print(missing_percentage)\n",
    "\n",
    "    missing_df = pd.DataFrame(missing_percentage)\n",
    "    missing_df.to_csv(f\"{output_folder}/missing.csv\", index=True)\n",
    "\n",
    "    high_missing_cols = missing_percentage[missing_percentage > 50].index #more than 50% are missing\n",
    "    print(\"Columns with more than 50% missing values:\", high_missing_cols) \n",
    "    \n",
    "    data['item_id'] = 'item_1'    \n",
    "    required_columns = ['timestamp', 'day_ahead_prices_EURO', 'item_id']\n",
    "    for col in required_columns:\n",
    "        if col not in data.columns:\n",
    "            raise ValueError(f\"Spalte '{col}' fehlt in der Datei {file_path}.\")\n",
    "        \n",
    "    \n",
    "    known_covariates_columns = ['item_id','month','weekday','week_of_year','is_weekend','is_holiday','superbowl_bool','oktoberfest_bool','berlinale_bool','precipitationTotal_mm_KoelnBonn_review', 'sunshine_min_KoelnBonn_review','stationPressure_hPa_KoelnBonn_review', 'surfacePressure_hPa_KoelnBonn_review','T_temperature_C_KoelnBonn_review', 'humidity_Percent_KoelnBonn_review','wind_speed_ms_KoelnBonn_review', 'wind_direction_degree_KoelnBonn_review', 'clouds_KoelnBonn_review','T_temperature_C_Hamburg_review', 'humidity_Percent_Hamburg_review', 'stationPressure_hPa_Hamburg_review','surfacePressure_hPa_Hamburg_review', 'wind_speed_ms_Hamburg_review', 'wind_direction_degree_Hamburg_review','clouds_Hamburg_review', 'precipitationTotal_mm_Hamburg_review', 'sunshine_min_Hamburg_review', 'precipitationTotal_mm_Muenchen_review', 'sunshine_min_Muenchen_review', 'stationPressure_hPa_Muenchen_review','surfacePressure_hPa_Muenchen_review', 'T_temperature_C_Muenchen_review', 'humidity_Percent_Muenchen_review','clouds_Muenchen_review', 'wind_speed_ms_Muenchen_review', 'wind_direction_degree_Muenchen_review', 'Covid factor']\n",
    "\n",
    "    existing_columns = []\n",
    "    for col in known_covariates_columns:\n",
    "        if col in data.columns:\n",
    "            existing_columns.append(col)  \n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' is missing and will be ignored.\")\n",
    "\n",
    "    known_covariates = data[['timestamp'] + existing_columns]\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    train_data =data[(data['timestamp'] > '2015-01-04') & (data['timestamp'] < '2023-12-01')]\n",
    "    val_data = data[(data['timestamp'] >= '2023-12-01') & (data['timestamp'] < '2024-06-01')]\n",
    "    test_data = data[(data['timestamp'] >= '2024-06-01') & (data['timestamp'] < '2024-12-01')]\n",
    "\n",
    "    print(f\"Trainingsdaten bis 01.12.2023: {train_data.shape[0]} Zeilen\")\n",
    "    print(f\"Validierungsdaten bis 01.12.2023: {val_data.shape[0]} Zeilen\")\n",
    "    print(f\"Testdaten ab 01.06.2024: {test_data.shape[0]} Zeilen\")\n",
    "\n",
    "    \n",
    "    if train_data.empty or val_data.empty or test_data.empty:\n",
    "        raise ValueError(\"Eine der Datenaufteilungen (Train, Validation, Test) ist leer.\")\n",
    "\n",
    "    \n",
    "    train_data.to_csv(f\"{output_folder}/train_data.csv\", index=False)\n",
    "    val_data.to_csv(f\"{output_folder}/val_data.csv\", index=False)\n",
    "    test_data.to_csv(f\"{output_folder}/test_data.csv\", index=False)\n",
    "\n",
    "    \n",
    "    predictor = TimeSeriesPredictor(\n",
    "        target=\"day_ahead_prices_EURO\",  \n",
    "        prediction_length=24,\n",
    "                           \n",
    "        eval_metric=\"MAPE\"             \n",
    "    )\n",
    "    if set_preset=='best_quality' or set_preset=='fast_training' or set_preset=='high_quality' or set_preset=='medium_quality':\n",
    "        predictor.fit(train_data=train_data, \n",
    "                    tuning_data=val_data,            \n",
    "                    presets=set_preset,\n",
    "                    time_limit=set_time_limit\n",
    "                    )\n",
    "    elif set_preset=='hp1':\n",
    "        predictor.fit(train_data=train_data, \n",
    "                        tuning_data=val_data,\n",
    "                        time_limit=set_time_limit,            \n",
    "                        hyperparameters = {\n",
    "                            'ADIDA':{},\n",
    "                            'AutoARIMA':{}, \n",
    "                            'AutoCES':{}, \n",
    "                            'AutoETS':{}, \n",
    "                            'Croston':{}, \n",
    "                            'IMAPA':{}, \n",
    "                            'NPTS':{},                  \n",
    "                            'Chronos': {\n",
    "                                'num_epochs': 2000,        \n",
    "                                'batch_size': 64,          \n",
    "                            },\n",
    "                            'DeepAR': {\n",
    "                                'num_lstm_layers': 4,      \n",
    "                                'num_lstm_units': 256,     \n",
    "                                'dropout_rate': 0.3,       \n",
    "                                'learning_rate': 1e-4,     \n",
    "                                'num_epochs': 2000,        \n",
    "                                'batch_size': 64,          \n",
    "                            },\n",
    "                            'DirectTabular': {\n",
    "                                'num_layers': 10,           \n",
    "                                'hidden_size': 512,        \n",
    "                                'dropout_rate': 0.3,       \n",
    "                            },\n",
    "                            'DLinear': {\n",
    "                                'num_layers': 10,          \n",
    "                                'hidden_size': 512,       \n",
    "                                'dropout_rate': 0.3,       \n",
    "                            },\n",
    "                            'PatchTST': {\n",
    "                                'num_layers': 10,           \n",
    "                                'hidden_size': 512,        \n",
    "                                'dropout_rate': 0.3,       \n",
    "                            },\n",
    "                            'RecursiveTabular': {\n",
    "                                'num_layers': 10,           \n",
    "                                'hidden_size': 512,        \n",
    "                                'dropout_rate': 0.3,       \n",
    "                            },\n",
    "                            'SimpleFeedForward': {\n",
    "                                'num_layers': 10,           \n",
    "                                'hidden_size': 512,        \n",
    "                                'dropout_rate': 0.3,       \n",
    "                            },\n",
    "                            'TemporalFusionTransformer': {\n",
    "                                'num_encoder_layers': 6,  \n",
    "                                'num_decoder_layers': 6,\n",
    "                                'attention_heads': 16,\n",
    "                                'hidden_size': 512,        \n",
    "                                'dropout_rate': 0.3,       \n",
    "                            },\n",
    "                            'TiDE': {\n",
    "                                'num_layers': 10,           \n",
    "                                'hidden_size': 512,        \n",
    "                                'dropout_rate': 0.3,       \n",
    "                            },\n",
    "                            'WaveNet': {\n",
    "                                'num_layers': 8,           \n",
    "                                'hidden_size': 512,        \n",
    "                                'dropout_rate': 0.3,       \n",
    "                            }\n",
    "                        }\n",
    "            )\n",
    "    elif set_preset=='hp2':\n",
    "        predictor.fit(train_data=train_data, \n",
    "              tuning_data=val_data,\n",
    "              time_limit=set_time_limit,\n",
    "              hyperparameter_tune_kwargs=\"auto\",                \n",
    "                hyperparameters = {\n",
    "                                'ADIDA':{},\n",
    "                                'AutoARIMA':{}, \n",
    "                                'AutoCES':{}, \n",
    "                                'AutoETS':{}, \n",
    "                                'Croston':{}, \n",
    "                                'IMAPA':{}, \n",
    "                                'NPTS':{},                  \n",
    "                                'Chronos': {\n",
    "                                    'num_epochs': 2000,        \n",
    "                                    'batch_size': 64,          \n",
    "                                },\n",
    "                                'DeepAR': {\n",
    "                                    'num_lstm_layers': 4,      \n",
    "                                    'num_lstm_units': 256,     \n",
    "                                    'dropout_rate': 0.3,       \n",
    "                                    'learning_rate': 1e-4,     \n",
    "                                    'num_epochs': 2000,        \n",
    "                                    'batch_size': 64,          \n",
    "                                },\n",
    "                                'DirectTabular': {\n",
    "                                    'num_layers': 10,           \n",
    "                                    'hidden_size': 512,        \n",
    "                                    'dropout_rate': 0.3,       \n",
    "                                },\n",
    "                                'DLinear': {\n",
    "                                    'num_layers': 10,          \n",
    "                                    'hidden_size': 512,       \n",
    "                                    'dropout_rate': 0.3,       \n",
    "                                },\n",
    "                                'PatchTST': {\n",
    "                                    'num_layers': 10,           \n",
    "                                    'hidden_size': 512,        \n",
    "                                    'dropout_rate': 0.3,       \n",
    "                                },\n",
    "                                'RecursiveTabular': {\n",
    "                                    'num_layers': 10,           \n",
    "                                    'hidden_size': 512,        \n",
    "                                    'dropout_rate': 0.3,       \n",
    "                                },\n",
    "                                'SimpleFeedForward': {\n",
    "                                    'num_layers': 10,           \n",
    "                                    'hidden_size': 512,        \n",
    "                                    'dropout_rate': 0.3,       \n",
    "                                },\n",
    "                                'TemporalFusionTransformer': {\n",
    "                                        'num_encoder_layers': 6,\n",
    "                                        'num_decoder_layers': 6,\n",
    "                                        'attention_heads': 16,\n",
    "                                        'hidden_size': 512,\n",
    "                                        'dropout_rate': space.Categorical(0.1,0.3),\n",
    "                                        'max_epochs': 400,\n",
    "                                        'hidden_dim': 64,\n",
    "                                        'batch_size': 64\n",
    "                                },                                \n",
    "                                'TiDE': {\n",
    "                                    'num_layers': 10,           \n",
    "                                    'hidden_size': 512,        \n",
    "                                    'dropout_rate': 0.3,       \n",
    "                                },\n",
    "                                'WaveNet': {\n",
    "                                    'num_layers': 8,           \n",
    "                                    'hidden_size': 512,        \n",
    "                                    'dropout_rate': 0.3,       \n",
    "                                }}\n",
    "                )\n",
    "    else: \n",
    "        print('No preset found')\n",
    "    \n",
    "    #Check Feature Importance\n",
    "    try:\n",
    "        importance = predictor.feature_importance(data=val_data)\n",
    "        importance_df = pd.DataFrame(importance)\n",
    "        importance_df.to_csv(f\"{output_folder}/feature_importance.csv\", index=True)\n",
    "        print(\"Feature Importance saved.\")\n",
    "    except ValueError as e:\n",
    "        print(\"Feature importance could not be computed:\", e)\n",
    "\n",
    "    predictions_c = predictor.predict(test_data,known_covariates=known_covariates)\n",
    "    actual_values = test_data[predictor.target].iloc[-predictions_c.shape[0]:]\n",
    "    predictor.leaderboard(test_data)\n",
    "    mse = (actual_values.values - predictions_c['mean']) ** 2  # MSE for each prediction\n",
    "    mape = np.abs((actual_values.values - predictions_c['mean']) / actual_values.values) * 100  # MAPE for each prediction\n",
    "    mean_mse = np.mean(mse)\n",
    "    mean_mape = np.mean(mape)\n",
    "    print(f\"Mean MSE: {mean_mse}\")\n",
    "    print(f\"Mean MAPE: {mean_mape}\")    \n",
    "    start_time = test_data[\"timestamp\"].min()\n",
    "    end_time = test_data[\"timestamp\"].max()\n",
    "    print(f\"Starttime: {start_time}, endtime: {end_time}\")\n",
    "\n",
    "    predictions = []\n",
    "    current_time = start_time\n",
    "\n",
    "    while current_time <= end_time:\n",
    "        current_day_data = data[            \n",
    "            (data[\"timestamp\"] < current_time)\n",
    "        ]\n",
    "        #(data[\"timestamp\"] >= current_time - relativedelta(months=6)) &\n",
    "        \n",
    "        if not current_day_data.empty:\n",
    "            predicted_values = predictor.predict(current_day_data, known_covariates=known_covariates)\n",
    "            print(f\"Predicted Values: {predicted_values}\")            \n",
    "            if \"mean\" in predicted_values:\n",
    "                mean_predicted_values = predicted_values[\"mean\"].values\n",
    "            else:\n",
    "                raise ValueError(\"'mean' column not found in predictions\")\n",
    "            next_24_hours = [current_time + timedelta(hours=i) for i in range(24)]\n",
    "            print(f\"Next 24 timestamps: {next_24_hours}\")            \n",
    "            if len(mean_predicted_values) != len(next_24_hours):\n",
    "                raise ValueError(\"Mismatch between predicted values and generated timestamps\")        \n",
    "            predictions.extend([\n",
    "                {\"timestamp\": ts, \"predicted\": pred}\n",
    "                for ts, pred in zip(next_24_hours, mean_predicted_values)\n",
    "            ])        \n",
    "        current_time += timedelta(days=1)\n",
    "\n",
    "    if predictions:\n",
    "        final_predictions = pd.DataFrame(predictions)\n",
    "        final_predictions[\"timestamp\"] = pd.to_datetime(final_predictions[\"timestamp\"])  # Sicherstellen, dass der Timestamp korrekt ist\n",
    "        final_predictions.to_csv(f\"{output_folder}/results_data.csv\", index=False)\n",
    "    else:\n",
    "        print(\"Keine Vorhersagen konnten gemacht werden.\")\n",
    "        final_predictions = pd.DataFrame(columns=[\"timestamp\", \"predicted\"])\n",
    "\n",
    "    test_data_comparison = test_data[['timestamp', 'day_ahead_prices_EURO']].rename(columns={'day_ahead_prices_EURO': 'actual_price'})\n",
    "    test_data_comparison['timestamp'] = pd.to_datetime(test_data_comparison['timestamp'])\n",
    "\n",
    "    comparison = pd.merge(test_data_comparison, final_predictions, on='timestamp', how='outer')\n",
    "    comparison.to_csv(f\"{output_folder}/comparison.csv\", index=False)\n",
    "\n",
    "\n",
    "    y_true = comparison[\"actual_price\"]\n",
    "    y_pred = comparison[\"predicted\"]\n",
    "    mse_sklearn = mean_squared_error(y_true, y_pred)\n",
    "    mape_sklearn = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    mae_sklearn= mae_sklearn = mean_absolute_error(y_true, y_pred)\n",
    "    r2_sklearn = r2_score(y_true, y_pred)\n",
    "    medae_sklearn = median_absolute_error(y_true, y_pred)\n",
    "    evs_sklearn = explained_variance_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE): {mape}\")\n",
    "\n",
    "    print(f\"Alle Ergebnisse wurden im Ordner '{output_folder}' gespeichert.\")\n",
    "    duration_skript=  time.time() - start_time_script\n",
    "    print(f\"Dauer: {duration_skript}\")\n",
    "    filename_mape = f\"MSE{mean_mse:.2f}MAPE{mean_mape:.2f}.txt\"\n",
    "    file_path_mape = os.path.join(output_folder, filename_mape)\n",
    "    with open(file_path_mape, \"w\") as file:\n",
    "        file.write(f\"Mean MSE: {mean_mse}\\n\")\n",
    "        file.write(f\"Mean MAPE: {mean_mape}\\n\")\n",
    "        file.write(f\"Mean MSE SKlearn: {mse_sklearn}\\n\")\n",
    "        file.write(f\"Mean MAPE Sklearn: {mape_sklearn}\\n\")\n",
    "        file.write(f\"Mean MAE SKlearn: {mae_sklearn}\\n\")\n",
    "        file.write(f\"Mean R2 SKlearn: {r2_sklearn}\\n\")\n",
    "        file.write(f\"Mean MEDAE Sklearn: {medae_sklearn}\\n\")\n",
    "        file.write(f\"Mean EVS Sklearn: {evs_sklearn}\\n\")\n",
    "        file.write(f\"Modelpreset: {set_preset}\\n\")\n",
    "        file.write(f\"Dataset: {dataset}\\n\")\n",
    "        file.write(f\"Loaded Path {file_path}\")\n",
    "        file.write(f\"Time Limit {set_time_limit}\")\n",
    "        file.write(f\"Duration: {duration_skript}\")\n",
    "    print(f\"Ergebnisse wurden in die Datei '{file_path_mape}' gespeichert.\")\n",
    "\n",
    "    data = {\n",
    "        \"Mean MSE\": mean_mse,\n",
    "        \"Mean MAPE\": mean_mape,\n",
    "        \"Modelpreset\": set_preset,\n",
    "        \"Date\":script_start_time,\n",
    "        \"Dataset\": dataset,\n",
    "        \"Loaded Path\": file_path,\n",
    "        \"Duration\": duration_skript,\n",
    "        \"Loaded Dataframe\": loaded_dataframe,\n",
    "        \"MSE SKlearn:\": mse_sklearn,\n",
    "        \"MAPE Sklearn\": mape_sklearn,\n",
    "        \"MAE SKlearn\": mae_sklearn,\n",
    "        \"R2 SKlearn\": r2_sklearn,\n",
    "        \"MEDAE Sklearn\": medae_sklearn,\n",
    "        \"EVS Sklearn\": evs_sklearn,\n",
    "        \"Timelimit\": set_time_limit\n",
    "    }\n",
    "\n",
    "    #write to csv\n",
    "    file_path_csv ='/data/horse/ws/fewa833b-time-series-forecast/AutoGluon/Felix/AG-Preset/preset-comparison.csv'\n",
    "    file_exists = os.path.isfile(file_path_csv)\n",
    "    with open(file_path_csv, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=data.keys())\n",
    "        if not file_exists:\n",
    "            #set header if new file\n",
    "            writer.writeheader()\n",
    "        #write data\n",
    "        writer.writerow(data)\n",
    "    print(f\"Ergebnisse wurden in die Datei '{file_path_csv}' gespeichert.\")\n",
    "\n",
    "#only if sheduled directly\n",
    "if __name__ == \"__main__\":\n",
    "    set_preset='best_quality'\n",
    "    set_time_limit=300\n",
    "    dataset='all'\n",
    "    train_autogluon(set_preset,set_time_limit,dataset)  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning AutoGluon\n",
    "\n",
    "In the AutoGluon group, an attempt was made to optimise the prediction with AutoGluon by using different settings. The aim was to evaluate the performance of different approaches and to analyse the importance of additional data and external influencing factors. For this purpose, different AutoGluon presets were used, including best_quality, fast_training, high_quality, medium_quality and two customised configurations with user-defined hyperparameters (hp1 and hp2).\n",
    "The models were trained on two distinct datasets:\n",
    "1. allData.csv: A comprehensive dataset with over 150 data points per hour, providing a detailed basis for more accurate predictions.\n",
    "2. lessData.csv: A reduced data set with around 30 data points per hour, which represents whether the model can also make good predictions with less data\n",
    "\n",
    "As part of the investigation of the effects of external factors on the prediction quality, the training runs were carried out in two variants. In the first variant, known covariates such as the day (e.g. weekday or public holiday) and weather conditions (e.g. temperature or precipitation) were included in the modelling. In the second variant, on the other hand, only the time stamps and the target value were used to optimise the models.\n",
    "\n",
    "To evaluate the effectiveness of the different approaches, a simple baseline was used as a reference point, which only used the time and the day-ahead price as input data. The aim was to determine the potential improvements in forecast accuracy through additional data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Read the CSV file into a pandas DataFrame\n",
    "file_path = \"C:/Users/Nudre/Desktop/VSC safe/preset-comparison.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "#Check for missing columns\n",
    "model_columns = ['Modelpreset', 'MSE SKlearn:', 'MAPE Sklearn', 'R2 SKlearn', 'MEDAE Sklearn', 'EVS Sklearn']\n",
    "existing_columns = [col for col in model_columns if col in df.columns]\n",
    "df_models = df[existing_columns]\n",
    "metrics = ['MSE SKlearn:', 'MAPE Sklearn', 'R2 SKlearn', 'MEDAE Sklearn', 'EVS Sklearn']\n",
    "\n",
    "#calculate the number of rows and columns based\n",
    "num_metrics = len(metrics)\n",
    "num_rows = (num_metrics + 2) // 3  \n",
    "num_cols = min(3, num_metrics)  \n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(18, num_rows * 5))\n",
    "if num_metrics == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "#Create the individual plots\n",
    "for i, metric in enumerate(metrics):\n",
    "    if metric in df_models.columns: \n",
    "        ax = axes[i // num_cols, i % num_cols]\n",
    "        sns.barplot(x='Modelpreset', y=metric, data=df_models, ax=ax, hue='Modelpreset', palette='Set2', legend=False)\n",
    "        ax.set_title(f'{metric}')\n",
    "        ax.set_xlabel('Modelpreset')\n",
    "        ax.set_ylabel(metric)\n",
    "\n",
    "#Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, an analysis was conducted to determine the extent to which the selection of presets affects the computing time and the outcomes obtained. The investigation focused on whether less complex presets, such as fast_training, could achieve results in a significantly shorter time while maintaining a marginally lower level of accuracy under specific conditions. This trade-off between computing time and model performance is of particular relevance when models are updated with additional data at a later stage to enhance resource efficiency. However, this hypothesis could not be substantiated within the scope of the present study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Read the CSV file into a pandas DataFrame\n",
    "file_path = \"C:/Users/Nudre/Desktop/VSC safe/preset-comparison.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#Ensure Mean MAPE is treated as a numeric value (if it was read as a string, remove '%')\n",
    "if df['Mean MAPE'].dtype == object:  #If the column is of object type (strings)\n",
    "    df['Mean MAPE'] = df['Mean MAPE'].str.replace('%', '').astype(float)\n",
    "\n",
    "#Set y- and x-axis\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "x_positions = np.arange(len(df))\n",
    "ax1.bar(x_positions - 0.2, df['Duration'], color='orange', label='Duration (s)', width=0.4, alpha=0.7)\n",
    "ax1.set_xlabel('Preset')\n",
    "ax1.set_ylabel('Duration (seconds)', color='orange')\n",
    "ax1.tick_params(axis='y', labelcolor='orange')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(x_positions + 0.2, df['Mean MAPE'], color='blue', label='Mean MAPE', width=0.4, alpha=0.7)\n",
    "ax2.set_ylabel('Mean MAPE', color='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "#Set the limits for the Mean MAPE axis to start from 0\n",
    "ax2.set_ylim(0, df['Mean MAPE'].max() + 5)\n",
    "\n",
    "#Set title and labels\n",
    "ax1.set_xticks(x_positions)\n",
    "ax1.set_xticklabels(df['Modelpreset'] + \"_\" + df['Dataset'], rotation=45, ha=\"right\")\n",
    "\n",
    "plt.title('Duration and Mean MAPE by Preset')\n",
    "fig.tight_layout()\n",
    "\n",
    "#Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Innovation / Hacking AutoGluon\n",
    "\n",
    "#### Leveraging News Embeddings\n",
    "\n",
    "Hypothesis:\n",
    "In the dynamic landscape of Germany's energy market, predicting day-ahead electricity prices is like solving a complex puzzle where each piece represents a different market influence. There's a wealth of information hidden in news articles that could provide crucial additional context for these predictions. With the help of embedding models we hope to utilize this information to improve the prediction of day-ahead-prices in the german energy market.\n",
    "\n",
    "Related Work:\n",
    "It has been shown that the inclusion of news embeddings in the training material increases the performance of predictive models for financial markets [Picasso et al. 2019]. Furthermore, similar techniques yielded promising results for the predicting of the day-ahead-energy prices in the UK [Yun Bai et al. 2024]. \n",
    "\n",
    "Approach:\n",
    "Day-ahead prices can be volatile and influenced by numerous factors that are often reported in news media before they impact prices:\n",
    "\n",
    "- Unexpected power plant outages\n",
    "- Changes in renewable energy policies\n",
    "- International energy market developments\n",
    "- Grid infrastructure updates\n",
    "- Geopolitical events affecting energy supply\n",
    "- Market sentiment and trader behavior\n",
    "\n",
    "Because there are hundreds of relevant articles each day, surmounting to an ungainly amount of unstructured information, we propose generating embeddings of these news.\n",
    "This not only allows us to distill the information into more structured and smaller vectors, but promises also to offer more easily extractible information as each embedding vector corresponds to a certain kind of information value. For example there might be a dimension pertaining to the impact of the news to the financial and energy market, and another to the likelihood of large energy consumption due to social events. This way the model can learn to give more importance to these potentially higher correlated dimensions.\n",
    "First we identify a suitable embedding model, which is suitable for our experiment. Secondly, we identify potential data sources and evaluate them based on their availibility, openness and finally how well their data allows us to predict the target. For this we test on a small dataset of 30 days. Thirdly, we create news embeddings from the last 10 years, to learn a Deep Learning Model and compare its performance to a baseline model.\n",
    "\n",
    "Embedding Model:\n",
    "We identified the \"all-MiniLM-L6-v2\" embedding model as a suitable candidate for this experiment [https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2]. It runs fast and thus suitable for fast experimentation, creating a 384-dimensial vector for each input text. Using PCA (Principal Component Analysis) we further reduced this dimensionality to 8, for faster experimentation. In the future a comparison of different embedding models, with different dimensionalities would be interesting, or even an embedding model finetuned on the downstream task of predicting energy prices.\n",
    "\n",
    "Data Sources:\n",
    "When identifying news sources, we had to ensure the following\n",
    "- API needs to be stable \n",
    "- Historical news articles need to be accessible back to 01.01.15\n",
    "- API needs to be affordable for the purposes of the student project\n",
    "- news articles need to be relevant\n",
    "\n",
    "First we identified NewsAPI (https://newsapi.org/) as a reliable and structured daily news source, offering a wide range of different news sources, which can be queried by keywords. Unfortunately they only offer 30 days of free historical data, and their paid plan is quite expensive. Searching through this list of curated news-apis (https://github.com/public-apis/public-apis?tab=readme-ov-file#news) however, yielded another comoprehensive news source, the guardian news api (https://open-platform.theguardian.com/). This european trust and donation based newspaper offers its extensive global news free of charge.\n",
    "We queried these news sources with \"energy OR electricity OR power market OR renewable\", and embedded the resulting articles with our chosen embedding model. In case there were multiple articles for the same hour, we created embeddings for each of them and took the mean, similar to [Picasso et al. 2019]. Next, we forward filled the embeddings to have no NaN values. Finally we appended the day ahead prices and the target. A third dataset consists only of the day ahead prices and the target, to offer a baseline to compare to. This resulted in 3 datasets. We trained the models on 30 days, as this was the maximum range of data we could fetch from NewsAPI.\n",
    "\n",
    "30 day datasets:\n",
    "- newsAPI embeddings + day-ahead + target -> RMSE: 28.82\n",
    "- Guardian embeddings + day-ahead + target -> RMSE: 27.36\n",
    "- day-ahead + target (baseline) -> RMSE: 27.86\n",
    "\n",
    "The is only a small mean difference between the results, which is probably due to the fact that the guardian and newsAPI articles hold little overall relevance regarding the german energy market.\n",
    "When looking at qualitative samples (prediction on 1 day only), we can see very localized performance increases in the embedding enriched models, indicating that the embeddings offer significant information gain when there were relevant articles available.\n",
    "\n",
    "Future experimentation into this interesting approach, especially regarding the curation of more relevant news sources, could yield more significant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspection of embeddings\n",
    "\n",
    "guardian_embeddings = pd.read_csv('merged_data/data_collection/guardian_embeddings.csv', parse_dates=['timestamp'])\n",
    "newsapi_embeddings = pd.read_csv('merged_data/data_collection/newsapi_embeddings.csv', parse_dates=['timestamp'])\n",
    "day_ahead_prices = pd.read_csv('merged_data/data_collection/day_ahead_prices.csv', parse_dates=['timestamp'])\n",
    "\n",
    "print(\"Guardian embeddings:\")\n",
    "print(guardian_embeddings[10:20])\n",
    "\n",
    "print(\"NewsApi embeddings:\")\n",
    "print(newsapi_embeddings[10:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference\n",
    "\n",
    "guardian_embeddings_day_ahead, newsapi_embeddings_day_ahead, day_ahead_prices_for_prediction, ground_truth = prepare_embedding_data(guardian_embeddings, newsapi_embeddings, day_ahead_prices)\n",
    "\n",
    "guardian_predictor_path =  \"models/models/guardian_embedding_model\"\n",
    "newsapi_predictor_path = \"models/models/newsapi_embedding_model\"\n",
    "no_embeddings_predictor_path = \"models/models/no_embeddings_model\"\n",
    "\n",
    "guardian_predictions = predict_embedding_data(guardian_predictor_path, guardian_embeddings_day_ahead)\n",
    "newsapi_predictions = predict_embedding_data(newsapi_predictor_path, newsapi_embeddings_day_ahead)\n",
    "no_embeddings_predictions = predict_embedding_data(no_embeddings_predictor_path, day_ahead_prices_for_prediction)\n",
    "\n",
    "visualize_embedding_model_results(guardian_predictions, ground_truth, \"Guardian Embeddings\")\n",
    "visualize_embedding_model_results(newsapi_predictions, ground_truth, \"NewsAPI Embedddings\")\n",
    "visualize_embedding_model_results(no_embeddings_predictions, ground_truth, \"No Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Intraday-Prices as additional feature\n",
    "\n",
    "Besides day-ahead prices, which are the target value of this time series forecast, electric energy is also traded on the more short-term Intraday market.\n",
    "While in general a high correlation of these prices can be assumed, as they refer to the same underlying product, there are cases in which the prices of day-ahead and intraday trades differ significantly, as the intraday market will react more short-term to unforeseen changes in supply and demand.\n",
    "These short-term differences can for example be induced by\n",
    "- extreme weather conditions offsetting forecasted and actual generation of renewables affecting supply\n",
    "- extreme weather conditions generating an unforeseen surge or decline in demand\n",
    "- unplanned power plant outages creating a lack of supply\n",
    "- short-term changes in balancing affecting supply\n",
    "\n",
    "As Germany saw their share of renewables in power generation rise, the role of intraday prices for demand regulation became increasingly important.\n",
    "Therefore the \"gate closure\" of intraday trades, meaning the time between end of trading and delivery, got shortened significantly.\n",
    "In July 2015 it was changed from 45 to 30 minutes, whereas nowadays it's down to 5 minutes, and allows for a high reactivity to changes in supply and demand. [Smard, Grohandelspreise]\n",
    "\n",
    "Given our time series dataset with all features that resulted from data gathering, we tried to predict the intraday price to use as an additional feature in predicting the day-ahead price.\n",
    "If we were to predict the intraday prices with high accuracy, this could possibly be an important feature for improving the day-ahead prediction.\n",
    "\n",
    "Data Sources:\n",
    "The open data sources for intraday electric energy prices are unfortunately very limited, as in many cases stock market data are sold for commercial data analysis.\n",
    "\"Netztransparenz\", which is an information transparency platform by the four German transmission grid operators, publishes a dataset with quarter-hourly intraday price indices from 30th June 2020 to date.\n",
    "These values are calculated as volume-weighted average of the last stock market orders over 500MW for quarter-hourly electricity.\n",
    "The dataset of Netztransparenz appears to be the only freely available dataset for German intraday electricity prices.\n",
    "\n",
    "\n",
    "#### TODO\n",
    "\n",
    "To investigate the potential of using intraday prices as an additional feature for day-ahead price prediction, a systematic approach was followed, involving the prediction of intraday prices and their integration into the day-ahead prediction.\n",
    "\n",
    "All models were consistently trained and evaluated on the allData.csv and lessData.csv datasets. Each configuration of AutoGluon presets (fast_training, medium_quality, high_quality, best_quality) was used to ensure a robust evaluation of performance under varying computational and accuracy constraints.\n",
    "\n",
    "To maintain consistency with the day-ahead price data and simplify analysis, the quarter-hourly intraday price data was aggregated into hourly values. This was achieved by calculating the mean of the four quarter-hourly values for each hour, resulting in a manageable dataset with hourly intraday prices that could be used for model training and prediction.\n",
    "\n",
    "After that a baseline model was developed on existing datasets and model presets to establish a reference point for evaluating and comparing the results of integrating predicted intraday prices into the day-ahead price forecast.\n",
    "\n",
    "Next, to demonstrate the potential improvement in day-ahead price prediction accuracy, historical intraday prices were incorporated into the baseline model as known covariates. This step simulated a scenario where intraday prices were perfectly predicted and used to enhance the day-ahead forecast.\n",
    "\n",
    "\n",
    "TODO: add plots\n",
    "\n",
    "The two bar charts compare the Mean Absolute Percentage Error (MAPE) across different AutoGluon model presets for two datasets: one with comprehensive features (allData.csv) and one with reduced features (lessData.csv). In both cases, the purple bars represent the baseline models, while the yellow bars show the results when historical intraday prices are included as a known covariate. For the larger dataset, including intraday prices consistently improves prediction accuracy across all presets, with the most significant reductions in MAPE observed for the medium_quality, high_quality, and best_quality presets, demonstrating the value of the additional feature. In contrast, for the reduced dataset, the benefits of including intraday prices are less pronounced. The fast_training preset shows no improvement, and the high_quality preset even experiences a slight increase in MAPE. However, small gains are observed for the medium_quality and best_quality presets. Overall, the inclusion of historical intraday prices proves more impactful for the larger dataset.\n",
    "\n",
    "Building on the observed improvement in day-ahead prediction performance when using historical intraday prices, the next step was to predict intraday prices for the same time intervals as those targeted in the day-ahead price forecast. Various models and train-validation-test splits were explored, with the best results achieved using the best_quality AutoGluon preset. This configuration resulted in a mean Mean Squared Error (MSE) of 93.65 and a mean Mean Absolute Percentage Error (MAPE) of 9.85 for intraday price prediction, demonstrating a reasonable level of accuracy in capturing short-term price dynamics.\n",
    "\n",
    "TODO: add plots comparing results using different model presets \n",
    "\n",
    "To assess the value of these predicted intraday prices, they were integrated as known covariates into the day-ahead price prediction models. Using the medium_quality preset and the allData.csv dataset, the day-ahead prediction model achieved a MAPE of 6.06 when incorporating the predicted intraday prices described above as input features. This result highlights that even predicted intraday prices, despite not being perfectly accurate, can still significantly enhance the day-ahead forecasting accuracy, emphasizing their potential as a valuable feature in energy price prediction tasks.\n",
    "\n",
    "TODO: add plots comparing results using different model presets \n",
    "TODO: add plot displaying forecast performance in a specific time interval\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Download Up to date Data\n",
    "# TODO Load best model from disk\n",
    "# TODO Compute Forecast\n",
    "# TODO Write to File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Guidance/Planning/Leadership\n",
    "\n",
    "Team Structure and Initial Organization:\n",
    "The BTW25 student challenge presented a unique opportunity to work in a large-scale collaborative environment with 12 team members. To effectively manage this substantial team size, we implemented a strategic division into three specialized groups, each focusing on critical aspects of the project:\n",
    "\n",
    "- XAI and Data Preparation Group: Focused on explainable AI approaches and foundational data analysis\n",
    "- AutoGluon and Preprocessing Group: Concentrated on implementing solutions using AutoGluon and gathering data\n",
    "- Deep Learning Models Group: Dedicated to developing complex neural network architectures\n",
    "\n",
    "Challenges of Interdependent Workflows:\n",
    "One of the primary challenges we faced was managing the interdependencies between these groups. Each team's output served as essential input for others, creating a complex web of dependencies. For example, the data fetching and preparation team's work directly impacted both the XAIs ability to explore the data and the deep learning team's capacity to develop their architectures. This interconnected structure meant that delays or changes in one group could create ripple effects throughout the entire project.\n",
    "\n",
    "Cross-Team Communication and Coordination:\n",
    "To address these challenges, we implemented an \"ambassador\" system. Members of the AutoGluon team were designated as liaisons to other groups, attending their meetings and serving as communication bridges. This approach proved invaluable for several reasons:\n",
    "\n",
    "- Real-time awareness of progress and challenges across all teams\n",
    "- Immediate feedback on compatibility issues between different components\n",
    "- Rapid dissemination of important updates or changes\n",
    "- Prevention of duplicate efforts across groups\n",
    "\n",
    "Standardization and Technical Integration:\n",
    "A crucial aspect of our success was the implementation of strict technical standards:\n",
    "\n",
    "Data Format Guidelines:\n",
    "\n",
    "- Standardized CSV formats for time series data\n",
    "- Consistent datetime formatting across all datasets\n",
    "- Uniform naming conventions for features and target variables\n",
    "- Desired range of data\n",
    "- Clear separation of train/val/test splits\n",
    "\n",
    "Model Wrapper Standardization:\n",
    "\n",
    "- Common interface for all models regardless of underlying implementation\n",
    "- Standardized prediction output formats\n",
    "- Unified evaluation metrics and reporting structures\n",
    "\n",
    "Effective Project Management Practices\n",
    "To maintain coherence across the large team, we established several key management practices:\n",
    "\n",
    "- Regular all-hands meetings for high-level coordination\n",
    "- Dedicated communication channels for each subgroup\n",
    "\n",
    "Learning Outcomes and Best Practices\n",
    "This experience provided valuable insights into managing large-scale ML projects:\n",
    "\n",
    "- The importance of clear communication channels and protocols\n",
    "- The value of standardized interfaces between different components\n",
    "- The effectiveness of cross-team ambassadors in maintaining project coherence\n",
    "- The necessity of flexible yet structured organization in academic projects\n",
    "\n",
    "Impact on Project Success\n",
    "These organizational strategies significantly contributed to our project's success by:\n",
    "\n",
    "- Minimizing integration issues between different components\n",
    "- Reducing redundant work across teams\n",
    "- Enabling rapid problem identification and resolution\n",
    "- Fostering knowledge sharing across specialization boundaries\n",
    "- Creating a cohesive final product despite the complexity of multiple approaches\n",
    "\n",
    "The experience demonstrated that effective organization and communication structures are as crucial to project success as technical expertise, particularly in large-scale academic collaborations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "|                                          |                                                                                                                                                                                                                                                       |\n",
    "|------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| [Yun Bai et al. 2024]                    | Yun Bai et al., 2024: News and Load: A Quantitative Exploration of Natural Language Processing Applications for Forecasting Day-Ahead Electricity System Demand                                                                                       |\n",
    "| [Bosch et al. 2023]                      | Bosch, S., Schlenker, F., Bohn, J., Kupies, S., & Schmidt, M. (2023). DeutschlandPionierland der Energiewende. In Energie-Weltatlas: Transformation des Energiesystems in globaler Perspektive (pp. 81-91). Wiesbaden: Springer Fachmedien Wiesbaden |\n",
    "| [Contreras et al. 2003]                   | Contreras, J., Espinola, R., Nogales, F. J., & Conejo, A. J. (2003). ARIMA models to predict next-day electricity prices. IEEE transactions on power systems, 18(3), 1014-1020                                                                        |\n",
    "| [Dumancic 2024]                          | Dumancic, M. (2024, March). Marktmacht in der Stromwirtschaft: Mehr Wettbewerb durch Zukunftstechnologien?. In Kartellrecht und Zukunftstechnologien (pp. 105-128). Nomos Verlagsgesellschaft mbH & Co. KG.                                           |\n",
    "| [Hein et al. 2020]                       | Hein, F., & Hermann, H. (2020). AgorameterDokumentation. Agora Energiewende: Berlin, Germany.                                                                                                                                                        |\n",
    "| [Lago et al. 2018]                       | Lago, J., De Ridder, F., & De Schutter, B. (2018). Forecasting spot electricity prices: Deep learning approaches and empirical comparison of traditional algorithms. Applied Energy, 221, 386-405.                                                    |\n",
    "| [Nestle et al. 2009]                     | Nestle, D., Ringelstein, J., & Selzam, P. (2009). Integration dezentraler und erneuerbarer Energien durch variable Strompreise im liberalisierten Energiemarkt. uwf UmweltWirtschaftsForum, 17, 361-365.                                              |\n",
    "| [Niedermeier 2023]                       | Niedermeier, T. (2023). Auswertung der Stromerzeugung in Deutschland von 20152022 und Abgleich mit den Ausbauzielen des EEG 2023 (Doctoral dissertation, Hochschule fr angewandte Wissenschaften Mnchen).                                          |\n",
    "| [Nunes et al. 2008]                      | Nunes, C., Pacheco, A., & Silva, T. (2008, May). Statistical models to predict electricity prices. In 2008 5th International Conference on the European Electricity Market (pp. 1-6). IEEE.                                                           |\n",
    "| [Ortner and Totschnig 2019]              | Ortner, A., & Totschnig, G. (2019). The future relevance of electricity balancing markets in Europe-A 2030 case study. Energy Strategy Reviews, 24, 111-120.                                                                                          |\n",
    "| [Picasso et al. 2019]                    | Picasso et al., 2019: Technical analysis and sentiment embeddings for market trend prediction                                                                                                                                                         |\n",
    "| [Schumacher et al. 2015]                 | Schumacher, Ingrid, et al. \"Der strommarkt und die strompreisbildung.\" Strategien zur Strombeschaffung in Unternehmen: Energieeinkauf optimieren, Kosten senken (2015): 9-37.                                                                         |\n",
    "| [Tschora et al. 2022]                    | Tschora, L., Pierre, E., Plantevit, M., & Robardet, C. (2022). Electricity price forecasting on the day-ahead market using machine learning. Applied Energy, 313, 118752.                                                                             |\n",
    "| [Smard, Grohandelspreise]               | Smard.de, Bundesnetzagentur, 15.12.2024, URL: https://www.smard.de/page/home/wiki-article/446/562                                                                                                                                                     |\n",
    "| [Netztransparenz, Index-Ausgleichspreis] | Netztransparenz.de, 50Hertz Transmission GmbH; Amprion GmbH; TenneT TSO GmbH; TransnetBW GmbH, 16.12.2024, URL: https://www.netztransparenz.de/de-de/Regelenergie/Ausgleichsenergiepreis/Index-Ausgleichsenergiepreis                                 |\n",
    "| TODO wo wird das zitiert ???             | https://www.researchgate.netpublication/348111996_A_Review_on_Linear_Regression_Comprehensive_in_Machine_Learning -> DOI: http://dx.doi.org/10.38094/jastt1457                                                                                        |\n",
    "| TODO wo wird das zitiert ???             | comparison of three time series forecasting methods (including linear regression) ->DOI: 10.47738/ijiis.v6i2.165                                                                                                                                      |\n",
    "| TODO wo wird das zitiert ???             | https://hummedia.manchester.ac.uk/institutes/cmist/archive-publications/working-papers/2008/2008-19-multiple-linear-regression.pdf                                                                                                                    |\n",
    "| [Ansari et al. 2024]                     | Abdul Fatir Ansari et al. 2024: Chronos: Learning the Language of Time Series https://doi.org/10.48550/arXiv.2403.07815                                                                                                                               |\n",
    "| [Steinmetz et al. 2022]                  | Steinmetz, H., Batzdorfer, V., Scherhag, J., & Bosnjak, M. (2022). The ZPID Lockdown Measures Dataset for Germany [Data set]. PsychArchives. https://doi.org/10.23668/PSYCHARCHIVES.6676                                                              |"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeseries-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
