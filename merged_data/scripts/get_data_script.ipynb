{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is responsible for getting stock market data, it is done by crawling the website yahoo finance and getting the data for our desired time frame. The chosen resources are Coal, Brent Oil and Natural Gas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# gets data from yahoo finance with the given url, the filename and resource have to be put in\n",
    "def get_Data(url, filename, resource):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table')\n",
    "\n",
    "    # Extract headers\n",
    "    headers = [header.text.strip() for header in table.find_all('th')]\n",
    "\n",
    "    # Close column = Value for end of the day, the rest of the columns are not needed\n",
    "    close_column_index = next(i for i, header in enumerate(headers) if header.lower().startswith('close'))\n",
    "\n",
    "    # Extract data from the table\n",
    "    data = []\n",
    "    for row in table.find_all('tr'):\n",
    "        columns = row.find_all('td')\n",
    "        if columns:\n",
    "            # Get correcct Date format\n",
    "            date = columns[0].text.strip()\n",
    "            date = datetime.strptime(date, '%b %d, %Y').strftime('%Y-%m-%d')\n",
    "            close_value = columns[close_column_index].text.strip()\n",
    "            data.append([date, close_value])\n",
    "\n",
    "\n",
    "    # data is in the wrong order, put it from earliest to latest\n",
    "    data.sort(key=lambda x: datetime.strptime(x[0], '%Y-%m-%d'))\n",
    "\n",
    "    # Save the data with headers as a CSV file\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', resource])\n",
    "        writer.writerows(data)\n",
    "\n",
    "    print(\"Data saved as \",filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved as  ../data_collection/brent_oil.csv\n",
      "Data saved as  ../data_collection/naturalGas.csv\n",
      "Data saved as  ../data_collection/coal.csv\n"
     ]
    }
   ],
   "source": [
    "url_brent = 'https://finance.yahoo.com/quote/BZ%3DF/history/?period1=1420070400&period2=1734903292&guccounter=1'\n",
    "url_gas = 'https://finance.yahoo.com/quote/NG%3DF/history/?period1=1420070400&period2=1734905333'\n",
    "url_coal = 'https://finance.yahoo.com/quote/MTFZ24.NYM/history/?period1=1420070400&period2=1734905597'\n",
    "\n",
    "get_Data(url_brent, '../data_collection/brent_oil.csv', 'Brent Oil')\n",
    "get_Data(url_gas, '../data_collection/naturalGas.csv', 'Natural Gas')\n",
    "get_Data(url_coal, '../data_collection/coal.csv', 'Coal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3023880499.py:15: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  full_hourly_range = pd.date_range(start='01.01.2015', end=df.index.max() + pd.Timedelta(days=1), freq='H')[:-1]\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3023880499.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_full[value_Name].fillna('', inplace=True)\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3023880499.py:24: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_full[value_Name].fillna('', inplace=True)\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3023880499.py:15: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  full_hourly_range = pd.date_range(start='01.01.2015', end=df.index.max() + pd.Timedelta(days=1), freq='H')[:-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Hours Filled:  ../data_collection/brent_oil.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3023880499.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_full[value_Name].fillna('', inplace=True)\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3023880499.py:24: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_full[value_Name].fillna('', inplace=True)\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3023880499.py:15: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  full_hourly_range = pd.date_range(start='01.01.2015', end=df.index.max() + pd.Timedelta(days=1), freq='H')[:-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Hours Filled:  ../data_collection/naturalGas.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3023880499.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_full[value_Name].fillna('', inplace=True)\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3023880499.py:24: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_full[value_Name].fillna('', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Hours Filled:  ../data_collection/coal.csv\n",
      "CSV files have been merged and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#the data is missing hour, as it is only daily, fills weekend gaps also\n",
    "def fill_missing_hours(csv):\n",
    "    df = pd.read_csv(csv)\n",
    "\n",
    "    value_Name = df.columns[1]\n",
    "\n",
    "    # Manually parse the 'date' column using the correct format (DD.MM.YY)\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "    df['Date'] = df['Date'].dt.normalize()\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # start 2015\n",
    "    full_hourly_range = pd.date_range(start='01.01.2015', end=df.index.max() + pd.Timedelta(days=1), freq='H')[:-1]\n",
    "\n",
    "    # put prefered null value here\n",
    "    df_full = df.reindex(full_hourly_range, fill_value=pd.NA)\n",
    "    df_full.reset_index(inplace=True)\n",
    "    df_full.rename(columns={'index': 'Date'}, inplace=True)\n",
    "    df_full[value_Name] = df_full.groupby(df_full['Date'].dt.floor('D'))[value_Name].transform(lambda group: group.ffill().bfill())\n",
    "\n",
    "    # fills emptys\n",
    "    df_full[value_Name].fillna('', inplace=True)\n",
    "    df_full.to_csv(csv, index=False)\n",
    "    print('Missing Hours Filled: ', csv)\n",
    "\n",
    "\n",
    "fill_missing_hours('../data_collection/brent_oil.csv')\n",
    "fill_missing_hours('../data_collection/naturalGas.csv')\n",
    "fill_missing_hours('../data_collection/coal.csv')\n",
    "\n",
    "df1 = pd.read_csv('../data_collection/brent_oil.csv')\n",
    "df2 = pd.read_csv('../data_collection/naturalGas.csv')\n",
    "df3 = pd.read_csv('../data_collection/coal.csv')\n",
    "\n",
    "merged_df = pd.merge(df1, df2, on='Date', how='outer')\n",
    "merged_df = pd.merge(merged_df, df3, on='Date', how='outer')\n",
    "\n",
    "merged_df.to_csv('../data_collection/merged_data.csv', index=False)\n",
    "\n",
    "print(\"CSV files have been merged and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is responsible for getting the ENTSO-DATA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ENTSO-E (European Network of Transmission System Operators for Electricity) Transparency Platform, launched in 2015, serves as Europe's central hub for electricity market data. Created to fulfill EU Regulation 543/2013, it collects and publishes data from over 42 transmission system operators across Europe. The platform transformed what was once a fragmented landscape of national data sources into a unified repository, providing crucial information about power generation, consumption, and cross-border flows. For market participants and analysts, this data source is invaluable as it offers insights into the fundamental drivers of electricity prices, including generation mix, grid constraints, and demand patterns.\n",
    "\n",
    "![Example Image](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRv8tncnUyen1EF-uNSqu9D2iw7FSClbohI4KQBGH2XPhI_7ve-W9BXXI5-vluG2t9FCLQ&usqp=CAU)\n",
    "\n",
    "We applied for an API-Key and programmatically extract relevant information in the following.\n",
    "\n",
    "We extracted day-ahead prices, generation mix, and cross-border flows for the year 2020. We also queried some predictions, including the generation mix for the next week, which might hint to the future day-ahead prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts\n",
      "Day-ahead prices done\n",
      "Load forecast done\n",
      "Generation forecast done\n",
      "Intraday wind and solar forecast done\n",
      "Day ahead wind and solar forecast done\n",
      "Physical crossborder flows done\n"
     ]
    }
   ],
   "source": [
    "# fetching data with right naming conventions\n",
    "\n",
    "from entsoe import EntsoeRawClient, EntsoePandasClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv \n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "\n",
    "load_dotenv()\n",
    "ENTSOE_API_KEY=\"562a20c4-03b0-4ee6-a692-19d534b4393a\"\n",
    "client = EntsoePandasClient(api_key=ENTSOE_API_KEY)\n",
    "\n",
    "start = pd.Timestamp('20150101', tz='UTC')\n",
    "change_date = pd.Timestamp('20181001', tz='UTC')\n",
    "end = pd.Timestamp(datetime.datetime.now(), tz='UTC')\n",
    "\n",
    "print(os.getcwd())\n",
    "out_dir = '../data_collection'\n",
    "\n",
    "country_code_old = 'DE_AT_LU'\n",
    "country_code_new = 'DE_LU'\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10), retry=retry_if_exception_type(Exception))\n",
    "def query_entsoe_data(query_func, country_code, start, end):\n",
    "    try:\n",
    "        df = query_func(country_code, start=start, end=end)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying data: {e}\")\n",
    "        raise\n",
    "    return df\n",
    "\n",
    "def merge_data(query_func):\n",
    "    data_old = query_entsoe_data(query_func, country_code_old, start, change_date)\n",
    "    \n",
    "    data_new = query_entsoe_data(query_func, country_code_new, change_date, end)\n",
    "\n",
    "    if not isinstance(data_old, pd.DataFrame):\n",
    "        data_old = data_old.to_frame()\n",
    "    if not isinstance(data_new, pd.DataFrame):\n",
    "        data_new = data_new.to_frame()\n",
    "    \n",
    "    if not data_old.empty and not data_new.empty:\n",
    "        if len(data_old.columns) != len(data_new.columns):\n",
    "            same_columns = list(set(data_old.columns) & set(data_new.columns))\n",
    "            data_old = data_old[same_columns]\n",
    "            data_new = data_new[same_columns]\n",
    "        else:\n",
    "            data_new.columns = data_old.columns\n",
    "    df_combined = pd.concat([data_old, data_new])\n",
    "    df_combined.index = df_combined.index.tz_convert('UTC')\n",
    "    return df_combined\n",
    "\n",
    "def save_df_with_timestamp(df, filename):\n",
    "    df_copy = df.copy()\n",
    "    df_copy.index.name = 'Date'\n",
    "    df_copy.to_csv(filename)\n",
    "\n",
    "# Day-ahead prices (EUR/MWh)\n",
    "day_ahead_prices = merge_data(client.query_day_ahead_prices)\n",
    "day_ahead_prices = day_ahead_prices.rename(columns={day_ahead_prices.columns[0]: 'day_ahead_prices_EURO'})\n",
    "save_df_with_timestamp(day_ahead_prices, '../data_collection/day_ahead_prices.csv')\n",
    "print('Day-ahead prices done')\n",
    "\n",
    "# Load forecast (MWh)\n",
    "load_forecast = merge_data(client.query_load_forecast)\n",
    "load_forecast = load_forecast.rename(columns={load_forecast.columns[0]: 'E_load_forecast_MWh'})\n",
    "save_df_with_timestamp(load_forecast, '../data_collection/load_forecast.csv')\n",
    "print('Load forecast done')\n",
    "\n",
    "# Generation forecast (MWh)\n",
    "generation_forecast = merge_data(client.query_generation_forecast)\n",
    "generation_forecast = generation_forecast.rename(columns={generation_forecast.columns[0]: 'E_generation_forecast_MWh'})\n",
    "save_df_with_timestamp(generation_forecast, '../data_collection/generation_forecast.csv')\n",
    "print('Generation forecast done')\n",
    "\n",
    "# Wind and solar forecasts (MWh)\n",
    "intraday_wind_solar_forecast = merge_data(client.query_intraday_wind_and_solar_forecast)\n",
    "for col in intraday_wind_solar_forecast.columns:\n",
    "    if 'Wind' in col:\n",
    "        intraday_wind_solar_forecast = intraday_wind_solar_forecast.rename(columns={col: 'E_wind_forecast_MWh'})\n",
    "    elif 'Solar' in col:\n",
    "        intraday_wind_solar_forecast = intraday_wind_solar_forecast.rename(columns={col: 'E_solar_forecast_MWh'})\n",
    "save_df_with_timestamp(intraday_wind_solar_forecast, '../data_collection/intraday_wind_solar_forecast.csv')\n",
    "print('Intraday wind and solar forecast done')\n",
    "\n",
    "# Day ahead wind and solar forecast (MWh)\n",
    "day_ahead_wind_solar_forecast = merge_data(client.query_wind_and_solar_forecast)\n",
    "for col in day_ahead_wind_solar_forecast.columns:\n",
    "    if 'Wind' in col:\n",
    "        day_ahead_wind_solar_forecast = day_ahead_wind_solar_forecast.rename(columns={col: 'E_wind_forecast_MWh'})\n",
    "    elif 'Solar' in col:\n",
    "        day_ahead_wind_solar_forecast = day_ahead_wind_solar_forecast.rename(columns={col: 'E_solar_forecast_MWh'})\n",
    "save_df_with_timestamp(day_ahead_wind_solar_forecast, '../data_collection/day_ahead_wind_solar_forecast.csv')\n",
    "print('Day ahead wind and solar forecast done')\n",
    "\n",
    "# Physical crossborder flows (MWh)\n",
    "physical_crossborder_flows = merge_data(lambda cc, start, end: client.query_physical_crossborder_allborders(start=start, end=end, country_code=cc, export=True))\n",
    "for col in physical_crossborder_flows.columns:\n",
    "    physical_crossborder_flows = physical_crossborder_flows.rename(columns={col: f'E_crossborder_{col}_actual_MWh'})\n",
    "save_df_with_timestamp(physical_crossborder_flows, '../data_collection/physical_crossborder_flows.csv')\n",
    "print('Physical crossborder flows done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/922687961.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d %H:%M:%S')\n"
     ]
    }
   ],
   "source": [
    "df4 = pd.read_csv('../data_collection/day_ahead_prices.csv')\n",
    "df5 = pd.read_csv('../data_collection/load_forecast.csv')\n",
    "df6 = pd.read_csv('../data_collection/generation_forecast.csv')\n",
    "df7 = pd.read_csv('../data_collection/intraday_wind_solar_forecast.csv')\n",
    "df8 = pd.read_csv('../data_collection/day_ahead_wind_solar_forecast.csv')\n",
    "df9 = pd.read_csv('../data_collection/physical_crossborder_flows.csv')\n",
    "\n",
    "\n",
    "merged_df2 = pd.merge(df5, df4, on='Date', how='outer')\n",
    "merged_df2 = pd.merge(merged_df2, df6, on='Date', how='outer')\n",
    "merged_df2 = pd.merge(merged_df2, df7, on='Date', how='outer')\n",
    "merged_df2 = pd.merge(merged_df2, df8, on='Date', how='outer')\n",
    "merged_df2 = pd.merge(merged_df2, df9, on='Date', how='outer')\n",
    "\n",
    "merged_df2.to_csv('../data_collection/merged_data2.csv', index=False)\n",
    "\n",
    "df = pd.read_csv('../data_collection/merged_data2.csv')\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df_filtered = df[df['Date'].dt.minute == 0]\n",
    "df_filtered['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_filtered.to_csv('../data_collection/merged_data3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is responsible for the Covid Lockdown Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0               state   Measure   2020-03-08  2020-03-09\n",
      "0         1.0  Baden-Wuerttemberg  leavehome         0.0         0.0\n",
      "1         1.0  Baden-Wuerttemberg       dist         0.0         0.0\n",
      "2         1.0  Baden-Wuerttemberg        msk         0.0         0.0\n",
      "3         1.0  Baden-Wuerttemberg     shppng         0.0         0.0\n",
      "4         1.0  Baden-Wuerttemberg       hcut         0.0         0.0\n",
      "<bound method NDFrame.head of                      Date  Covid factor\n",
      "0     2015-01-01 00:00:00           0.0\n",
      "1     2015-01-01 01:00:00           0.0\n",
      "2     2015-01-01 02:00:00           0.0\n",
      "3     2015-01-01 03:00:00           0.0\n",
      "4     2015-01-01 04:00:00           0.0\n",
      "...                   ...           ...\n",
      "87475 2024-12-23 19:00:00           0.0\n",
      "87476 2024-12-23 20:00:00           0.0\n",
      "87477 2024-12-23 21:00:00           0.0\n",
      "87478 2024-12-23 22:00:00           0.0\n",
      "87479 2024-12-23 23:00:00           0.0\n",
      "\n",
      "[87480 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "FILE_URL = 'https://pada.psycharchives.org/bitstream/9ff033a9-4084-4d0e-87eb-aa963a1324a5'\n",
    "covid_df = pd.read_csv(FILE_URL, sep=\",\", header=[0])\n",
    "print(covid_df.head().iloc[:,:5])\n",
    "\n",
    "# dict with influence of measure (see readme)\n",
    "measure_influence = {\n",
    "    'leavehome': 1,\n",
    "    'dist': 0,\n",
    "    'msk': 1,\n",
    "    'shppng': 2,\n",
    "    'hcut': 2,\n",
    "    'ess_shps': 2,\n",
    "    'zoo': 0,\n",
    "    'demo': 0,\n",
    "    'school': 1,\n",
    "    'church': 0,\n",
    "    'onefriend': 0,\n",
    "    'morefriends': 0,\n",
    "    'plygrnd': 0,\n",
    "    'daycare': 2,\n",
    "    'trvl': 1,\n",
    "    'gastr': 2\n",
    "}\n",
    "# dict with state relative population of country\n",
    "state_percentages = {\n",
    "    'Baden-Wuerttemberg': 0.133924061,\n",
    "    'Bayern': 0.158676851,\n",
    "    'Berlin': 0.044670274,\n",
    "    'Brandenburg': 0.030491172,\n",
    "    'Bremen': 0.008169464,\n",
    "    'Hamburg': 0.022560236,\n",
    "    'Hessen': 0.075833,\n",
    "    'Mecklenburg-Vorpommern': 0.019245033,\n",
    "    'Niedersachsen': 0.096398323,\n",
    "    'Nordrhein-Westfalen': 0.214840756,\n",
    "    'Rheinland-Pfalz': 0.049301337,\n",
    "    'Saarland': 0.011744796,\n",
    "    'Sachsen': 0.048299274,\n",
    "    'Sachsen-Anhalt': 0.025752514,\n",
    "    'Schleswig-Holstein': 0.035026746,\n",
    "    'Thueringen': 0.025066162\n",
    "}\n",
    "\n",
    "def evaluate_date(request_date):\n",
    "    if request_date in list(covid_df):\n",
    "        truncated_covid_df = covid_df[['state', 'Measure ', request_date]]\n",
    "        sum_value = 0\n",
    "        for index, row in truncated_covid_df.iterrows():\n",
    "            if row.isnull().values.any(): continue  # if any value in row is missing\n",
    "            if measure_influence[row['Measure ']] == 0: continue  # if measure has no influence\n",
    "            sum_value += ((int(row[request_date]) / 5) + 0.6) * state_percentages[row['state']] * measure_influence[\n",
    "                row['Measure ']]  # see readme documentation\n",
    "        return sum_value\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# generate and populate dataframe with all dates from 2015-1-1 - today\n",
    "from datetime import date, timedelta\n",
    "\n",
    "working_dt = date(2015, 1, 1)\n",
    "end_dt = date(date.today().year, date.today().month, date.today().day)\n",
    "delta = timedelta(days=1)\n",
    "\n",
    "data_rows = []\n",
    "\n",
    "# populate df\n",
    "while working_dt <= end_dt:\n",
    "    factor = evaluate_date(working_dt.isoformat())\n",
    "    date = working_dt.isoformat()\n",
    "    for hour in range(24):\n",
    "        timestamp = pd.Timestamp(working_dt.isoformat()) + pd.Timedelta(hours=hour)\n",
    "        data_rows.append({'Date': timestamp, 'Covid factor': factor})  # Add to rows list\n",
    "    working_dt += delta\n",
    "\n",
    "covid_factors_df = pd.DataFrame(data_rows)\n",
    "print(covid_factors_df.head)\n",
    "\n",
    "covid_factors_df.to_csv('../data_collection/covid.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is responsible for the SMARD Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3820852020.py:350: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Start_Date'] = pd.to_datetime(df['Start_Date'])\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3820852020.py:350: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Start_Date'] = pd.to_datetime(df['Start_Date'])\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3820852020.py:350: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Start_Date'] = pd.to_datetime(df['Start_Date'])\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3820852020.py:350: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Start_Date'] = pd.to_datetime(df['Start_Date'])\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3820852020.py:350: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Start_Date'] = pd.to_datetime(df['Start_Date'])\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3820852020.py:350: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Start_Date'] = pd.to_datetime(df['Start_Date'])\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3820852020.py:350: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Start_Date'] = pd.to_datetime(df['Start_Date'])\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3820852020.py:350: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Start_Date'] = pd.to_datetime(df['Start_Date'])\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3820852020.py:350: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Start_Date'] = pd.to_datetime(df['Start_Date'])\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3820852020.py:350: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Start_Date'] = pd.to_datetime(df['Start_Date'])\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3820852020.py:350: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Start_Date'] = pd.to_datetime(df['Start_Date'])\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3820852020.py:350: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Start_Date'] = pd.to_datetime(df['Start_Date'])\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/3820852020.py:350: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Start_Date'] = pd.to_datetime(df['Start_Date'])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import requests, datetime\n",
    "from io import StringIO\n",
    "#from datetime import datetime\n",
    "\n",
    "#-------------translation for Balancing:------------------\n",
    "balancing_id={\n",
    "    #automatic frequency, tag=af\n",
    "    \"automatic_frequency\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Volume activated (+) [MWh] Calculated resolutions\":\"af_E_Volume_Activated_Plus_MWh\",\n",
    "        \"Volume activated (-) [MWh] Calculated resolutions\":\"af_E_Volume_Activated_Minus_MWh\",\n",
    "        \"Activation price (+) [€/MWh] Calculated resolutions\":\"af_Activation_Price_Plus_EUR/MWh\",\n",
    "        \"Activation price (-) [€/MWh] Calculated resolutions\":\"af_Activation_Price_Minus_EUR/MWh\",\n",
    "        \"Volume procured (+) [MW] Calculated resolutions\":\"af_E_Volume_Procured_Plus_MW\",\n",
    "        \"Volume procured (-) [MW] Calculated resolutions\":\"af_E_Volume_Procured_Minus_MW\",\n",
    "        \"Procurement price (+) [€/MW] Calculated resolutions\":\"af_Procurement_Price_Plus_EUR/MW\",\n",
    "        \"Procurement price (-) [€/MW] Calculated resolutions\":\"af_Procurement_Price_Minus_EUR/MW\",\n",
    "    },\n",
    "    #tag=mf\n",
    "    \"manual_frequency\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Volume activated (+) [MWh] Calculated resolutions\":\"mf_E_Volume_Activated_Plus_MWh\",\n",
    "        \"Volume activated (-) [MWh] Calculated resolutions\":\"mf_E_Volume_Activated_Minus_MWh\",\n",
    "        \"Activation price (+) [€/MWh] Calculated resolutions\":\"mf_Activation_Price_Plus_EUR/MWh\",\n",
    "        \"Activation price (-) [€/MWh] Calculated resolutions\":\"mf_Activation_Price_Minus_EUR/MWh\",\n",
    "        \"Volume procured (+) [MW] Calculated resolutions\":\"mf_E_Volume_Procured_Plus_MW\",\n",
    "        \"Volume procured (-) [MW] Calculated resolutions\":\"mf_E_Volume_Procured_Minus_MW\",\n",
    "        \"Procurement price (+) [€/MW] Calculated resolutions\":\"mf_Procurement_Price_Plus_EUR/MW\",\n",
    "        \"Procurement price (-) [€/MW] Calculated resolutions\":\"mf_Procurement_Price_Minus_EUR/MW\",\n",
    "    },\n",
    "     #balancing energy\n",
    "    \"balancing_energy\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Volume (+) [MWh] Calculated resolutions\":\"E_Volume_Calculated_Plus_MWh\",\n",
    "        \"Volume (-) [MWh] Calculated resolutions\":\"E_Volume_Calculated_Minus_MWh\",\n",
    "        \"Price [€/MWh] Calculated resolutions\":\"Price_Calculated_EUR/MWh\",\n",
    "        \"Net income [€] Calculated resolutions\":\"Net_Income_EUR\",\n",
    "    },\n",
    "    #costs\n",
    "    \"costs\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Balancing services [€] Calculated resolutions\":\"Balancing_Services_Calculated_EUR\",\n",
    "        \"Network security [€] Calculated resolutions\":\"Network_Security_Calculated_EUR\",\n",
    "        \"Countertrading [€] Calculated resolutions\":\"Countertrading_Calculated_EUR\",\n",
    "    },\n",
    "    #frequency_containment_reserve\n",
    "    \"frequency_containment\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Volume procured [MW] Calculated resolutions\":\"E_Volume_Procured_Calculated_MW\",\n",
    "        \"Procurement price [€/MW] Calculated resolutions\":\"Price_Procument_Calculated_EUR/MW\"\n",
    "    },\n",
    "    \"imported_balancing_services\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Austria [MWh] Calculated resolutions\":\"import_E_Austria_Calculated_MWh\",\n",
    "    },\n",
    "    \"exported_balancing_services\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Austria [MWh] Calculated resolutions\":\"export_E_Austria_Calculated_MWh\",\n",
    "    }         \n",
    "}    \n",
    "\n",
    "#actual consumption tag=actual\n",
    "electricity_consumption_id={\n",
    "    \"actual\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Total (grid load) [MWh] Calculated resolutions\":\"actual_E_Total_Gridload_MWh\",\n",
    "        \"Residual load [MWh] Calculated resolutions\":\"actual_E_Residual_Load_MWh\",\n",
    "        \"Hydro pumped storage [MWh] Calculated resolutions\":\"actual_E_Hydro_Pumped_Storage_MWh\",\n",
    "    },\n",
    "    #forecasted consumption tag=forecast\n",
    "    \"forecast\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Total (grid load) [MWh] Calculated resolutions\":\"forecast_E_Total_Gridload_MWh\",\n",
    "        \"Residual load [MWh] Calculated resolutions\":\"forecast_actual_E_Residual_Load_MWh\"\n",
    "    }\n",
    "}\n",
    "\n",
    "electricity_generation_id={\n",
    "    #actual generation\n",
    "    \"actual\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Biomass [MWh] Calculated resolutions\":\"actual_generation_E_Biomass_MWh\",\n",
    "        \"Hydropower [MWh] Calculated resolutions\":\"actual_generation_E_Hydropower_MWh\",\n",
    "        \"Wind offshore [MWh] Calculated resolutions\":\"actual_generation_E_Windoffshore_MWh\",\n",
    "        \"Wind onshore [MWh] Calculated resolutions\":\"actual_generation_E_Windonshore_MWh\",\n",
    "        \"Photovoltaics [MWh] Calculated resolutions\":\"actual_generation_E_Photovoltaics_MWh\",\n",
    "        \"Other renewable [MWh] Calculated resolutions\":\"actual_generation_E_OtherRenewable_MWh\",\n",
    "        \"Nuclear [MWh] Calculated resolutions\":\"actual_generation_E_Nuclear_MWh\",\n",
    "        \"Lignite [MWh] Calculated resolutions\":\"actual_generation_E_Lignite_MWh\",\n",
    "        \"Hard coal [MWh] Calculated resolutions\":\"actual_generation_E_HardCoal_MWh\",\n",
    "        \"Fossil gas [MWh] Calculated resolutions\":\"actual_generation_E_FossilGas_MWh\",\n",
    "        \"Hydro pumped storage [MWh] Calculated resolutions\":\"actual_generation_E_HydroPumpedStorage_MWh\",\n",
    "        \"Other conventional [MWh] Calculated resolutions\":\"actual_generation_E_OtherConventional_MWh\"\n",
    "    },\n",
    "    \n",
    "    #forecastet generation day ahead\n",
    "    \"forecast\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Biomass [MWh] Calculated resolutions\":\"forecast_generation_E_Biomass_MWh\",\n",
    "        \"Hydropower [MWh] Calculated resolutions\":\"forecast_generation_E_Hydropower_MWh\",\n",
    "        \"Wind offshore [MWh] Calculated resolutions\":\"forecast_generation_E_Windoffshore_MWh\",\n",
    "        \"Wind onshore [MWh] Calculated resolutions\":\"forecast_generation_E_Windonshore_MWh\",\n",
    "        \"Photovoltaics [MWh] Calculated resolutions\":\"forecast_generation_E_Photovoltaics_MWh\",\n",
    "        \"Other renewable [MWh] Calculated resolutions\":\"forecast_generation_E_OtherRenewable_MWh\",\n",
    "        \"Nuclear [MWh] Calculated resolutions\":\"forecast_generation_E_Nuclear_MWh\",\n",
    "        \"Lignite [MWh] Calculated resolutions\":\"forecast_generation_E_Lignite_MWh\",\n",
    "        \"Hard coal [MWh] Calculated resolutions\":\"forecast_generation_E_HardCoal_MWh\",\n",
    "        \"Fossil gas [MWh] Calculated resolutions\":\"forecast_generation_E_FossilGas_MWh\",\n",
    "        \"Hydro pumped storage [MWh] Calculated resolutions\":\"forecast_generation_E_HydroPumpedStorage_MWh\",\n",
    "        \"Other [MWh] Calculated resolutions\":\"forecast_generation_E_Other_MWh\",\n",
    "        \"Total [MWh] Original resolutions\":\"forecast_generation_E_Total_MWh\",\n",
    "        \"Photovoltaics and wind [MWh] Calculated resolutions\":\"forecast_generation_E_PhotovoltaicsAndWind_MWh\",\n",
    "        \"Other [MWh] Original resolutions\":\"forecast_generation_E_Original_MWh\"\n",
    "    },\n",
    "\n",
    "    #installed generation capacity\n",
    "    #key=instGenCapacity\n",
    "    \"installed_generation_capacity\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Biomass [MW] Calculated resolutions\":\"instGenCapacity_E_Biomass_MW\",\n",
    "        \"Hydropower [MW] Calculated resolutions\":\"instGenCapacity_E_Hydropower_MW\",\n",
    "        \"Wind offshore [MW] Calculated resolutions\":\"instGenCapacity_E_Windoffshore_MW\",\n",
    "        \"Wind onshore [MW] Calculated resolutions\":\"instGenCapacity_E_Windonshore_MW\",\n",
    "        \"Photovoltaics [MW] Calculated resolutions\":\"instGenCapacity_E_Photovoltaics_MW\",\n",
    "        \"Other renewable [MW] Calculated resolutions\":\"instGenCapacity_E_OtherRenewable_MW\",\n",
    "        \"Nuclear [MW] Calculated resolutions\":\"instGenCapacity_E_Nuclear_MW\",\n",
    "        \"Lignite [MW] Calculated resolutions\":\"instGenCapacity_E_Lignite_MW\",\n",
    "        \"Hard coal [MW] Calculated resolutions\":\"instGenCapacity_E_HardCoal_MW\",\n",
    "        \"Fossil gas [MW] Calculated resolutions\":\"instGenCapacity_E_FossilGas_MW\",\n",
    "        \"Hydro pumped storage [MW] Calculated resolutions\":\"instGenCapacity_E_HydroPumpedStorage_MW\",\n",
    "        \"Other conventional [MW] Calculated resolutions\":\"instGenCapacity_E_OtherConventional_MW\"\n",
    "    }\n",
    "}\n",
    "\n",
    "market_id={\n",
    "    #key=dayAhead\n",
    "    \"day_ahead_prices\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Germany/Luxembourg [€/MWh] Original resolutions\":\"dayAhead_Price_GermanyAndLuxembourg_EUR/MWh\",\n",
    "        \"∅ DE/LU neighbours [€/MWh] Original resolutions\":\"dayAhead_Price_GermanyAndLuxembourgAverage_EUR/MWh\",\n",
    "        \"Belgium [€/MWh] Original resolutions\":\"dayAhead_Price_Belgium_EUR/MWh\",\n",
    "        \"Denmark 1 [€/MWh] Original resolutions\":\"dayAhead_Price_Denmark1_EUR/MWh\",\n",
    "        \"Denmark 2 [€/MWh] Original resolutions\":\"dayAhead_Price_Denmark2_EUR/MWh\",\n",
    "        \"France [€/MWh] Original resolutions\":\"dayAhead_Price_France_EUR/MWh\",\n",
    "        \"Netherlands [€/MWh] Original resolutions\":\"dayAhead_Price_Netherlands_EUR/MWh\",\n",
    "        \"Norway 2 [€/MWh] Original resolutions\":\"dayAhead_Price_Norway2_EUR/MWh\",\n",
    "        \"Austria [€/MWh] Original resolutions\":\"dayAhead_Price_Austria_EUR/MWh\",\n",
    "        \"Poland [€/MWh] Original resolutions\":\"dayAhead_Price_Poland_EUR/MWh\",\n",
    "        \"Sweden 4 [€/MWh] Original resolutions\":\"dayAhead_Price_Sweden4_EUR/MWh\",\n",
    "        \"Switzerland [€/MWh] Original resolutions\":\"dayAhead_Price_Switzerland_EUR/MWh\",\n",
    "        \"Czech Republic [€/MWh] Original resolutions\":\"dayAhead_Price_CzechRepublic_EUR/MWh\",\n",
    "        \"DE/AT/LU [€/MWh] Original resolutions\":\"dayAhead_Price_DE/AT/LU_EUR/MWh\",\n",
    "        \"Northern Italy [€/MWh] Original resolutions\":\"dayAhead_Price_NothernItaly_EUR/MWh\",\n",
    "        \"Slovenia [€/MWh] Original resolutions\":\"dayAhead_Price_Slovenia_EUR/MWh\",\n",
    "        \"Hungary [€/MWh] Original resolutions\":\"dayAhead_Price_Hungary_EUR/MWh\"\n",
    "    },\n",
    "    \n",
    "    \"cross_border_physical\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Net export [MWh] Calculated resolutions\":\"E_NetExport_crossBorderPhysical_MWh\",\n",
    "        \"Netherlands (export) [MWh] Calculated resolutions\":\"E_NetherlandExport_corssBorderPhysical_MWh\",\n",
    "        \"Netherlands (import) [MWh] Calculated resolutions\":\"E_NetherlandImport_corssBorderPhysical_MW\",\n",
    "        \"Switzerland (export) [MWh] Calculated resolutions\":\"E_SwitzerlandExport_corssBorderPhysical_MWh\",\n",
    "        \"Switzerland (import) [MWh] Calculated resolutions\":\"E_SwitzerlandImport_corssBorderPhysical_MWh\",\n",
    "        \"Denmark (export) [MWh] Calculated resolutions\":\"E_DenmarkExport_corssBorderPhysical_MWh\",\n",
    "        \"Denmark (import) [MWh] Calculated resolutions\":\"E_Denmark_Import_corssBorderPhysical_MWh\",\n",
    "        \"Czech Republic (export) [MWh] Calculated resolutions\":\"E_CzechrepublicExport_corssBorderPhysical_MWh\",\n",
    "        \"Czech Republic (import) [MWh] Calculated resolutions\":\"E_CzechrepublicImport_corssBorderPhysical_MWh\",\n",
    "        \"Luxembourg (export) [MWh] Calculated resolutions\":\"E_LuxembourgExport_corssBorderPhysical_MWh\",\n",
    "        \"Luxembourg (import) [MWh] Calculated resolutions\":\"E_LuxembourgImport_corssBorderPhysical_MWh\",\n",
    "        \"Sweden (export) [MWh] Calculated resolutions\":\"E_SwedenExport_corssBorderPhysical_MWh\",\n",
    "        \"Sweden (import) [MWh] Calculated resolutions\":\"E_SwedenImportv_corssBorderPhysical_MWh\",\n",
    "        \"Austria (export) [MWh] Calculated resolutions\":\"E_AustriaExport_corssBorderPhysical_MWh\",\n",
    "        \"Austria (import) [MWh] Calculated resolutions\":\"E_AustriaImport_corssBorderPhysical_MWh\",\n",
    "        \"France (export) [MWh] Calculated resolutions\":\"E_FranceExport_corssBorderPhysical_MWh\",        \n",
    "        \"France (import) [MWh] Calculated resolutions\":\"E_FranceImport_corssBorderPhysical_MWh\",\n",
    "        \"Poland (export) [MWh] Calculated resolutions\":\"E_PolandExport_corssBorderPhysical_MWh\",\n",
    "        \"Poland (import) [MWh] Calculated resolutions\":\"E_PolandImport_corssBorderPhysical_MWh\",\n",
    "        \"Norway (export) [MWh] Calculated resolutions\":\"E_NorwayExport_corssBorderPhysical_MWh\",\n",
    "        \"Norway (import) [MWh] Calculated resolutions\":\"E_NorwayImport_corssBorderPhysical_MWh\",\n",
    "        \"Belgium (export) [MWh] Calculated resolutions\":\"E_BelgiumExport_corssBorderPhysical_MWh\",\n",
    "        \"Belgium (import) [MWh] Calculated resolutions\":\"E_BelgiumImport_corssBorderPhysical_MWh\",\n",
    "    },\n",
    "    \"scheudled_commercial_exchanges\":{\n",
    "        \"Start date\":\"Start_Date\",\n",
    "        \"End date\":\"End_Date\",\n",
    "        \"Net export [MWh] Calculated resolutions\":\"E_NetExport_MWh\",\n",
    "        \"Netherlands (export) [MWh] Calculated resolutions\":\"E_NetherlandExport_MWh\",\n",
    "        \"Netherlands (import) [MWh] Calculated resolutions\":\"E_NetherlandImport_MW\",\n",
    "        \"Switzerland (export) [MWh] Calculated resolutions\":\"E_SwitzerlandExport_MWh\",\n",
    "        \"Switzerland (import) [MWh] Calculated resolutions\":\"E_SwitzerlandImport_MWh\",\n",
    "        \"Denmark (export) [MWh] Calculated resolutions\":\"E_DenmarkExport_MWh\",\n",
    "        \"Denmark (import) [MWh] Calculated resolutions\":\"E_Denmark_Import_MWh\",\n",
    "        \"Czech Republic (export) [MWh] Calculated resolutions\":\"E_CzechrepublicExport_MWh\",\n",
    "        \"Czech Republic (import) [MWh] Calculated resolutions\":\"E_CzechrepublicImport_MWh\",\n",
    "        \"Luxembourg (export) [MWh] Calculated resolutions\":\"E_LuxembourgExport_MWh\",\n",
    "        \"Luxembourg (import) [MWh] Calculated resolutions\":\"E_LuxembourgImport_MWh\",\n",
    "        \"Sweden (export) [MWh] Calculated resolutions\":\"E_SwedenExport_MWh\",\n",
    "        \"Sweden (import) [MWh] Calculated resolutions\":\"E_SwedenImport_MWh\",\n",
    "        \"Austria (export) [MWh] Calculated resolutions\":\"E_AustriaExport_MWh\",\n",
    "        \"Austria (import) [MWh] Calculated resolutions\":\"E_AustriaImport_MWh\",\n",
    "        \"France (export) [MWh] Calculated resolutions\":\"E_FranceExport_MWh\",        \n",
    "        \"France (import) [MWh] Calculated resolutions\":\"E_FranceImport_MWh\",\n",
    "        \"Poland (export) [MWh] Calculated resolutions\":\"E_PolandExport_MWh\",\n",
    "        \"Poland (import) [MWh] Calculated resolutions\":\"E_PolandImport_MWh\",\n",
    "        \"Norway (export) [MWh] Calculated resolutions\":\"E_NorwayExport_MWh\",\n",
    "        \"Norway (import) [MWh] Calculated resolutions\":\"E_NorwayImport_MWh\",\n",
    "        \"Belgium (export) [MWh] Calculated resolutions\":\"E_BelgiumExport_MWh\",\n",
    "        \"Belgium (import) [MWh] Calculated resolutions\":\"E_BelgiumImport_MWh\",\n",
    "    }\n",
    "}\n",
    "\n",
    "def main():\n",
    "\n",
    "    output_path = sys.argv[1]\n",
    "\n",
    "    dict_ids = [balancing_id[\"automatic_frequency\"],\n",
    "                balancing_id[\"balancing_energy\"],\n",
    "                balancing_id[\"costs\"],\n",
    "                balancing_id[\"exported_balancing_services\"],\n",
    "                balancing_id[\"frequency_containment\"],\n",
    "                balancing_id[\"imported_balancing_services\"],\n",
    "                balancing_id[\"manual_frequency\"],\n",
    "                electricity_consumption_id[\"actual\"],\n",
    "                electricity_consumption_id[\"forecast\"],\n",
    "                electricity_generation_id[\"actual\"],\n",
    "                electricity_generation_id[\"forecast\"],\n",
    "                market_id[\"cross_border_physical\"],\n",
    "                market_id[\"scheudled_commercial_exchanges\"],\n",
    "                market_id[\"day_ahead_prices\"]    \n",
    "    ]\n",
    "    \n",
    "    final_df = None\n",
    "\n",
    "    for i in range(13):\n",
    "        working_df = download(i)\n",
    "        working_df = new_format(working_df, dict_ids[i])\n",
    "\n",
    "        if i > 0:\n",
    "            working_df=working_df.drop(working_df.columns[1],axis=1)\n",
    "        #only called once\n",
    "        if final_df is None:\n",
    "            final_df = working_df\n",
    "        else:\n",
    "            final_df = pd.merge(final_df, working_df, on=working_df.columns[0], how='inner', copy=True)\n",
    "    \n",
    "    final_df.to_csv(output_path, sep=',', index=False)\n",
    "\n",
    "    #use gzip to compress .csv outputfile to <file_out>.gz\n",
    "    path_object = Path(output_path)\n",
    "    output_pathgz = path_object.with_suffix('.gz')\n",
    "    final_df.to_csv(output_pathgz, sep=',', index=False, compression='gzip')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def download(download_id):\n",
    "    #14 different files\n",
    "    match download_id:\n",
    "        # AUTOMATIC FREQUENCY RESTORATION\n",
    "        case 0:\n",
    "            response = requests.post('https://www.smard.de/nip-download-manager/nip/download/market-data',\n",
    "                                data='{\"request_form\":[{\"format\":\"CSV\",\"moduleIds\":[18004368,18004369,18004370,18004351,18004371,18004372,18004373,18004374],\"region\":\"DE\",\"timestamp_from\":1420066800000,\"timestamp_to\":'+str(int(datetime.datetime.today().timestamp()))+'000,\"type\":\"discrete\",\"language\":\"en\",\"resolution\":\"hour\"}]}')\n",
    "        # BALANCING ENERGY\n",
    "        case 1:\n",
    "            response = requests.post('https://www.smard.de/nip-download-manager/nip/download/market-data',\n",
    "                                data='{\"request_form\":[{\"format\":\"CSV\",\"moduleIds\":[15004383,15004384,15004382,15004390],\"region\":\"DE\",\"timestamp_from\":1420066800000,\"timestamp_to\":'+str(int(datetime.datetime.today().timestamp()))+'000,\"type\":\"discrete\",\"language\":\"en\",\"resolution\":\"hour\"}]}')        \n",
    "        # COSTS  \n",
    "        case 2:\n",
    "            response = requests.post('https://www.smard.de/nip-download-manager/nip/download/market-data',\n",
    "                                data='{\"request_form\":[{\"format\":\"CSV\",\"moduleIds\":[16004391,16000419,16000418],\"region\":\"DE\",\"timestamp_from\":1420066800000,\"timestamp_to\":'+str(int(datetime.datetime.today().timestamp()))+'000,\"type\":\"discrete\",\"language\":\"en\",\"resolution\":\"hour\"}]}')\n",
    "        # EXPORTED BALANCING SERVICES\n",
    "        case 3:\n",
    "            response = requests.post('https://www.smard.de/nip-download-manager/nip/download/market-data',\n",
    "                                data='{\"request_form\":[{\"format\":\"CSV\",\"moduleIds\":[20004385],\"region\":\"DE\",\"timestamp_from\":1420066800000,\"timestamp_to\":'+str(int(datetime.datetime.today().timestamp()))+'000,\"type\":\"discrete\",\"language\":\"en\",\"resolution\":\"hour\"}]}')\n",
    "        #FREQUENCY CONTAINMENT RESERVE\n",
    "        case 4:\n",
    "            response = requests.post('https://www.smard.de/nip-download-manager/nip/download/market-data',\n",
    "                                data='{\"request_form\":[{\"format\":\"CSV\",\"moduleIds\":[17004363, 17004367],\"region\":\"DE\",\"timestamp_from\":1420066800000,\"timestamp_to\":'+str(int(datetime.datetime.today().timestamp()))+'000,\"type\":\"discrete\",\"language\":\"en\",\"resolution\":\"hour\"}]}')\n",
    "        # IMPORTED BALANCING SERVICES\n",
    "        case 5:\n",
    "            response = requests.post('https://www.smard.de/nip-download-manager/nip/download/market-data',\n",
    "                                data='{\"request_form\":[{\"format\":\"CSV\",\"moduleIds\":[21004386],\"region\":\"DE\",\"timestamp_from\":1420066800000,\"timestamp_to\":'+str(int(datetime.datetime.today().timestamp()))+'000,\"type\":\"discrete\",\"language\":\"en\",\"resolution\":\"hour\"}]}')\n",
    "        # MANUAL FREQUENCY RESTORATION RESERVE\n",
    "        case 6:\n",
    "            response = requests.post('https://www.smard.de/nip-download-manager/nip/download/market-data',\n",
    "                                data='{\"request_form\":[{\"format\":\"CSV\",\"moduleIds\":[19004377,19004375,19004376,19004352,19004378,19004379,19004380,19004381],\"region\":\"DE\",\"timestamp_from\":1420066800000,\"timestamp_to\":'+str(int(datetime.datetime.today().timestamp()))+'000,\"type\":\"discrete\",\"language\":\"en\",\"resolution\":\"hour\"}]}')\n",
    "        \n",
    "        #electricity consumption, actual\n",
    "        case 7:\n",
    "            response = requests.post('https://www.smard.de/nip-download-manager/nip/download/market-data',\n",
    "                                data='{\"request_form\":[{\"format\":\"CSV\",\"moduleIds\":[5000410,5004387,5004359],\"region\":\"DE\",\"timestamp_from\":1420066800000,\"timestamp_to\":'+str(int(datetime.datetime.today().timestamp()))+'000,\"type\":\"discrete\",\"language\":\"en\",\"resolution\":\"hour\"}]}')\n",
    "        #forecast consumption\n",
    "        case 8:\n",
    "            response = requests.post('https://www.smard.de/nip-download-manager/nip/download/market-data',\n",
    "                                data='{\"request_form\":[{\"format\":\"CSV\",\"moduleIds\":[6000411,6004362],\"region\":\"DE\",\"timestamp_from\":1420066800000,\"timestamp_to\":'+str(int(datetime.datetime.today().timestamp()))+'000,\"type\":\"discrete\",\"language\":\"en\",\"resolution\":\"hour\"}]}')\n",
    "        #electricity generation actual\n",
    "        case 9:\n",
    "            response = requests.post('https://www.smard.de/nip-download-manager/nip/download/market-data',\n",
    "                                data='{\"request_form\":[{\"format\":\"CSV\",\"moduleIds\":[1001224,1004066,1004067,1004068,1001223,1004069,1004071,1004070,1001226,1001228,1001227,1001225],\"region\":\"DE\",\"timestamp_from\":1420066800000,\"timestamp_to\":'+str(int(datetime.datetime.today().timestamp()))+'000,\"type\":\"discrete\",\"language\":\"en\",\"resolution\":\"hour\"}]}')\n",
    "        #electricity generation forecast\n",
    "        case 10:\n",
    "            response = requests.post('https://www.smard.de/nip-download-manager/nip/download/market-data',\n",
    "                                data='{\"request_form\":[{\"format\":\"CSV\",\"moduleIds\":[2000122,2005097,2000715,2003791,2000123,2000125],\"region\":\"DE\",\"timestamp_from\":1420066800000,\"timestamp_to\":'+str(int(datetime.datetime.today().timestamp()))+'000,\"type\":\"discrete\",\"language\":\"en\",\"resolution\":\"hour\"}]}')\n",
    "        #MARKET\n",
    "        # CROSSBORDER FLOWS\n",
    "        case 11:\n",
    "            response = requests.post('https://www.smard.de/nip-download-manager/nip/download/market-data',\n",
    "                                data='{\"request_form\":[{\"format\":\"CSV\",\"moduleIds\":[31004963,31004736,31004737,31004740,31004741,31004988,31004990,31004992,31004994,31004738,31004742,31004743,31004744,31004880,31004881,31004882,31004883,31004884,31004885,31004886,31004887,31004888,31004739],\"region\":\"DE\",\"timestamp_from\":1420066800000,\"timestamp_to\":'+str(int(datetime.datetime.today().timestamp()))+'000,\"type\":\"discrete\",\"language\":\"en\",\"resolution\":\"hour\"}]}')\n",
    "        # CROSSBORDER SCHEDULED FLOWS\n",
    "        case 12:\n",
    "            response = requests.post('https://www.smard.de/nip-download-manager/nip/download/market-data',\n",
    "                                data='{\"request_form\":[{\"format\":\"CSV\",\"moduleIds\":[22004629,22004722,22004724,22004404,22004409,22004545,22004546,22004548,22004550,22004551,22004552,22004405,22004547,22004403,22004406,22004407,22004408,22004410,22004412,22004549,22004553,22004998,22004712],\"region\":\"DE\",\"timestamp_from\":1420066800000,\"timestamp_to\":'+str(int(datetime.datetime.today().timestamp()))+'000,\"type\":\"discrete\",\"language\":\"en\",\"resolution\":\"hour\"}]}')\n",
    "        # DAYAHEAD\n",
    "        case 13:\n",
    "            response = requests.post('https://www.smard.de/nip-download-manager/nip/download/market-data',\n",
    "                                data='{\"request_form\":[{\"format\":\"CSV\",\"moduleIds\":[8004169,8004170,8000251,8005078,8000252,8000253,8000254,8000255,8000256,8000257,8000258,8000259,8000260,8000261,8000262,8004996,8004997],\"region\":\"DE\",\"timestamp_from\":1420066800000,\"timestamp_to\":'+str(int(datetime.datetime.today().timestamp()))+'000,\"type\":\"discrete\",\"language\":\"en\",\"resolution\":\"hour\"}]}')\n",
    "        \n",
    "    csvfile_data = response.content.decode('utf-8-sig')\n",
    "    download_df = pd.read_csv(StringIO(csvfile_data), sep=\";\", header=[0], na_values='-', low_memory=False)\n",
    "    \n",
    "    return download_df\n",
    "\n",
    "\n",
    "\n",
    "def new_format(df, my_dict):\n",
    "        \n",
    "    #use fitting dict to rename table head\n",
    "    df.rename(columns=my_dict, inplace=True)\n",
    "    \n",
    "    #change Datetime_format; replace '-' with np.nan\n",
    "    df['Start_Date'] = pd.to_datetime(df['Start_Date'])\n",
    "    df['End_Date'] = pd.to_datetime(df['End_Date'])\n",
    "    df.replace(\"-\",np.nan, inplace=True)\n",
    "\n",
    "    #remove , seperator for thousand\n",
    "    df.replace(\",\",\"\", inplace=True, regex=True)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def merge(fin_df, work_df, i):\n",
    "\n",
    "    if i > 0:\n",
    "        work_df=work_df.drop(work_df.columns[1],axis=1)\n",
    "    \n",
    "    fin_df = pd.merge(fin_df, work_df, on=work_df.columns[0], how='inner', copy=True)\n",
    "    \n",
    "\n",
    "output_path = '../data_collection/smard.csv'\n",
    "\n",
    "dict_ids = [balancing_id[\"automatic_frequency\"],\n",
    "            balancing_id[\"balancing_energy\"],\n",
    "            balancing_id[\"costs\"],\n",
    "            balancing_id[\"exported_balancing_services\"],\n",
    "            balancing_id[\"frequency_containment\"],\n",
    "            balancing_id[\"imported_balancing_services\"],\n",
    "            balancing_id[\"manual_frequency\"],\n",
    "            electricity_consumption_id[\"actual\"],\n",
    "            electricity_consumption_id[\"forecast\"],\n",
    "            electricity_generation_id[\"actual\"],\n",
    "            electricity_generation_id[\"forecast\"],\n",
    "            market_id[\"cross_border_physical\"],\n",
    "            market_id[\"scheudled_commercial_exchanges\"],\n",
    "            market_id[\"day_ahead_prices\"]    \n",
    "    ]\n",
    "    \n",
    "final_df = None\n",
    "\n",
    "for i in range(13):\n",
    "    working_df = download(i)\n",
    "    working_df = new_format(working_df, dict_ids[i])\n",
    "\n",
    "    if i > 0:\n",
    "       working_df=working_df.drop(working_df.columns[1],axis=1)\n",
    "        #only called once\n",
    "    if final_df is None:\n",
    "            final_df = working_df\n",
    "    else:\n",
    "        final_df = pd.merge(final_df, working_df, on=working_df.columns[0], how='inner', copy=True)\n",
    "    \n",
    "final_df.to_csv(output_path, sep=',', index=False)\n",
    "\n",
    "#use gzip to compress .csv outputfile to <file_out>.gz\n",
    "path_object = Path(output_path)\n",
    "output_pathgz = path_object.with_suffix('.gz')\n",
    "final_df.to_csv(output_pathgz, sep=',', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is responsible for the weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte den Download für 3 Stationen mit 3 Threads.\n",
      "Speicherort ./weather/computing_folder/01262, computing_folder ./weather/computing_folder, station_id 01262\n",
      "Speicherort ./weather/computing_folder/01975, computing_folder ./weather/computing_folder, station_id 01975\n",
      "Speicherort ./weather/computing_folder/02667, computing_folder ./weather/computing_folder, station_id 02667\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/historical/stundenwerte_TU_01975_19490101_20231231_hist.zipLade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/historical/stundenwerte_TU_02667_19600101_20231231_hist.zip\n",
      "\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/historical/stundenwerte_TU_01262_19920517_20231231_hist.zip\n",
      "Daten geladen für: produkt_tu_stunde_19920517_20231231_01262.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01262\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/recent/stundenwerte_TU_01262_akt.zip\n",
      "Daten geladen für: produkt_tu_stunde_19600101_20231231_02667.txt\n",
      "Daten geladen für: produkt_tu_stunde_20230622_20241222_01262.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01262\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/cloudiness/historical/stundenwerte_N_01262_19920517_20231231_hist.zip\n",
      "Daten geladen für: produkt_tu_stunde_19490101_20231231_01975.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/02667\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/recent/stundenwerte_TU_02667_akt.zip\n",
      "Daten geladen für: produkt_n_stunde_19920517_20231231_01262.txt\n",
      "Daten geladen für: produkt_tu_stunde_20230622_20241222_02667.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/02667\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/cloudiness/historical/stundenwerte_N_02667_19490101_20231231_hist.zip\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01262\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/cloudiness/recent/stundenwerte_N_01262_akt.zip\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01975\n",
      "Daten geladen für: produkt_n_stunde_20230622_20241222_01262.txt\n",
      "Daten geladen für: produkt_n_stunde_19490101_20231231_02667.txt\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/recent/stundenwerte_TU_01975_akt.zip\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01262\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/pressure/historical/stundenwerte_P0_01262_19920517_20231231_hist.zip\n",
      "Daten geladen für: produkt_tu_stunde_20230622_20241222_01975.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01975\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/02667\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/cloudiness/historical/stundenwerte_N_01975_19490101_20231231_hist.zip\n",
      "Daten geladen für: produkt_p0_stunde_19920517_20231231_01262.txt\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/cloudiness/recent/stundenwerte_N_02667_akt.zip\n",
      "Daten geladen für: produkt_n_stunde_20230622_20241222_02667.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/02667\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01262\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/pressure/historical/stundenwerte_P0_02667_19490101_20231231_hist.zip\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/pressure/recent/stundenwerte_P0_01262_akt.zip\n",
      "Daten geladen für: produkt_n_stunde_19490101_20231231_01975.txt\n",
      "Daten geladen für: produkt_p0_stunde_20230622_20241222_01262.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01262\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/sun/historical/stundenwerte_SD_01262_19920519_20231231_hist.zip\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01975\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/cloudiness/recent/stundenwerte_N_01975_akt.zip\n",
      "Daten geladen für: produkt_p0_stunde_19490101_20231231_02667.txt\n",
      "Daten geladen für: produkt_n_stunde_20230622_20241222_01975.txt\n",
      "Daten geladen für: produkt_sd_stunde_19920519_20231231_01262.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01975\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/pressure/historical/stundenwerte_P0_01975_19490101_20231231_hist.zip\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01262\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/02667\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/sun/recent/stundenwerte_SD_01262_akt.zip\n",
      "Daten geladen für: produkt_p0_stunde_19490101_20231231_01975.txt\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/pressure/recent/stundenwerte_P0_02667_akt.zip\n",
      "Daten geladen für: produkt_sd_stunde_20230622_20241222_01262.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01262\n",
      "Daten geladen für: produkt_p0_stunde_20230622_20241222_02667.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/02667\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/wind/historical/stundenwerte_FF_01262_19920519_20231231_hist.zip\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/sun/historical/stundenwerte_SD_02667_19610101_20231231_hist.zip\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01975\n",
      "Daten geladen für: produkt_ff_stunde_19920519_20231231_01262.txt\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/pressure/recent/stundenwerte_P0_01975_akt.zip\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01262\n",
      "Daten geladen für: produkt_sd_stunde_19610101_20231231_02667.txt\n",
      "Daten geladen für: produkt_p0_stunde_20230622_20241222_01975.txt\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/wind/recent/stundenwerte_FF_01262_akt.zip\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01975\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/sun/historical/stundenwerte_SD_01975_19490101_20231231_hist.zip\n",
      "Daten geladen für: produkt_ff_stunde_20230622_20241222_01262.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01262\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/02667\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/sun/recent/stundenwerte_SD_02667_akt.zip\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/recent/stundenwerte_RR_01262_akt.zip\n",
      "Daten geladen für: produkt_sd_stunde_20230622_20241222_02667.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/02667\n",
      "Daten geladen für: produkt_rr_stunde_20230622_20241222_01262.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01262\n",
      "Daten geladen für: produkt_sd_stunde_19490101_20231231_01975.txt\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/wind/historical/stundenwerte_FF_02667_19570701_20231231_hist.zip\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/historical/stundenwerte_RR_01262_19950901_20231231_hist.zip\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01975\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/sun/recent/stundenwerte_SD_01975_akt.zip\n",
      "Daten geladen für: produkt_rr_stunde_19950901_20231231_01262.txt\n",
      "Daten geladen für: produkt_sd_stunde_20230622_20241222_01975.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01975\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/wind/historical/stundenwerte_FF_01975_19500101_20231231_hist.zip\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01262\n",
      "Historisch bis 2015 gekürzt: clouds_01262_hist.txt\n",
      "Daten geladen für: produkt_ff_stunde_19570701_20231231_02667.txt\n",
      "Historisch bis 2015 gekürzt: sun_01262_hist.txt\n",
      "Historisch bis 2015 gekürzt: pressure_01262_hist.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/02667\n",
      "Historisch bis 2015 gekürzt: wind_01262_hist.txt\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/wind/recent/stundenwerte_FF_02667_akt.zip\n",
      "Historisch bis 2015 gekürzt: precipitation_01262_hist.txt\n",
      "Historisch bis 2015 gekürzt: temp_01262_hist.txt\n",
      "Start Remove Columns\n",
      "Spalten aus clouds_01262_hist.txt entfernt.\n",
      "Daten geladen für: produkt_ff_stunde_20230622_20241222_02667.txt\n",
      "Spalten aus clouds_01262_recent.txt entfernt.\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/02667\n",
      "Spalten aus pressure_01262_hist.txt entfernt.\n",
      "Spalten aus pressure_01262_recent.txt entfernt.\n",
      "Spalten aus sun_01262_recent.txt entfernt.\n",
      "Spalten aus sun_01262_hist.txt entfernt.\n",
      "Daten geladen für: produkt_ff_stunde_19500101_20231231_01975.txt\n",
      "Spalten aus temp_01262_recent.txt entfernt.\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/recent/stundenwerte_RR_02667_akt.zip\n",
      "Spalten aus temp_01262_hist.txt entfernt.\n",
      "Daten geladen für: produkt_rr_stunde_20230622_20241222_02667.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/02667\n",
      "Spalten aus wind_01262_hist.txt entfernt.\n",
      "Spalten aus wind_01262_recent.txt entfernt.\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/historical/stundenwerte_RR_02667_19950901_20231231_hist.zip\n",
      "Spalten aus precipitation_01262_hist.txt entfernt.\n",
      "Spalten aus precipitation_01262_recent.txt entfernt.\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01975\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder/01262/sun_01262_combined.txt\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/wind/recent/stundenwerte_FF_01975_akt.zip\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder/01262/clouds_01262_combined.txt\n",
      "Daten geladen für: produkt_rr_stunde_19950901_20231231_02667.txt\n",
      "Daten geladen für: produkt_ff_stunde_20230622_20241222_01975.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder/01262/temp_01262_combined.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01975\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/recent/stundenwerte_RR_01975_akt.zip\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/02667\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder/01262/pressure_01262_combined.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder/01262/wind_01262_combined.txt\n",
      "Daten geladen für: produkt_rr_stunde_20230622_20241222_01975.txt\n",
      "Historisch bis 2015 gekürzt: sun_02667_hist.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01975\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder/01262/precipitation_01262_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder/01262/precipitation_01262_combined.txt\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/historical/stundenwerte_RR_01975_19950905_20231231_hist.zip\n",
      "Daten hinzugefügt von: ./weather/computing_folder/01262/sun_01262_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder/01262/pressure_01262_combined.txt\n",
      "Historisch bis 2015 gekürzt: clouds_02667_hist.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder/01262/temp_01262_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder/01262/clouds_01262_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder/01262/wind_01262_combined.txt\n",
      "Historisch bis 2015 gekürzt: wind_02667_hist.txt\n",
      "Alle kombinierten Daten für Station 01262 gespeichert in: ./weather/stations/01262/01262_data_combined.csv\n",
      "Download abgeschlossen für Station 01262.\n",
      "Daten geladen für: produkt_rr_stunde_19950905_20231231_01975.txt\n",
      "Historisch bis 2015 gekürzt: pressure_02667_hist.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder/01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: /Users/clarahuefner/Desktop/final-submission-main/merged_data/scripts/weather/computing_folder/01975\n",
      "Historisch bis 2015 gekürzt: temp_02667_hist.txt\n",
      "Historisch bis 2015 gekürzt: temp_01975_hist.txt\n",
      "Historisch bis 2015 gekürzt: precipitation_02667_hist.txt\n",
      "Start Remove Columns\n",
      "Historisch bis 2015 gekürzt: precipitation_01975_hist.txt\n",
      "Spalten aus clouds_02667_hist.txt entfernt.\n",
      "Spalten aus clouds_02667_recent.txt entfernt.\n",
      "Spalten aus pressure_02667_hist.txt entfernt.\n",
      "Spalten aus pressure_02667_recent.txt entfernt.\n",
      "Historisch bis 2015 gekürzt: pressure_01975_hist.txt\n",
      "Spalten aus sun_02667_hist.txt entfernt.\n",
      "Spalten aus sun_02667_recent.txt entfernt.\n",
      "Spalten aus temp_02667_hist.txt entfernt.\n",
      "Historisch bis 2015 gekürzt: wind_01975_hist.txt\n",
      "Spalten aus temp_02667_recent.txt entfernt.\n",
      "Spalten aus wind_02667_recent.txt entfernt.\n",
      "Spalten aus wind_02667_hist.txt entfernt.\n",
      "Historisch bis 2015 gekürzt: sun_01975_hist.txt\n",
      "Spalten aus precipitation_02667_recent.txt entfernt.\n",
      "Spalten aus precipitation_02667_hist.txt entfernt.\n",
      "Historisch bis 2015 gekürzt: clouds_01975_hist.txt\n",
      "Start Remove Columns\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder/02667/precipitation_02667_combined.txt\n",
      "Spalten aus clouds_01975_recent.txt entfernt.\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder/02667/sun_02667_combined.txt\n",
      "Spalten aus clouds_01975_hist.txt entfernt.\n",
      "Spalten aus pressure_01975_recent.txt entfernt.\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder/02667/clouds_02667_combined.txt\n",
      "Spalten aus pressure_01975_hist.txt entfernt.\n",
      "Spalten aus sun_01975_recent.txt entfernt.\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder/02667/wind_02667_combined.txt\n",
      "Spalten aus sun_01975_hist.txt entfernt.\n",
      "Spalten aus temp_01975_hist.txt entfernt.\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder/02667/pressure_02667_combined.txt\n",
      "Spalten aus temp_01975_recent.txt entfernt.\n",
      "Spalten aus wind_01975_recent.txt entfernt.\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder/02667/temp_02667_combined.txt\n",
      "Spalten aus wind_01975_hist.txt entfernt.\n",
      "Daten hinzugefügt von: ./weather/computing_folder/02667/precipitation_02667_combined.txt\n",
      "Spalten aus precipitation_01975_recent.txt entfernt.\n",
      "Daten hinzugefügt von: ./weather/computing_folder/02667/sun_02667_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder/02667/pressure_02667_combined.txt\n",
      "Spalten aus precipitation_01975_hist.txt entfernt.\n",
      "Daten hinzugefügt von: ./weather/computing_folder/02667/temp_02667_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder/02667/wind_02667_combined.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder/01975/pressure_01975_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder/02667/clouds_02667_combined.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder/01975/clouds_01975_combined.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder/01975/wind_01975_combined.txt\n",
      "Alle kombinierten Daten für Station 02667 gespeichert in: ./weather/stations/02667/02667_data_combined.csv\n",
      "Download abgeschlossen für Station 02667.\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder/01975/precipitation_01975_combined.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder/01975/temp_01975_combined.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder/01975/sun_01975_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder/01975/temp_01975_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder/01975/pressure_01975_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder/01975/wind_01975_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder/01975/clouds_01975_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder/01975/precipitation_01975_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder/01975/sun_01975_combined.txt\n",
      "Alle kombinierten Daten für Station 01975 gespeichert in: ./weather/stations/01975/01975_data_combined.csv\n",
      "Download abgeschlossen für Station 01975.\n",
      "Ausführungszeit: 27.04599666595459 Sekunden\n",
      "Starte den Download für Station 10870...\n",
      "Die Wettervorhersage wurde in ./weather/computing_folder/weather_forecast_Muenchen.json gespeichert.\n",
      "Die Wettervorhersage wurde in weather_forecast_Muenchen.csv konvertiert\n",
      "\n",
      "Starte den Download für Station 10147...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/2155369691.py:596: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  date = [datetime.utcfromtimestamp((start_time + i * time_step) / 1000) for i in range(len(forecast_data[\"temperature\"]))]\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/2155369691.py:596: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  date = [datetime.utcfromtimestamp((start_time + i * time_step) / 1000) for i in range(len(forecast_data[\"temperature\"]))]\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/2155369691.py:596: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  date = [datetime.utcfromtimestamp((start_time + i * time_step) / 1000) for i in range(len(forecast_data[\"temperature\"]))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Wettervorhersage wurde in ./weather/computing_folder/weather_forecast_Hamburg.json gespeichert.\n",
      "Die Wettervorhersage wurde in weather_forecast_Hamburg.csv konvertiert\n",
      "\n",
      "Starte den Download für Station 10513...\n",
      "Die Wettervorhersage wurde in ./weather/computing_folder/weather_forecast_KoelnBonn.json gespeichert.\n",
      "Die Wettervorhersage wurde in weather_forecast_KoelnBonn.csv konvertiert\n",
      "\n",
      "Starte den Spaltenentfernumg\n",
      "File: ['weather_forecast_Muenchen.csv', 'weather_forecast_KoelnBonn.csv', 'weather_forecast_Hamburg.csv']...\n",
      "Starte den Spaltenentfernumg für weather_forecast_Muenchen.csv...\n",
      "Spalten im DataFrame: ['date', 'T_temperature_C', 'T_temperature_standarddeviation_C', 'precipitationTotal_mm', 'sunshine_min', 'dewPoint2m', 'surfacePressure_hPa', 'humidity_Percent', 'isDay_bool']\n",
      "Spalten aus weather_forecast_Muenchen.csv entfernt.\n",
      "Starte den Spaltenentfernumg für weather_forecast_KoelnBonn.csv...\n",
      "Spalten im DataFrame: ['date', 'T_temperature_C', 'T_temperature_standarddeviation_C', 'precipitationTotal_mm', 'sunshine_min', 'dewPoint2m', 'surfacePressure_hPa', 'humidity_Percent', 'isDay_bool']\n",
      "Spalten aus weather_forecast_KoelnBonn.csv entfernt.\n",
      "Starte den Spaltenentfernumg für weather_forecast_Hamburg.csv...\n",
      "Spalten im DataFrame: ['date', 'T_temperature_C', 'T_temperature_standarddeviation_C', 'precipitationTotal_mm', 'sunshine_min', 'dewPoint2m', 'surfacePressure_hPa', 'humidity_Percent', 'isDay_bool']\n",
      "Spalten aus weather_forecast_Hamburg.csv entfernt.\n",
      "Ausführungszeit: 27.372106075286865 Sekunden\n",
      "Starte die Verknüfung aller Daten für 3 Stationen mit 3 Threads.\n",
      "Kombiniert: 01975 -> ./weather/stations_combined/Hamburg_review.csv\n",
      "Dateien verknüpft aller Daten für Station ('01975', 'Hamburg').\n",
      "Kombiniert: 01262 -> ./weather/stations_combined/Muenchen_review.csv\n",
      "Dateien verknüpft aller Daten für Station ('01262', 'Muenchen').\n",
      "Kombiniert: 02667 -> ./weather/stations_combined/KoelnBonn_review.csv\n",
      "Dateien verknüpft aller Daten für Station ('02667', 'KoelnBonn').\n",
      "Ausführungszeit: 28.243818044662476 Sekunden\n",
      "Spalten umbennant für KoelnBonn_review\n",
      "Spalten umbennant für Hamburg_review\n",
      "Spalten umbennant für Muenchen_review\n",
      "Daten hinzugefügt von: ./weather/stations_combined/KoelnBonn_review.csv\n",
      "Daten hinzugefügt von: ./weather/stations_combined/Hamburg_review.csv\n",
      "Daten hinzugefügt von: ./weather/stations_combined/Muenchen_review.csv\n",
      "Alle kombinierten Daten gespeichert in: ../data_collection/weather.csv\n",
      "Spalten umbennant für weather_forecast_Muenchen\n",
      "Spalten umbennant für weather_forecast_KoelnBonn\n",
      "Spalten umbennant für weather_forecast_Hamburg\n",
      "Daten hinzugefügt von: ./weather/stations/weather_forecast_Muenchen.csv\n",
      "Daten hinzugefügt von: ./weather/stations/weather_forecast_KoelnBonn.csv\n",
      "Daten hinzugefügt von: ./weather/stations/weather_forecast_Hamburg.csv\n",
      "Kombinierter Forecast gespeichert in: ../forecast/weather_forecast.csv\n",
      "Ausführungszeit: 30.32799005508423 Sekunden\n"
     ]
    }
   ],
   "source": [
    "#Inititalisierung von Variablen und Ordnern\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "import io\n",
    "import re\n",
    "import concurrent.futures\n",
    "#Definiere abzufragende Stationen\n",
    "combine_historicforecast_bool =False\n",
    "station_ids_r = [ \"01262\", \"01975\", \"02667\"]\n",
    "station_ids_f = [ \"10870\", \"10147\", \"10513\"]\n",
    "station_place = [ \"Muenchen\", \"Hamburg\", \"KoelnBonn\" ]\n",
    "#Ordnerstruktur für die Brechnung und Ausgabe\n",
    "output_folder = \"./weather/\"\n",
    "station_folder = \"./weather/stations\"\n",
    "computing_folder = \"./weather/computing_folder\"\n",
    "stations_combined = \"./weather/stations_combined\"\n",
    "data_collection_folder=\"../data_collection\"\n",
    "forecas_folder=\"../forecast\"\n",
    "#Basis-URL für die DWD Wetterdaten\n",
    "base_url_review = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/\"\n",
    "url_forecast = \"https://dwd.api.proxy.bund.dev/v30/stationOverviewExtended\"\n",
    "# Nicht benötigte Spalten   \n",
    "columns_remove_clouds = [\"STATIONS_ID\",\"eor\", \"QN_8\",\"V_N_I\"]\n",
    "columns_remove_pressure = [\"STATIONS_ID\",\"eor\", \"QN_8\"]\n",
    "columns_remove_sun = [\"STATIONS_ID\",\"eor\", \"QN_7\"]\n",
    "columns_remove_temp = [\"STATIONS_ID\",\"QN_9\", \"eor\"]\n",
    "columns_remove_wind = [\"STATIONS_ID\",\"eor\", \"QN_3\"]\n",
    "columns_remove_precipitation = [\"STATIONS_ID\",\"eor\", \"QN_8\", \"WRTR\", \"RS_IND\"]\n",
    "\n",
    "columns_remove_forecast = ['isDay','dewPoint2m']\n",
    "#URL Endungen für die Vergangenheit\n",
    "data_types = {\n",
    "    \"temperature_historical\": \"air_temperature/historical/\",\n",
    "    \"temperature_recent\": \"air_temperature/recent/\",\n",
    "    \"cloudiness_historical\": \"cloudiness/historical/\",\n",
    "    \"cloudiness_recent\": \"cloudiness/recent/\",\n",
    "    \"pressure_historical\": \"pressure/historical/\",\n",
    "    \"pressure_recent\": \"pressure/recent/\",\n",
    "    \"sun_historical\": \"sun/historical/\",\n",
    "    \"sun_recent\": \"sun/recent/\",\n",
    "    \"wind_historical\": \"wind/historical/\",\n",
    "    \"wind_recent\": \"wind/recent/\",\n",
    "    \"precipitation_recent\": \"precipitation/recent/\",\n",
    "    \"precipitation_historical\": \"precipitation/historical/\",\n",
    "}\n",
    "#header für Api zugriff \n",
    "headers = {\n",
    "    \"accept\": \"application/json\"\n",
    "}\n",
    "#Definitionen der Funktionen vom Basiswetterskript\n",
    "def combine_historic(station_r, place): \n",
    "  #Kombiniere die Dateien paarweise\n",
    "  try:\n",
    "    file_r = os.path.join(station_folder, station_r, f\"{station_r}_data_combined.csv\")\n",
    "    \n",
    "    #Daten einlesen\n",
    "    df_r = pd.read_csv(file_r)\n",
    "    combined_df=df_r\n",
    "    #Ausgabe-Dateiname\n",
    "    output_file = os.path.join(stations_combined, f\"{place}_review.csv\")\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Kombiniert: {station_r} -> {output_file}\")\n",
    "\n",
    "  except FileNotFoundError as e:\n",
    "    print(f\"Datei nicht gefunden: {e}\")\n",
    "  except Exception as e:\n",
    "    print(f\"Fehler beim Verarbeiten von {station_r}: {e}\")\n",
    "def combine_all_stations():\n",
    "  files = [f for f in os.listdir(stations_combined) if f.endswith('.csv')]\n",
    "\n",
    "  #Umbennenen der Spalten nach Stationsnamen\n",
    "  for file in files:\n",
    "    file_path = os.path.join(stations_combined, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    #Extrahiere den Dateinamen\n",
    "    file_name = os.path.splitext(file)[0]\n",
    "    columname=[df.columns[0]] + [f'{col}_{file_name}' for col in df.columns[1:]]\n",
    "    df.columns = columname\n",
    "    print(f'Spalten umbennant für {file_name}')\n",
    "    #station_column_filename = os.path.join(stations_combined, file_name)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "  #Verbinde alle DataFrames nebeneinander  \n",
    "  all_data_frames = []\n",
    "  for file in files:\n",
    "    file_path = os.path.join(stations_combined, file)  \n",
    "    \n",
    "    #Lade Daten aus Datei und füge sie zur Liste\n",
    "    try:\n",
    "      df = pd.read_csv(file_path, delimiter=\",\", parse_dates=[\"date\"], date_format=\"%Y%m%d%H\")\n",
    "      all_data_frames.append(df)\n",
    "      print(f\"Daten hinzugefügt von: {file_path}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Fehler beim Laden der Datei {file}: {e}\")\n",
    "  \n",
    "  #Wenn geladen wurden -> kombiniere\n",
    "  if all_data_frames:\n",
    "    combined_data = all_data_frames[0]\n",
    "    for df in all_data_frames[1:]:\n",
    "      #Test MESS_DATUM als Datum\n",
    "      df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")                \n",
    "      #Daten zusammenführen\n",
    "      combined_data = pd.merge(combined_data, df, on=[  \"date\"], how=\"outer\")\n",
    "\n",
    "    #Sortieren und doppelte löschen\n",
    "    combined_data = combined_data.sort_values(by=[  \"date\"]).drop_duplicates(subset=[  \"date\"], keep='last')\n",
    "\n",
    "  #Speichern\n",
    "  final_filename = os.path.join(data_collection_folder, f\"weather.csv\")\n",
    "  combined_data.to_csv(final_filename, index=False)\n",
    "  print(f\"Alle kombinierten Daten gespeichert in: {final_filename}\")\n",
    "def start_combine_historic():\n",
    "    max_workers = min(os.cpu_count(), len(station_ids_r))  #Maximal so viele Stationen wie vorhanden oder CPU Anzahl\n",
    "    print(f\"Starte die Verknüfung aller Daten für {len(station_ids_r)} Stationen mit {max_workers} Threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        #Jede Station wird parallel heruntergeladen\n",
    "        future_to_station = {executor.submit(combine_historic, station_r, place): (station_r,  place) for station_r,  place in zip(station_ids_r,  station_place) }    \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()  #Funktion ausführen und Fehler abfangen\n",
    "                print(f\"Dateien verknüpft aller Daten für Station {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Verknüfung aller Daten für Station {station_id}: {e}\")\n",
    "def combine_forecast():\n",
    "\n",
    "  files = [f for f in os.listdir(station_folder) if f.endswith('.csv')]\n",
    "\n",
    "  #Umbennenen der Spalten nach Stationsnamen\n",
    "  for file in files:\n",
    "    file_path = os.path.join(station_folder, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    #Extrahiere den Dateinamen\n",
    "    file_name = os.path.splitext(file)[0]\n",
    "    columname=[df.columns[0]] + [f'{col}_{file_name}' for col in df.columns[1:]]\n",
    "    df.columns = columname\n",
    "    print(f'Spalten umbennant für {file_name}')\n",
    "    #station_column_filename = os.path.join(stations_combined, file_name)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "  #Verbinde alle DataFrames nebeneinander  \n",
    "  all_data_frames = []\n",
    "  for file in files:\n",
    "    file_path = os.path.join(station_folder, file)  \n",
    "    \n",
    "    #Lade Daten aus Datei und füge sie zur Liste\n",
    "    try:\n",
    "      df = pd.read_csv(file_path, delimiter=\",\", parse_dates=[\"date\"], date_format=\"%Y%m%d%H\")\n",
    "      all_data_frames.append(df)\n",
    "      print(f\"Daten hinzugefügt von: {file_path}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Fehler beim Laden der Datei {file}: {e}\")\n",
    "  \n",
    "  #Wenn geladen wurden -> kombiniere\n",
    "  if all_data_frames:\n",
    "    combined_data = all_data_frames[0]\n",
    "    for df in all_data_frames[1:]:\n",
    "      #Test MESS_DATUM als Datum\n",
    "      df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")                \n",
    "      #Daten zusammenführen\n",
    "      combined_data = pd.merge(combined_data, df, on=[  \"date\"], how=\"outer\")\n",
    "\n",
    "    #Sortieren und doppelte löschen\n",
    "    combined_data = combined_data.sort_values(by=[  \"date\"]).drop_duplicates(subset=[  \"date\"], keep='last')\n",
    "\n",
    "\n",
    "  final_filename = os.path.join(forecas_folder, f\"weather_forecast.csv\")\n",
    "  combined_data.to_csv(final_filename, index=False)\n",
    "  print(f\"Kombinierter Forecast gespeichert in: {final_filename}\")\n",
    "def create_folder():\n",
    "  os.makedirs(computing_folder, exist_ok=True)\n",
    "  os.makedirs(stations_combined, exist_ok=True)\n",
    "  for station in station_ids_r:\n",
    "    output_folder_station = os.path.join(computing_folder, station)\n",
    "    os.makedirs(output_folder_station, exist_ok=True)\n",
    "    station_folder =os.path.join(output_folder,'stations',station)\n",
    "    os.makedirs(station_folder, exist_ok=True)\n",
    "\n",
    "#Wetter Reviewfunktionen:\n",
    "#Funktion zur Suche und Herunterladen der Wetterdaten pro Station\n",
    "def station_folderget_weather_data_for_station_review(station_id):\n",
    "    #os.makedirsrs(computing_folder, exist_ok=True)\n",
    "    #os.makedirsrs(station_folder, exist_ok=True)\n",
    "    output_filepath = os.path.join(computing_folder,station_id)\n",
    "    #os.makedirsrs(output_filepath, exist_ok=True)\n",
    "    print(f\"Speicherort {output_filepath}, computing_folder {computing_folder}, station_id {station_id}\")    \n",
    "    for data_type, endpoint in data_types.items():\n",
    "        url = base_url_review + endpoint\n",
    "        \n",
    "        #Esrtellt Liste von Dateien im Verzeichnis\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        #Suche nach passender ZIP-Datei\n",
    "        for line in response.text.splitlines():\n",
    "            if station_id in line and \"zip\" in line:\n",
    "                filename = re.search(r'href=\"(.*?)\"', line).group(1)\n",
    "                file_url = url + filename\n",
    "                \n",
    "                #Lade ZIP-Datei herunter\n",
    "                print(f\"Lade herunter: {file_url}\")\n",
    "                file_response = requests.get(file_url)\n",
    "                file_response.raise_for_status()\n",
    "                \n",
    "                #Entpacke ZIP-Datei und suche passender TXT-Datei in der ZIP\n",
    "                with zipfile.ZipFile(io.BytesIO(file_response.content)) as z:\n",
    "                    if data_type == \"cloudiness_historical\" or data_type == \"cloudiness_recent\":\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_n_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    elif data_type == \"pressure_historical\" or data_type == \"pressure_recent\":\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_p0_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    elif data_type == \"sun_historical\" or data_type == \"sun_recent\":\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_sd_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    elif data_type == \"wind_historical\" or data_type == \"wind_recent\":\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_ff_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    elif data_type == \"precipitation_historical\" or data_type == \"precipitation_recent\":\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_rr_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    else:\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_tu_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    \n",
    "                    if not txt_files:\n",
    "                        print(f\"Keine TXT-Datei im erwarteten Format für Station {station_id} gefunden.\")\n",
    "                        continue  \n",
    "\n",
    "                    #Wenn TXT-Datei gefunden wurde, lade sie in pandas\n",
    "                    txt_filename = txt_files[0]\n",
    "                    with z.open(txt_filename) as f:\n",
    "                        #Test ob ladbar\n",
    "                        try:\n",
    "                            df = pd.read_csv(f, sep=\";\", encoding=\"utf-8\")\n",
    "                            if df.empty:\n",
    "                                print(f\"Warnung: Die Datei {txt_filename} ist leer.\")\n",
    "                            else:\n",
    "                                print(\"Daten geladen für:\", txt_filename)\n",
    "\n",
    "                                #Ausgabeordener checken\n",
    "\n",
    "                                #Dateinamen nach Datenart setzen\n",
    "                                if data_type == \"temperature_historical\":\n",
    "                                    new_filename = f\"temp_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"temperature_recent\":\n",
    "                                    new_filename = f\"temp_{station_id}_recent.txt\"\n",
    "                                elif data_type == \"cloudiness_historical\":\n",
    "                                    new_filename = f\"clouds_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"cloudiness_recent\":\n",
    "                                    new_filename = f\"clouds_{station_id}_recent.txt\"\n",
    "                                elif data_type == \"pressure_historical\":\n",
    "                                    new_filename = f\"pressure_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"pressure_recent\":\n",
    "                                    new_filename = f\"pressure_{station_id}_recent.txt\"\n",
    "                                elif data_type == \"sun_historical\":\n",
    "                                    new_filename = f\"sun_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"sun_recent\":\n",
    "                                    new_filename = f\"sun_{station_id}_recent.txt\"\n",
    "                                elif data_type == \"wind_historical\":\n",
    "                                    new_filename = f\"wind_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"wind_recent\":\n",
    "                                    new_filename = f\"wind_{station_id}_recent.txt\"      \n",
    "                                elif data_type == \"precipitation_historical\":\n",
    "                                    new_filename = f\"precipitation_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"precipitation_recent\":\n",
    "                                    new_filename = f\"precipitation_{station_id}_recent.txt\"                                \n",
    "                                \n",
    "                                #Speichere TXT-Datei im angegebenen Ordner\n",
    "                                #print(f\"Speicherort {output_filepath}, computing_folder {computing_folder}, station_id {station_id}, new_filename {new_filename}\")\n",
    "                                output_filename = os.path.join(output_filepath, new_filename)                                \n",
    "                                df.to_csv(output_filename, sep=\";\", encoding=\"utf-8\", index=False)\n",
    "                                print(f\"Wetterdaten gespeichert unter: {output_filepath}\")   \n",
    "                                print(f\"Die Datei wurde erfolgreich gespeichert unter: {os.path.abspath(output_filepath)}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Fehler beim Laden der Datei {txt_filename}: {e}\")\n",
    "    cut_historic_bevor_2015(station_id)\n",
    "#Funktion zum Herunterladen der Wetterdaten für alle angegebenen Stationen\n",
    "def download_weather_data_for_all_stations_review(station_ids):\n",
    "    max_workers = min(os.cpu_count(), len(station_ids))  #Maximal 10 Threads oder so viele Stationen wie vorhanden\n",
    "    print(f\"Starte den Download für {len(station_ids)} Stationen mit {max_workers} Threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        #Jede Station wird parallel heruntergeladen\n",
    "        future_to_station = {executor.submit(station_folderget_weather_data_for_station_review, station_id): station_id for station_id in station_ids}        \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()  #Funktion ausführen und Fehler abfangen\n",
    "                print(f\"Download abgeschlossen für Station {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Herunterladen von Daten für Station {station_id}: {e}\")\n",
    "def cut_historic_bevor_2015(station_id):\n",
    "    computing_folder_station = os.path.join(computing_folder, station_id)\n",
    "    station_files = [f for f in os.listdir(computing_folder_station) if re.match(r'(.+)_hist\\.txt', f)]    \n",
    "    for file in station_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        with open(file_path, 'r') as infile:\n",
    "            lines = infile.readlines()\n",
    "        \n",
    "        #Filtert Zeilen nach 2015 sind\n",
    "        filtered_lines = []\n",
    "        for line in lines[:1]:\n",
    "            filtered_lines.append(line)\n",
    "        for line in lines[1:]:\n",
    "            columns = line.strip().split(';')\n",
    "            if len(columns) > 1:  \n",
    "                mess_datum = columns[1]\n",
    "                year = int(mess_datum[:4])                \n",
    "                if year >= 2015:\n",
    "                    filtered_lines.append(line)\n",
    "\n",
    "        #Schreibe Zeilen in die Datei zurück\n",
    "        with open(file_path, 'w') as outfile:\n",
    "            outfile.writelines(filtered_lines)\n",
    "        print(f\"Historisch bis 2015 gekürzt: {file}\")\n",
    "    \n",
    "    #Aufruf nur benutzen, wenn start_... in weather nicht ausgeführt wird\n",
    "    remove_columns_review(station_id)\n",
    "def start_cut_historic_bevor_2015(station_ids):\n",
    "    max_workers = min(os.cpu_count(), len(station_ids))  #Maximal 10 Threads oder so viele Stationen wie vorhanden\n",
    "    print(f\"Starte die Kürzung bis 2015 für {len(station_ids)} Stationen mit {max_workers} Threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        #Jede Station wird parallel heruntergeladen\n",
    "        future_to_station = {executor.submit(cut_historic_bevor_2015, station_id): station_id for station_id in station_ids}        \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()  #Funktion ausführen und Fehler abfangen\n",
    "                print(f\"Dateien bis 2015 gekürzt für Station {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Kürzen bis 2015 für Station {station_id}: {e}\")\n",
    "def remove_columns_review(station_id):\n",
    "    print('Start Remove Columns')\n",
    "    computing_folder_station =os.path.join(computing_folder, station_id)\n",
    "    temp_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"temp_\") and f.endswith(\".txt\")]\n",
    "    clouds_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"clouds_\") and f.endswith(\".txt\")]\n",
    "    pressure_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"pressure_\") and f.endswith(\".txt\")]\n",
    "    sun_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"sun_\") and f.endswith(\".txt\")]\n",
    "    wind_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"wind_\") and f.endswith(\".txt\")]\n",
    "    precipitation_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"precipitation_\") and f.endswith(\".txt\")]\n",
    "    \n",
    "    for file in clouds_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True)            \n",
    "            #Entferne die Spalten, ganz oben definiert\n",
    "            df = df.drop(columns=[col for col in columns_remove_clouds if col in df.columns])            \n",
    "            #Speichere modifizierte Datei\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"Spalten aus {file} entfernt.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Datei {file}: {e}\")\n",
    "    \n",
    "    for file in pressure_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True)            \n",
    "            #Entferne die Spalten, ganz oben definiert\n",
    "            df = df.drop(columns=[col for col in columns_remove_pressure if col in df.columns])            \n",
    "            #Speichere modifizierte Datei\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"Spalten aus {file} entfernt.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Datei {file}: {e}\")\n",
    "\n",
    "    for file in sun_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True)            \n",
    "            #Entferne die Spalten, ganz oben definiert\n",
    "            df = df.drop(columns=[col for col in columns_remove_sun if col in df.columns])            \n",
    "            #Speichere modifizierte Datei\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"Spalten aus {file} entfernt.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Datei {file}: {e}\")\n",
    "\n",
    "    for file in temp_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True)            \n",
    "            #Entferne die Spalten, ganz oben definiert\n",
    "            df = df.drop(columns=[col for col in columns_remove_temp if col in df.columns])            \n",
    "            #Speichere modifizierte Datei\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"Spalten aus {file} entfernt.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Datei {file}: {e}\")\n",
    "\n",
    "    for file in wind_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True)            \n",
    "            #Entferne die Spalten, ganz oben definiert\n",
    "            df = df.drop(columns=[col for col in columns_remove_wind if col in df.columns])            \n",
    "            #Speichere modifizierte Datei\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"Spalten aus {file} entfernt.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Datei {file}: {e}\")\n",
    "\n",
    "    for file in precipitation_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True)            \n",
    "            #Entferne die Spalten, ganz oben definiert\n",
    "            df = df.drop(columns=[col for col in columns_remove_precipitation if col in df.columns])            \n",
    "            #Speichere modifizierte Datei\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"Spalten aus {file} entfernt.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Datei {file}: {e}\")\n",
    "\n",
    "    #Aufruf nur benutzen, wenn start_... in weather nicht ausgeführt wird\n",
    "    combine_historic_recent(station_id)\n",
    "def start_remove_columns_review(station_ids):\n",
    "    max_workers = min(os.cpu_count(), len(station_ids))  #Maximal 10 Threads oder so viele Stationen wie vorhanden\n",
    "    print(f\"Starte die Löschen von Spalten für {len(station_ids)} Stationen mit {max_workers} Threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        #Jede Station wird parallel heruntergeladen\n",
    "        future_to_station = {executor.submit(remove_columns_review, station_id): station_id for station_id in station_ids}        \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()  #Funktion ausführen und Fehler abfangen\n",
    "                print(f\"Spalten gelöscht für Station {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Löschen von Spalten für Station {station_id}: {e}\")\n",
    "def combine_historic_recent(station_id):\n",
    "    computing_folder_station = os.path.join(computing_folder, station_id)    \n",
    "    #Suche nach Dateien für jeweilige Station\n",
    "    station_files = [f for f in os.listdir(computing_folder_station) if re.match(r'(.+)_' + station_id + r'_(hist|recent)\\.txt', f)]\n",
    "    \n",
    "    #Gruppiere Dateien nach Wettertyp und Station-ID\n",
    "    file_pairs = {}\n",
    "    for file in station_files:\n",
    "        match = re.match(r'(.+)_' + station_id + r'_(hist|recent)\\.txt', file)\n",
    "        if match:\n",
    "            wettertyp, period = match.groups()  #Wettertyp und Zeitraum\n",
    "            key = f\"{wettertyp}_{station_id}\"\n",
    "            if key not in file_pairs:\n",
    "                file_pairs[key] = {}\n",
    "            file_pairs[key][period] = os.path.join(computing_folder_station, file)\n",
    "\n",
    "    #Führe historische und aktuelle Daten zusammen\n",
    "    for key, file_pair in file_pairs.items():\n",
    "        if 'hist' in file_pair and 'recent' in file_pair:\n",
    "            #Einlesen historische, aktuellen Daten\n",
    "            hist_df = pd.read_csv(file_pair['hist'], delimiter=\";\")\n",
    "            recent_df = pd.read_csv(file_pair['recent'], delimiter=\";\")\n",
    "            hist_df[\"MESS_DATUM\"] = pd.to_datetime(hist_df[\"MESS_DATUM\"], format=\"%Y%m%d%H\", errors=\"coerce\")\n",
    "            recent_df[\"MESS_DATUM\"] = pd.to_datetime(recent_df[\"MESS_DATUM\"], format=\"%Y%m%d%H\", errors=\"coerce\")\n",
    "\n",
    "            #Kombinieren Daten und entferne Duplikaten\n",
    "            combined_df = pd.concat([hist_df, recent_df]).drop_duplicates(subset=[\"MESS_DATUM\"], keep='last')\n",
    "            combined_df = combined_df.sort_values(by=[\"MESS_DATUM\"])\n",
    "            combined_df[\"MESS_DATUM\"] = combined_df[\"MESS_DATUM\"].dt.strftime(\"%Y%m%d%H\")\n",
    "\n",
    "            #Speichern unter kombinierten Namen\n",
    "            combined_filename = os.path.join(computing_folder_station, f\"{key}_combined.txt\")\n",
    "            combined_df.to_csv(combined_filename, sep=\";\", index=False)\n",
    "            print(f\"Kombinierte Datei gespeichert: {combined_filename}\")\n",
    "        else:\n",
    "            print(f\"Fehlende Datei für {key}: entweder historische oder aktuelle Datei fehlt.\")\n",
    "    \n",
    "    #Aufruf nur benutzen, wenn start_... in weather nicht ausgeführt wird\n",
    "    combine_all_station_data_review(station_id)\n",
    "def start_combine_historic_recent(station_ids):\n",
    "    max_workers = min(os.cpu_count(), len(station_ids))  #Maximal 10 Threads oder so viele Stationen wie vorhanden\n",
    "    print(f\"Starte die Verknüfung Historsich mit Aktuell für {len(station_ids)} Stationen mit {max_workers} Threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        #Jede Station wird parallel heruntergeladen\n",
    "        future_to_station = {executor.submit(combine_historic_recent, station_id): station_id for station_id in station_ids}        \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()  #Funktion ausführen und Fehler abfangen\n",
    "                print(f\"Dateien verknüpft Historsich mit Aktuell für Station {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Verknüfung Historsich mit Aktuell für Station {station_id}: {e}\")\n",
    "def combine_all_station_data_review(station_id):\n",
    "    computing_folder_station = os.path.join(computing_folder, station_id)\n",
    "    station_folder_station = os.path.join(station_folder, station_id) \n",
    "    #os.makedirsrs(station_folder_station, exist_ok=True)  \n",
    "    #Suche nach Dateien mit dem Suffix \"_combined\" \n",
    "    combined_files = [f for f in os.listdir(computing_folder_station) if f.endswith(f\"_{station_id}_combined.txt\")]\n",
    "    all_data_frames = []\n",
    "    #print(f\"Combined Files: {combined_files}\")\n",
    "    for file in combined_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        #Lade Daten aus Datei und füge sie zur Liste\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", parse_dates=[\"MESS_DATUM\"], date_format=\"%Y%m%d%H\")\n",
    "            all_data_frames.append(df)\n",
    "            print(f\"Daten hinzugefügt von: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Laden der Datei {file}: {e}\")\n",
    "\n",
    "    #print(\"Alle Dateien geladen\")\n",
    "    #Wenn geladen wurden -> kombiniere\n",
    "    if all_data_frames:\n",
    "        combined_data = all_data_frames[0]\n",
    "        for df in all_data_frames[1:]:\n",
    "            #Test MESS_DATUM als Datum\n",
    "            df[\"MESS_DATUM\"] = pd.to_datetime(df[\"MESS_DATUM\"], format=\"%Y%m%d%H\", errors=\"coerce\")                \n",
    "            #Daten zusammenführen\n",
    "            combined_data = pd.merge(combined_data, df, on=[  \"MESS_DATUM\"], how=\"outer\")\n",
    "\n",
    "        #Sortieren und doppelte löschen\n",
    "        combined_data = combined_data.sort_values(by=[  \"MESS_DATUM\"]).drop_duplicates(subset=[  \"MESS_DATUM\"], keep='last')\n",
    "        combined_data[\"MESS_DATUM\"] = combined_data[\"MESS_DATUM\"].dt.strftime(\"%Y%m%d%H\")\n",
    "        \n",
    "        # Header ändern\n",
    "        header_mapping = {\n",
    "            \"STATIONS_ID\": \"STATIONS_ID\",\n",
    "            \"MESS_DATUM\": \"date\",\n",
    "            \"V_N_I\": \"Wolken_Interp\",\n",
    "            \"V_N\": \"clouds\",\n",
    "            \"P\": \"stationPressure_hPa\",\n",
    "            \"P0\": \"surfacePressure_hPa\",\n",
    "            \"SD_SO\": \"sunshine_min\",\n",
    "            \"TT_TU\": \"T_temperature_C\",\n",
    "            \"RF_TU\": \"humidity_Percent\",\n",
    "            \"F\": \"wind_speed_ms\",\n",
    "            \"D\": \"wind_direction_degree\",\n",
    "            \"R1\": \"precipitationTotal_mm\",\n",
    "            \"RS_IND\": \"precipitation_indicator\"\n",
    "\n",
    "        }\n",
    "    \n",
    "        combined_data.rename(columns=header_mapping, inplace=True)\n",
    "\n",
    "        #Speichern in Datei\n",
    "        final_filename = os.path.join(station_folder_station, f\"{station_id}_data_combined.csv\")\n",
    "        combined_data.to_csv(final_filename, sep=\",\", index=False)\n",
    "        print(f\"Alle kombinierten Daten für Station {station_id} gespeichert in: {final_filename}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Keine kombinierten Dateien für Station {station_id} gefunden.\")\n",
    "def start_combine_all_station_data_review(station_ids):\n",
    "    max_workers = min(os.cpu_count(), len(station_ids))  #Maximal 10 Threads oder so viele Stationen wie vorhanden\n",
    "    print(f\"Starte die Verknüfung aller Daten für {len(station_ids)} Stationen mit {max_workers} Threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        #Jede Station wird parallel heruntergeladen\n",
    "        future_to_station = {executor.submit(combine_all_station_data_review, station_id): station_id for station_id in station_ids}        \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()  #Funktion ausführen und Fehler abfangen\n",
    "                print(f\"Dateien verknüpft aller Daten für Station {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Verknüfung aller Daten für Station {station_id}: {e}\")\n",
    "#Wetter Forecastfunktionen:\n",
    "def get_weather_data_for_station_forecast(station_id, station_place):\n",
    "    params = {\n",
    "        \"stationIds\": station_id\n",
    "    }\n",
    "    #Anfrage vorbereiten\n",
    "    request = requests.Request(\"GET\", url_forecast, headers=headers, params=params)\n",
    "    prepared_request = request.prepare()\n",
    "    \n",
    "    response = requests.Session().send(prepared_request)\n",
    "    #Ausgabeordener checken\n",
    "    #os.makedirs(computing_folder, exist_ok=True)\n",
    "    #os.makedirs(station_folder, exist_ok=True)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        filename = os.path.join(computing_folder, f\"weather_forecast_{station_place}.json\")\n",
    "        with open(filename, \"w\") as file:\n",
    "            json.dump(data, file, indent=4)\n",
    "        print(f\"Die Wettervorhersage wurde in {filename} gespeichert.\")\n",
    "        \n",
    "        #JSON-Daten laden und verarbeiten\n",
    "        with open(filename) as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        for station_id, station_data in data.items():\n",
    "            forecast_data = station_data[\"forecast1\"]\n",
    "            start_time = forecast_data[\"start\"]\n",
    "            time_step = forecast_data[\"timeStep\"]\n",
    "\n",
    "            date = [datetime.utcfromtimestamp((start_time + i * time_step) / 1000) for i in range(len(forecast_data[\"temperature\"]))]\n",
    "            \n",
    "            variables = {\n",
    "                \"T_temperature_C\": forecast_data.get(\"temperature\", []),\n",
    "                \"T_temperature_standarddeviation_C\": forecast_data.get(\"temperatureStd\", []),\n",
    "                \"precipitationTotal_mm\": forecast_data.get(\"precipitationTotal\", []),\n",
    "                \"sunshine_min\": forecast_data.get(\"sunshine\", []),\n",
    "                \"dewPoint2m\": forecast_data.get(\"dewPoint2m\", []),\n",
    "                \"surfacePressure_hPa\": forecast_data.get(\"surfacePressure\", []),\n",
    "                \"humidity_Percent\": forecast_data.get(\"humidity\", []),\n",
    "                \"isDay_bool\": forecast_data.get(\"isDay\", []),\n",
    "                #\"icon\": forecast_data.get(\"icon\", []),\n",
    "                #\"icon1h\": forecast_data.get(\"icon1h\", [])\n",
    "            }\n",
    "            \n",
    "            #Alle Listen auf gleiche Länge bringen\n",
    "            max_length = max(len(date), *(len(values) for values in variables.values()))\n",
    "            date.extend([None] * (max_length - len(date)))  # date auf max. Länge auffüllen\n",
    "            for key, values in variables.items():\n",
    "                variables[key].extend([None] * (max_length - len(values)))  # Werte-Listen auffüllen\n",
    "            \n",
    "            # DataFrame erstellen\n",
    "            df = pd.DataFrame({\n",
    "                \"date\": date,\n",
    "                **variables\n",
    "            })\n",
    "\n",
    "            #DataFrame Temperatur von ZehntelGrad in Grad             \n",
    "            df[\"T_temperature_C\"] = df[\"T_temperature_C\"].apply(lambda x: x / 10 if pd.notnull(x) else x)\n",
    "            df[\"T_temperature_standarddeviation_C\"] = df[\"T_temperature_standarddeviation_C\"].apply(lambda x: x / 10 if pd.notnull(x) else x)\n",
    "            df[\"surfacePressure_hPa\"] = df[\"surfacePressure_hPa\"].apply(lambda x: x / 10 if pd.notnull(x) else x)\n",
    "            df[\"humidity_Percent\"] = df[\"humidity_Percent\"].apply(lambda x: x / 10 if pd.notnull(x) else x)\n",
    "\n",
    "            #Date ins richte Foramt konvertieren\n",
    "            df[\"date\"] = df[\"date\"].apply(lambda x: x.strftime(\"%Y%m%d%H\"))\n",
    "\n",
    "            df.to_csv(os.path.join(station_folder, f\"weather_forecast_{station_place}.csv\"), index=False)\n",
    "            print(f\"Die Wettervorhersage wurde in weather_forecast_{station_place}.csv konvertiert\")\n",
    "    else:\n",
    "        print(f\"Fehler bei der Anfrage: {response.status_code}\")\n",
    "def download_weatherforecast_data_for_all_stations_forecast(station_ids, station_places):\n",
    "    for (station_id , station_place) in zip(station_ids, station_places):\n",
    "        print(f\"Starte den Download für Station {station_id}...\")\n",
    "        get_weather_data_for_station_forecast(station_id, station_place)\n",
    "        print()\n",
    "def remove_columns_forecast():\n",
    "    print(\"Starte den Spaltenentfernumg\")\n",
    "    forecast_files = [f for f in os.listdir(station_folder) if f.startswith(\"weather_forecast_\")]  \n",
    "    print(f\"File: {forecast_files}...\")\n",
    "    for file in forecast_files:\n",
    "        print(f\"Starte den Spaltenentfernumg für {file}...\")\n",
    "        file_path = os.path.join(station_folder, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\",\", skipinitialspace=True)  \n",
    "            print(f\"Spalten im DataFrame: {list(df.columns)}\")          \n",
    "            #Entferne die Spalten, ganz oben definiert\n",
    "            df = df.drop(columns=[col for col in columns_remove_forecast if col in df.columns])     \n",
    "            #Speichere modifizierte Datei\n",
    "            df.to_csv(file_path, sep=\",\", index=False)\n",
    "            print(f\"Spalten aus {file} entfernt.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Datei {file}: {e}\")\n",
    "#Starte Download und Verarbeitung der Wetterdaten\n",
    "start = time.time()\n",
    "#Erstelle die Ordner\n",
    "create_folder()\n",
    "\n",
    "#Starte Rückblick-Download\n",
    "download_weather_data_for_all_stations_review(station_ids_r)\n",
    "\n",
    "end = time.time()\n",
    "verstrichene_zeit = end - start\n",
    "print(f'Ausführungszeit: {verstrichene_zeit} Sekunden')\n",
    "\n",
    "#Starte Vorhersagen-Download\n",
    "download_weatherforecast_data_for_all_stations_forecast(station_ids_f, station_place)\n",
    "remove_columns_forecast()\n",
    "\n",
    "end = time.time()\n",
    "verstrichene_zeit = end - start\n",
    "print(f'Ausführungszeit: {verstrichene_zeit} Sekunden')\n",
    "\n",
    "#Kombiniere die historischen und vorhergesagten Daten\n",
    "start_combine_historic()\n",
    "end = time.time()\n",
    "verstrichene_zeit = end - start\n",
    "print(f'Ausführungszeit: {verstrichene_zeit} Sekunden')\n",
    "#Alle Stationen kombinieren\n",
    "combine_all_stations()\n",
    "combine_forecast()\n",
    "\n",
    "end = time.time()\n",
    "verstrichene_zeit = end - start\n",
    "print(f'Ausführungszeit: {verstrichene_zeit} Sekunden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/513469211.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
      "/var/folders/km/2p9w9k150f7ddl_dd2zttlg00000gn/T/ipykernel_14891/513469211.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d %H:%M:%S')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been merged and saved.\n"
     ]
    }
   ],
   "source": [
    "df_res = pd.read_csv('../data_collection/merged_data.csv')\n",
    "df_ens = pd.read_csv('../data_collection/merged_data3.csv')\n",
    "df_smard = pd.read_csv('../data_collection/smard.csv')\n",
    "df_smard = df_smard.rename(columns={'Start_Date': 'Date'})\n",
    "df.to_csv('../data_collection/smard.csv', index=False)\n",
    "df_smard = pd.read_csv('../data_collection/smard.csv')\n",
    "df_smard['Date'] = pd.to_datetime(df['Date'])\n",
    "df_filtered = df[df['Date'].dt.minute == 0]\n",
    "df_filtered['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_filtered.to_csv('../data_collection/smard.csv', index=False)\n",
    "df_smard = pd.read_csv('../data_collection/smard.csv')\n",
    "\n",
    "df_weather = pd.read_csv('../data_collection/weather.csv')\n",
    "df_weather = df_weather.rename(columns={'date': 'Date'})\n",
    "df.to_csv('../data_collection/weather.csv', index=False)\n",
    "df_weather = pd.read_csv('../data_collection/weather.csv')\n",
    "df_weather['Date'] = pd.to_datetime(df['Date'])\n",
    "df_filtered = df[df['Date'].dt.minute == 0]\n",
    "df_filtered['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_filtered.to_csv('../data_collection/weather.csv', index=False)\n",
    "df_weather = pd.read_csv('../data_collection/weather.csv')\n",
    "df_covid = pd.read_csv('../data_collection/covid.csv')\n",
    "\n",
    "df_social = pd.read_csv('../data_collection/major_social_events.csv')\n",
    "df_carbon = pd.read_csv('../data_collection/carbon_price_forward_filled.csv')\n",
    "\n",
    "merge_big = pd.merge(df_ens, df_res, on='Date', how='outer')\n",
    "merge_big = pd.merge(merge_big, df_smard, on='Date', how='outer')\n",
    "merge_big = pd.merge(merge_big, df_social, on='Date', how='outer')\n",
    "merge_big = pd.merge(merge_big, df_carbon, on='Date', how='outer')\n",
    "merge_big = pd.merge(merge_big, df_weather, on='Date', how='outer')\n",
    "merge_big = pd.merge(merge_big, df_covid, on='Date', how='outer')\n",
    "merge_big.to_csv('../allData.csv', index=False)\n",
    "\n",
    "print(\"CSV files have been merged and saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
