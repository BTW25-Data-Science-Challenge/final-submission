{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inititalisierung von Variablen und Ordnern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "import io\n",
    "import re\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definiere abzufragende Stationen\n",
    "combine_historicforecast_bool =False\n",
    "station_ids_r = [ \"01262\", \"01975\", \"02667\"]\n",
    "station_ids_f = [ \"10870\", \"10147\", \"10513\"]\n",
    "station_place = [ \"Muenchen\", \"Hamburg\", \"KoelnBonn\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordnerstruktur für die Brechnung und Ausgabe\n",
    "output_folder = \"./weather/\"\n",
    "station_folder = \"./weather/stations\"\n",
    "computing_folder = \"./weather/computing_folder\"\n",
    "stations_combined = \"./weather/stations_combined\"\n",
    "data_collection_folder=\"../data_collection\"\n",
    "forecas_folder=\"../forecast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basis-URL für die DWD Wetterdaten\n",
    "base_url_review = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/\"\n",
    "url_forecast = \"https://dwd.api.proxy.bund.dev/v30/stationOverviewExtended\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicht benötigte Spalten   \n",
    "columns_remove_clouds = [\"STATIONS_ID\",\"eor\", \"QN_8\",\"V_N_I\"]\n",
    "columns_remove_pressure = [\"STATIONS_ID\",\"eor\", \"QN_8\"]\n",
    "columns_remove_sun = [\"STATIONS_ID\",\"eor\", \"QN_7\"]\n",
    "columns_remove_temp = [\"STATIONS_ID\",\"QN_9\", \"eor\"]\n",
    "columns_remove_wind = [\"STATIONS_ID\",\"eor\", \"QN_3\"]\n",
    "columns_remove_precipitation = [\"STATIONS_ID\",\"eor\", \"QN_8\", \"WRTR\", \"RS_IND\"]\n",
    "\n",
    "columns_remove_forecast = ['isDay','dewPoint2m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL Endungen für die Vergangenheit\n",
    "data_types = {\n",
    "    \"temperature_historical\": \"air_temperature/historical/\",\n",
    "    \"temperature_recent\": \"air_temperature/recent/\",\n",
    "    \"cloudiness_historical\": \"cloudiness/historical/\",\n",
    "    \"cloudiness_recent\": \"cloudiness/recent/\",\n",
    "    \"pressure_historical\": \"pressure/historical/\",\n",
    "    \"pressure_recent\": \"pressure/recent/\",\n",
    "    \"sun_historical\": \"sun/historical/\",\n",
    "    \"sun_recent\": \"sun/recent/\",\n",
    "    \"wind_historical\": \"wind/historical/\",\n",
    "    \"wind_recent\": \"wind/recent/\",\n",
    "    \"precipitation_recent\": \"precipitation/recent/\",\n",
    "    \"precipitation_historical\": \"precipitation/historical/\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#header für Api zugriff \n",
    "headers = {\n",
    "    \"accept\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitionen der Funktionen vom Basiswetterskript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_historic(station_r, place): \n",
    "  #Kombiniere die Dateien paarweise\n",
    "  try:\n",
    "    file_r = os.path.join(station_folder, station_r, f\"{station_r}_data_combined.csv\")\n",
    "    \n",
    "    #Daten einlesen\n",
    "    df_r = pd.read_csv(file_r)\n",
    "    combined_df=df_r\n",
    "    #Ausgabe-Dateiname\n",
    "    output_file = os.path.join(stations_combined, f\"{place}_review.csv\")\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Kombiniert: {station_r} -> {output_file}\")\n",
    "\n",
    "  except FileNotFoundError as e:\n",
    "    print(f\"Datei nicht gefunden: {e}\")\n",
    "  except Exception as e:\n",
    "    print(f\"Fehler beim Verarbeiten von {station_r}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_all_stations():\n",
    "  files = [f for f in os.listdir(stations_combined) if f.endswith('.csv')]\n",
    "\n",
    "  #Umbennenen der Spalten nach Stationsnamen\n",
    "  for file in files:\n",
    "    file_path = os.path.join(stations_combined, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    #Extrahiere den Dateinamen\n",
    "    file_name = os.path.splitext(file)[0]\n",
    "    columname=[df.columns[0]] + [f'{col}_{file_name}' for col in df.columns[1:]]\n",
    "    df.columns = columname\n",
    "    print(f'Spalten umbennant für {file_name}')\n",
    "    #station_column_filename = os.path.join(stations_combined, file_name)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "  #Verbinde alle DataFrames nebeneinander  \n",
    "  all_data_frames = []\n",
    "  for file in files:\n",
    "    file_path = os.path.join(stations_combined, file)  \n",
    "    \n",
    "    #Lade Daten aus Datei und füge sie zur Liste\n",
    "    try:\n",
    "      df = pd.read_csv(file_path, delimiter=\",\", parse_dates=[\"date\"], date_format=\"%Y%m%d%H\")\n",
    "      all_data_frames.append(df)\n",
    "      print(f\"Daten hinzugefügt von: {file_path}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Fehler beim Laden der Datei {file}: {e}\")\n",
    "  \n",
    "  #Wenn geladen wurden -> kombiniere\n",
    "  if all_data_frames:\n",
    "    combined_data = all_data_frames[0]\n",
    "    for df in all_data_frames[1:]:\n",
    "      #Test MESS_DATUM als Datum\n",
    "      df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")                \n",
    "      #Daten zusammenführen\n",
    "      combined_data = pd.merge(combined_data, df, on=[  \"date\"], how=\"outer\")\n",
    "\n",
    "    #Sortieren und doppelte löschen\n",
    "    combined_data = combined_data.sort_values(by=[  \"date\"]).drop_duplicates(subset=[  \"date\"], keep='last')\n",
    "\n",
    "  #Speichern\n",
    "  final_filename = os.path.join(data_collection_folder, f\"weather.csv\")\n",
    "  combined_data.to_csv(final_filename, index=False)\n",
    "  print(f\"Alle kombinierten Daten gespeichert in: {final_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_combine_historic():\n",
    "    max_workers = min(os.cpu_count(), len(station_ids_r))  #Maximal so viele Stationen wie vorhanden oder CPU Anzahl\n",
    "    print(f\"Starte die Verknüfung aller Daten für {len(station_ids_r)} Stationen mit {max_workers} Threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        #Jede Station wird parallel heruntergeladen\n",
    "        future_to_station = {executor.submit(combine_historic, station_r, place): (station_r,  place) for station_r,  place in zip(station_ids_r,  station_place) }    \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()  #Funktion ausführen und Fehler abfangen\n",
    "                print(f\"Dateien verknüpft aller Daten für Station {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Verknüfung aller Daten für Station {station_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_forecast():\n",
    "\n",
    "  files = [f for f in os.listdir(station_folder) if f.endswith('.csv')]\n",
    "\n",
    "  #Umbennenen der Spalten nach Stationsnamen\n",
    "  for file in files:\n",
    "    file_path = os.path.join(station_folder, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    #Extrahiere den Dateinamen\n",
    "    file_name = os.path.splitext(file)[0]\n",
    "    columname=[df.columns[0]] + [f'{col}_{file_name}' for col in df.columns[1:]]\n",
    "    df.columns = columname\n",
    "    print(f'Spalten umbennant für {file_name}')\n",
    "    #station_column_filename = os.path.join(stations_combined, file_name)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "  #Verbinde alle DataFrames nebeneinander  \n",
    "  all_data_frames = []\n",
    "  for file in files:\n",
    "    file_path = os.path.join(station_folder, file)  \n",
    "    \n",
    "    #Lade Daten aus Datei und füge sie zur Liste\n",
    "    try:\n",
    "      df = pd.read_csv(file_path, delimiter=\",\", parse_dates=[\"date\"], date_format=\"%Y%m%d%H\")\n",
    "      all_data_frames.append(df)\n",
    "      print(f\"Daten hinzugefügt von: {file_path}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Fehler beim Laden der Datei {file}: {e}\")\n",
    "  \n",
    "  #Wenn geladen wurden -> kombiniere\n",
    "  if all_data_frames:\n",
    "    combined_data = all_data_frames[0]\n",
    "    for df in all_data_frames[1:]:\n",
    "      #Test MESS_DATUM als Datum\n",
    "      df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")                \n",
    "      #Daten zusammenführen\n",
    "      combined_data = pd.merge(combined_data, df, on=[  \"date\"], how=\"outer\")\n",
    "\n",
    "    #Sortieren und doppelte löschen\n",
    "    combined_data = combined_data.sort_values(by=[  \"date\"]).drop_duplicates(subset=[  \"date\"], keep='last')\n",
    "\n",
    "\n",
    "  final_filename = os.path.join(forecas_folder, f\"weather_forecast.csv\")\n",
    "  combined_data.to_csv(final_filename, index=False)\n",
    "  print(f\"Kombinierter Forecast gespeichert in: {final_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder():\n",
    "  os.makedirs(computing_folder, exist_ok=True)\n",
    "  os.makedirs(stations_combined, exist_ok=True)\n",
    "  for station in station_ids_r:\n",
    "    output_folder_station = os.path.join(computing_folder, station)\n",
    "    os.makedirs(output_folder_station, exist_ok=True)\n",
    "    station_folder =os.path.join(output_folder,'stations',station)\n",
    "    os.makedirs(station_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wetter Reviewfunktionen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funktion zur Suche und Herunterladen der Wetterdaten pro Station\n",
    "def station_folderget_weather_data_for_station_review(station_id):\n",
    "    #os.makedirsrs(computing_folder, exist_ok=True)\n",
    "    #os.makedirsrs(station_folder, exist_ok=True)\n",
    "    output_filepath = os.path.join(computing_folder,station_id)\n",
    "    #os.makedirsrs(output_filepath, exist_ok=True)\n",
    "    print(f\"Speicherort {output_filepath}, computing_folder {computing_folder}, station_id {station_id}\")    \n",
    "    for data_type, endpoint in data_types.items():\n",
    "        url = base_url_review + endpoint\n",
    "        \n",
    "        #Esrtellt Liste von Dateien im Verzeichnis\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        #Suche nach passender ZIP-Datei\n",
    "        for line in response.text.splitlines():\n",
    "            if station_id in line and \"zip\" in line:\n",
    "                filename = re.search(r'href=\"(.*?)\"', line).group(1)\n",
    "                file_url = url + filename\n",
    "                \n",
    "                #Lade ZIP-Datei herunter\n",
    "                print(f\"Lade herunter: {file_url}\")\n",
    "                file_response = requests.get(file_url)\n",
    "                file_response.raise_for_status()\n",
    "                \n",
    "                #Entpacke ZIP-Datei und suche passender TXT-Datei in der ZIP\n",
    "                with zipfile.ZipFile(io.BytesIO(file_response.content)) as z:\n",
    "                    if data_type == \"cloudiness_historical\" or data_type == \"cloudiness_recent\":\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_n_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    elif data_type == \"pressure_historical\" or data_type == \"pressure_recent\":\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_p0_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    elif data_type == \"sun_historical\" or data_type == \"sun_recent\":\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_sd_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    elif data_type == \"wind_historical\" or data_type == \"wind_recent\":\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_ff_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    elif data_type == \"precipitation_historical\" or data_type == \"precipitation_recent\":\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_rr_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    else:\n",
    "                        txt_files = [name for name in z.namelist() if re.match(r'produkt_tu_stunde_\\d{8}_\\d{8}_' + station_id + r'\\.txt', name)]\n",
    "                    \n",
    "                    if not txt_files:\n",
    "                        print(f\"Keine TXT-Datei im erwarteten Format für Station {station_id} gefunden.\")\n",
    "                        continue  \n",
    "\n",
    "                    #Wenn TXT-Datei gefunden wurde, lade sie in pandas\n",
    "                    txt_filename = txt_files[0]\n",
    "                    with z.open(txt_filename) as f:\n",
    "                        #Test ob ladbar\n",
    "                        try:\n",
    "                            df = pd.read_csv(f, sep=\";\", encoding=\"utf-8\")\n",
    "                            if df.empty:\n",
    "                                print(f\"Warnung: Die Datei {txt_filename} ist leer.\")\n",
    "                            else:\n",
    "                                print(\"Daten geladen für:\", txt_filename)\n",
    "\n",
    "                                #Ausgabeordener checken\n",
    "\n",
    "                                #Dateinamen nach Datenart setzen\n",
    "                                if data_type == \"temperature_historical\":\n",
    "                                    new_filename = f\"temp_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"temperature_recent\":\n",
    "                                    new_filename = f\"temp_{station_id}_recent.txt\"\n",
    "                                elif data_type == \"cloudiness_historical\":\n",
    "                                    new_filename = f\"clouds_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"cloudiness_recent\":\n",
    "                                    new_filename = f\"clouds_{station_id}_recent.txt\"\n",
    "                                elif data_type == \"pressure_historical\":\n",
    "                                    new_filename = f\"pressure_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"pressure_recent\":\n",
    "                                    new_filename = f\"pressure_{station_id}_recent.txt\"\n",
    "                                elif data_type == \"sun_historical\":\n",
    "                                    new_filename = f\"sun_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"sun_recent\":\n",
    "                                    new_filename = f\"sun_{station_id}_recent.txt\"\n",
    "                                elif data_type == \"wind_historical\":\n",
    "                                    new_filename = f\"wind_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"wind_recent\":\n",
    "                                    new_filename = f\"wind_{station_id}_recent.txt\"      \n",
    "                                elif data_type == \"precipitation_historical\":\n",
    "                                    new_filename = f\"precipitation_{station_id}_hist.txt\"\n",
    "                                elif data_type == \"precipitation_recent\":\n",
    "                                    new_filename = f\"precipitation_{station_id}_recent.txt\"                                \n",
    "                                \n",
    "                                #Speichere TXT-Datei im angegebenen Ordner\n",
    "                                #print(f\"Speicherort {output_filepath}, computing_folder {computing_folder}, station_id {station_id}, new_filename {new_filename}\")\n",
    "                                output_filename = os.path.join(output_filepath, new_filename)                                \n",
    "                                df.to_csv(output_filename, sep=\";\", encoding=\"utf-8\", index=False)\n",
    "                                print(f\"Wetterdaten gespeichert unter: {output_filepath}\")   \n",
    "                                print(f\"Die Datei wurde erfolgreich gespeichert unter: {os.path.abspath(output_filepath)}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Fehler beim Laden der Datei {txt_filename}: {e}\")\n",
    "    cut_historic_bevor_2015(station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funktion zum Herunterladen der Wetterdaten für alle angegebenen Stationen\n",
    "def download_weather_data_for_all_stations_review(station_ids):\n",
    "    max_workers = min(os.cpu_count(), len(station_ids))  #Maximal 10 Threads oder so viele Stationen wie vorhanden\n",
    "    print(f\"Starte den Download für {len(station_ids)} Stationen mit {max_workers} Threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        #Jede Station wird parallel heruntergeladen\n",
    "        future_to_station = {executor.submit(station_folderget_weather_data_for_station_review, station_id): station_id for station_id in station_ids}        \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()  #Funktion ausführen und Fehler abfangen\n",
    "                print(f\"Download abgeschlossen für Station {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Herunterladen von Daten für Station {station_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_historic_bevor_2015(station_id):\n",
    "    computing_folder_station = os.path.join(computing_folder, station_id)\n",
    "    station_files = [f for f in os.listdir(computing_folder_station) if re.match(r'(.+)_hist\\.txt', f)]    \n",
    "    for file in station_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        with open(file_path, 'r') as infile:\n",
    "            lines = infile.readlines()\n",
    "        \n",
    "        #Filtert Zeilen nach 2015 sind\n",
    "        filtered_lines = []\n",
    "        for line in lines[:1]:\n",
    "            filtered_lines.append(line)\n",
    "        for line in lines[1:]:\n",
    "            columns = line.strip().split(';')\n",
    "            if len(columns) > 1:  \n",
    "                mess_datum = columns[1]\n",
    "                year = int(mess_datum[:4])                \n",
    "                if year >= 2015:\n",
    "                    filtered_lines.append(line)\n",
    "\n",
    "        #Schreibe Zeilen in die Datei zurück\n",
    "        with open(file_path, 'w') as outfile:\n",
    "            outfile.writelines(filtered_lines)\n",
    "        print(f\"Historisch bis 2015 gekürzt: {file}\")\n",
    "    \n",
    "    #Aufruf nur benutzen, wenn start_... in weather nicht ausgeführt wird\n",
    "    remove_columns_review(station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_cut_historic_bevor_2015(station_ids):\n",
    "    max_workers = min(os.cpu_count(), len(station_ids))  #Maximal 10 Threads oder so viele Stationen wie vorhanden\n",
    "    print(f\"Starte die Kürzung bis 2015 für {len(station_ids)} Stationen mit {max_workers} Threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        #Jede Station wird parallel heruntergeladen\n",
    "        future_to_station = {executor.submit(cut_historic_bevor_2015, station_id): station_id for station_id in station_ids}        \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()  #Funktion ausführen und Fehler abfangen\n",
    "                print(f\"Dateien bis 2015 gekürzt für Station {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Kürzen bis 2015 für Station {station_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns_review(station_id):\n",
    "    print('Start Remove Columns')\n",
    "    computing_folder_station =os.path.join(computing_folder, station_id)\n",
    "    temp_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"temp_\") and f.endswith(\".txt\")]\n",
    "    clouds_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"clouds_\") and f.endswith(\".txt\")]\n",
    "    pressure_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"pressure_\") and f.endswith(\".txt\")]\n",
    "    sun_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"sun_\") and f.endswith(\".txt\")]\n",
    "    wind_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"wind_\") and f.endswith(\".txt\")]\n",
    "    precipitation_files = [f for f in os.listdir(computing_folder_station) if f.startswith(\"precipitation_\") and f.endswith(\".txt\")]\n",
    "    \n",
    "    for file in clouds_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True)            \n",
    "            #Entferne die Spalten, ganz oben definiert\n",
    "            df = df.drop(columns=[col for col in columns_remove_clouds if col in df.columns])            \n",
    "            #Speichere modifizierte Datei\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"Spalten aus {file} entfernt.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Datei {file}: {e}\")\n",
    "    \n",
    "    for file in pressure_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True)            \n",
    "            #Entferne die Spalten, ganz oben definiert\n",
    "            df = df.drop(columns=[col for col in columns_remove_pressure if col in df.columns])            \n",
    "            #Speichere modifizierte Datei\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"Spalten aus {file} entfernt.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Datei {file}: {e}\")\n",
    "\n",
    "    for file in sun_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True)            \n",
    "            #Entferne die Spalten, ganz oben definiert\n",
    "            df = df.drop(columns=[col for col in columns_remove_sun if col in df.columns])            \n",
    "            #Speichere modifizierte Datei\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"Spalten aus {file} entfernt.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Datei {file}: {e}\")\n",
    "\n",
    "    for file in temp_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True)            \n",
    "            #Entferne die Spalten, ganz oben definiert\n",
    "            df = df.drop(columns=[col for col in columns_remove_temp if col in df.columns])            \n",
    "            #Speichere modifizierte Datei\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"Spalten aus {file} entfernt.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Datei {file}: {e}\")\n",
    "\n",
    "    for file in wind_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True)            \n",
    "            #Entferne die Spalten, ganz oben definiert\n",
    "            df = df.drop(columns=[col for col in columns_remove_wind if col in df.columns])            \n",
    "            #Speichere modifizierte Datei\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"Spalten aus {file} entfernt.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Datei {file}: {e}\")\n",
    "\n",
    "    for file in precipitation_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", skipinitialspace=True)            \n",
    "            #Entferne die Spalten, ganz oben definiert\n",
    "            df = df.drop(columns=[col for col in columns_remove_precipitation if col in df.columns])            \n",
    "            #Speichere modifizierte Datei\n",
    "            df.to_csv(file_path, sep=\";\", index=False)\n",
    "            print(f\"Spalten aus {file} entfernt.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Datei {file}: {e}\")\n",
    "\n",
    "    #Aufruf nur benutzen, wenn start_... in weather nicht ausgeführt wird\n",
    "    combine_historic_recent(station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_remove_columns_review(station_ids):\n",
    "    max_workers = min(os.cpu_count(), len(station_ids))  #Maximal 10 Threads oder so viele Stationen wie vorhanden\n",
    "    print(f\"Starte die Löschen von Spalten für {len(station_ids)} Stationen mit {max_workers} Threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        #Jede Station wird parallel heruntergeladen\n",
    "        future_to_station = {executor.submit(remove_columns_review, station_id): station_id for station_id in station_ids}        \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()  #Funktion ausführen und Fehler abfangen\n",
    "                print(f\"Spalten gelöscht für Station {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Löschen von Spalten für Station {station_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_historic_recent(station_id):\n",
    "    computing_folder_station = os.path.join(computing_folder, station_id)    \n",
    "    #Suche nach Dateien für jeweilige Station\n",
    "    station_files = [f for f in os.listdir(computing_folder_station) if re.match(r'(.+)_' + station_id + r'_(hist|recent)\\.txt', f)]\n",
    "    \n",
    "    #Gruppiere Dateien nach Wettertyp und Station-ID\n",
    "    file_pairs = {}\n",
    "    for file in station_files:\n",
    "        match = re.match(r'(.+)_' + station_id + r'_(hist|recent)\\.txt', file)\n",
    "        if match:\n",
    "            wettertyp, period = match.groups()  #Wettertyp und Zeitraum\n",
    "            key = f\"{wettertyp}_{station_id}\"\n",
    "            if key not in file_pairs:\n",
    "                file_pairs[key] = {}\n",
    "            file_pairs[key][period] = os.path.join(computing_folder_station, file)\n",
    "\n",
    "    #Führe historische und aktuelle Daten zusammen\n",
    "    for key, file_pair in file_pairs.items():\n",
    "        if 'hist' in file_pair and 'recent' in file_pair:\n",
    "            #Einlesen historische, aktuellen Daten\n",
    "            hist_df = pd.read_csv(file_pair['hist'], delimiter=\";\")\n",
    "            recent_df = pd.read_csv(file_pair['recent'], delimiter=\";\")\n",
    "            hist_df[\"MESS_DATUM\"] = pd.to_datetime(hist_df[\"MESS_DATUM\"], format=\"%Y%m%d%H\", errors=\"coerce\")\n",
    "            recent_df[\"MESS_DATUM\"] = pd.to_datetime(recent_df[\"MESS_DATUM\"], format=\"%Y%m%d%H\", errors=\"coerce\")\n",
    "\n",
    "            #Kombinieren Daten und entferne Duplikaten\n",
    "            combined_df = pd.concat([hist_df, recent_df]).drop_duplicates(subset=[\"MESS_DATUM\"], keep='last')\n",
    "            combined_df = combined_df.sort_values(by=[\"MESS_DATUM\"])\n",
    "            combined_df[\"MESS_DATUM\"] = combined_df[\"MESS_DATUM\"].dt.strftime(\"%Y%m%d%H\")\n",
    "\n",
    "            #Speichern unter kombinierten Namen\n",
    "            combined_filename = os.path.join(computing_folder_station, f\"{key}_combined.txt\")\n",
    "            combined_df.to_csv(combined_filename, sep=\";\", index=False)\n",
    "            print(f\"Kombinierte Datei gespeichert: {combined_filename}\")\n",
    "        else:\n",
    "            print(f\"Fehlende Datei für {key}: entweder historische oder aktuelle Datei fehlt.\")\n",
    "    \n",
    "    #Aufruf nur benutzen, wenn start_... in weather nicht ausgeführt wird\n",
    "    combine_all_station_data_review(station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_combine_historic_recent(station_ids):\n",
    "    max_workers = min(os.cpu_count(), len(station_ids))  #Maximal 10 Threads oder so viele Stationen wie vorhanden\n",
    "    print(f\"Starte die Verknüfung Historsich mit Aktuell für {len(station_ids)} Stationen mit {max_workers} Threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        #Jede Station wird parallel heruntergeladen\n",
    "        future_to_station = {executor.submit(combine_historic_recent, station_id): station_id for station_id in station_ids}        \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()  #Funktion ausführen und Fehler abfangen\n",
    "                print(f\"Dateien verknüpft Historsich mit Aktuell für Station {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Verknüfung Historsich mit Aktuell für Station {station_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_all_station_data_review(station_id):\n",
    "    computing_folder_station = os.path.join(computing_folder, station_id)\n",
    "    station_folder_station = os.path.join(station_folder, station_id) \n",
    "    #os.makedirsrs(station_folder_station, exist_ok=True)  \n",
    "    #Suche nach Dateien mit dem Suffix \"_combined\" \n",
    "    combined_files = [f for f in os.listdir(computing_folder_station) if f.endswith(f\"_{station_id}_combined.txt\")]\n",
    "    all_data_frames = []\n",
    "    #print(f\"Combined Files: {combined_files}\")\n",
    "    for file in combined_files:\n",
    "        file_path = os.path.join(computing_folder_station, file)\n",
    "        \n",
    "        #Lade Daten aus Datei und füge sie zur Liste\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\";\", parse_dates=[\"MESS_DATUM\"], date_format=\"%Y%m%d%H\")\n",
    "            all_data_frames.append(df)\n",
    "            print(f\"Daten hinzugefügt von: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Laden der Datei {file}: {e}\")\n",
    "\n",
    "    #print(\"Alle Dateien geladen\")\n",
    "    #Wenn geladen wurden -> kombiniere\n",
    "    if all_data_frames:\n",
    "        combined_data = all_data_frames[0]\n",
    "        for df in all_data_frames[1:]:\n",
    "            #Test MESS_DATUM als Datum\n",
    "            df[\"MESS_DATUM\"] = pd.to_datetime(df[\"MESS_DATUM\"], format=\"%Y%m%d%H\", errors=\"coerce\")                \n",
    "            #Daten zusammenführen\n",
    "            combined_data = pd.merge(combined_data, df, on=[  \"MESS_DATUM\"], how=\"outer\")\n",
    "\n",
    "        #Sortieren und doppelte löschen\n",
    "        combined_data = combined_data.sort_values(by=[  \"MESS_DATUM\"]).drop_duplicates(subset=[  \"MESS_DATUM\"], keep='last')\n",
    "        combined_data[\"MESS_DATUM\"] = combined_data[\"MESS_DATUM\"].dt.strftime(\"%Y%m%d%H\")\n",
    "        \n",
    "        # Header ändern\n",
    "        header_mapping = {\n",
    "            \"STATIONS_ID\": \"STATIONS_ID\",\n",
    "            \"MESS_DATUM\": \"date\",\n",
    "            \"V_N_I\": \"Wolken_Interp\",\n",
    "            \"V_N\": \"clouds\",\n",
    "            \"P\": \"stationPressure_hPa\",\n",
    "            \"P0\": \"surfacePressure_hPa\",\n",
    "            \"SD_SO\": \"sunshine_min\",\n",
    "            \"TT_TU\": \"T_temperature_C\",\n",
    "            \"RF_TU\": \"humidity_Percent\",\n",
    "            \"F\": \"wind_speed_ms\",\n",
    "            \"D\": \"wind_direction_degree\",\n",
    "            \"R1\": \"precipitationTotal_mm\",\n",
    "            \"RS_IND\": \"precipitation_indicator\"\n",
    "\n",
    "        }\n",
    "    \n",
    "        combined_data.rename(columns=header_mapping, inplace=True)\n",
    "\n",
    "        #Speichern in Datei\n",
    "        final_filename = os.path.join(station_folder_station, f\"{station_id}_data_combined.csv\")\n",
    "        combined_data.to_csv(final_filename, sep=\",\", index=False)\n",
    "        print(f\"Alle kombinierten Daten für Station {station_id} gespeichert in: {final_filename}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Keine kombinierten Dateien für Station {station_id} gefunden.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_combine_all_station_data_review(station_ids):\n",
    "    max_workers = min(os.cpu_count(), len(station_ids))  #Maximal 10 Threads oder so viele Stationen wie vorhanden\n",
    "    print(f\"Starte die Verknüfung aller Daten für {len(station_ids)} Stationen mit {max_workers} Threads.\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        #Jede Station wird parallel heruntergeladen\n",
    "        future_to_station = {executor.submit(combine_all_station_data_review, station_id): station_id for station_id in station_ids}        \n",
    "        for future in concurrent.futures.as_completed(future_to_station):\n",
    "            station_id = future_to_station[future]\n",
    "            try:\n",
    "                future.result()  #Funktion ausführen und Fehler abfangen\n",
    "                print(f\"Dateien verknüpft aller Daten für Station {station_id}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Verknüfung aller Daten für Station {station_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wetter Forecastfunktionen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_data_for_station_forecast(station_id, station_place):\n",
    "    params = {\n",
    "        \"stationIds\": station_id\n",
    "    }\n",
    "    #Anfrage vorbereiten\n",
    "    request = requests.Request(\"GET\", url_forecast, headers=headers, params=params)\n",
    "    prepared_request = request.prepare()\n",
    "    \n",
    "    response = requests.Session().send(prepared_request)\n",
    "    #Ausgabeordener checken\n",
    "    #os.makedirs(computing_folder, exist_ok=True)\n",
    "    #os.makedirs(station_folder, exist_ok=True)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        filename = os.path.join(computing_folder, f\"weather_forecast_{station_place}.json\")\n",
    "        with open(filename, \"w\") as file:\n",
    "            json.dump(data, file, indent=4)\n",
    "        print(f\"Die Wettervorhersage wurde in {filename} gespeichert.\")\n",
    "        \n",
    "        #JSON-Daten laden und verarbeiten\n",
    "        with open(filename) as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        for station_id, station_data in data.items():\n",
    "            forecast_data = station_data[\"forecast1\"]\n",
    "            start_time = forecast_data[\"start\"]\n",
    "            time_step = forecast_data[\"timeStep\"]\n",
    "\n",
    "            date = [datetime.utcfromtimestamp((start_time + i * time_step) / 1000) for i in range(len(forecast_data[\"temperature\"]))]\n",
    "            \n",
    "            variables = {\n",
    "                \"T_temperature_C\": forecast_data.get(\"temperature\", []),\n",
    "                \"T_temperature_standarddeviation_C\": forecast_data.get(\"temperatureStd\", []),\n",
    "                \"precipitationTotal_mm\": forecast_data.get(\"precipitationTotal\", []),\n",
    "                \"sunshine_min\": forecast_data.get(\"sunshine\", []),\n",
    "                \"dewPoint2m\": forecast_data.get(\"dewPoint2m\", []),\n",
    "                \"surfacePressure_hPa\": forecast_data.get(\"surfacePressure\", []),\n",
    "                \"humidity_Percent\": forecast_data.get(\"humidity\", []),\n",
    "                \"isDay_bool\": forecast_data.get(\"isDay\", []),\n",
    "                #\"icon\": forecast_data.get(\"icon\", []),\n",
    "                #\"icon1h\": forecast_data.get(\"icon1h\", [])\n",
    "            }\n",
    "            \n",
    "            #Alle Listen auf gleiche Länge bringen\n",
    "            max_length = max(len(date), *(len(values) for values in variables.values()))\n",
    "            date.extend([None] * (max_length - len(date)))  # date auf max. Länge auffüllen\n",
    "            for key, values in variables.items():\n",
    "                variables[key].extend([None] * (max_length - len(values)))  # Werte-Listen auffüllen\n",
    "            \n",
    "            # DataFrame erstellen\n",
    "            df = pd.DataFrame({\n",
    "                \"date\": date,\n",
    "                **variables\n",
    "            })\n",
    "\n",
    "            #DataFrame Temperatur von ZehntelGrad in Grad             \n",
    "            df[\"T_temperature_C\"] = df[\"T_temperature_C\"].apply(lambda x: x / 10 if pd.notnull(x) else x)\n",
    "            df[\"T_temperature_standarddeviation_C\"] = df[\"T_temperature_standarddeviation_C\"].apply(lambda x: x / 10 if pd.notnull(x) else x)\n",
    "            df[\"surfacePressure_hPa\"] = df[\"surfacePressure_hPa\"].apply(lambda x: x / 10 if pd.notnull(x) else x)\n",
    "            df[\"humidity_Percent\"] = df[\"humidity_Percent\"].apply(lambda x: x / 10 if pd.notnull(x) else x)\n",
    "\n",
    "            #Date ins richte Foramt konvertieren\n",
    "            df[\"date\"] = df[\"date\"].apply(lambda x: x.strftime(\"%Y%m%d%H\"))\n",
    "\n",
    "            df.to_csv(os.path.join(station_folder, f\"weather_forecast_{station_place}.csv\"), index=False)\n",
    "            print(f\"Die Wettervorhersage wurde in weather_forecast_{station_place}.csv konvertiert\")\n",
    "    else:\n",
    "        print(f\"Fehler bei der Anfrage: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_weatherforecast_data_for_all_stations_forecast(station_ids, station_places):\n",
    "    for (station_id , station_place) in zip(station_ids, station_places):\n",
    "        print(f\"Starte den Download für Station {station_id}...\")\n",
    "        get_weather_data_for_station_forecast(station_id, station_place)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns_forecast():\n",
    "    print(\"Starte den Spaltenentfernumg\")\n",
    "    forecast_files = [f for f in os.listdir(station_folder) if f.startswith(\"weather_forecast_\")]  \n",
    "    print(f\"File: {forecast_files}...\")\n",
    "    for file in forecast_files:\n",
    "        print(f\"Starte den Spaltenentfernumg für {file}...\")\n",
    "        file_path = os.path.join(station_folder, file)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=\",\", skipinitialspace=True)  \n",
    "            print(f\"Spalten im DataFrame: {list(df.columns)}\")          \n",
    "            #Entferne die Spalten, ganz oben definiert\n",
    "            df = df.drop(columns=[col for col in columns_remove_forecast if col in df.columns])     \n",
    "            #Speichere modifizierte Datei\n",
    "            df.to_csv(file_path, sep=\",\", index=False)\n",
    "            print(f\"Spalten aus {file} entfernt.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Datei {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starte Download und Verarbeitung der Wetterdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte den Download für 3 Stationen mit 3 Threads.\n",
      "Speicherort ./weather/computing_folder\\01262, computing_folder ./weather/computing_folder, station_id 01262\n",
      "Speicherort ./weather/computing_folder\\01975, computing_folder ./weather/computing_folder, station_id 01975\n",
      "Speicherort ./weather/computing_folder\\02667, computing_folder ./weather/computing_folder, station_id 02667\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/historical/stundenwerte_TU_01975_19490101_20231231_hist.zip\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/historical/stundenwerte_TU_01262_19920517_20231231_hist.zip\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/historical/stundenwerte_TU_02667_19600101_20231231_hist.zip\n",
      "Daten geladen für: produkt_tu_stunde_19920517_20231231_01262.txt\n",
      "Daten geladen für: produkt_tu_stunde_19490101_20231231_01975.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01262\n",
      "Daten geladen für: produkt_tu_stunde_19600101_20231231_02667.txt\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/recent/stundenwerte_TU_01262_akt.zip\n",
      "Daten geladen für: produkt_tu_stunde_20230617_20241217_01262.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01262\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/cloudiness/historical/stundenwerte_N_01262_19920517_20231231_hist.zip\n",
      "Daten geladen für: produkt_n_stunde_19920517_20231231_01262.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01975\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01262\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/recent/stundenwerte_TU_01975_akt.zip\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\02667\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/cloudiness/recent/stundenwerte_N_01262_akt.zip\n",
      "Daten geladen für: produkt_tu_stunde_20230617_20241217_01975.txt\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/recent/stundenwerte_TU_02667_akt.zip\n",
      "Daten geladen für: produkt_n_stunde_20230617_20241217_01262.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01975\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01262\n",
      "Daten geladen für: produkt_tu_stunde_20230617_20241217_02667.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\02667\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/cloudiness/historical/stundenwerte_N_01975_19490101_20231231_hist.zip\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/pressure/historical/stundenwerte_P0_01262_19920517_20231231_hist.zip\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/cloudiness/historical/stundenwerte_N_02667_19490101_20231231_hist.zip\n",
      "Daten geladen für: produkt_p0_stunde_19920517_20231231_01262.txt\n",
      "Daten geladen für: produkt_n_stunde_19490101_20231231_01975.txt\n",
      "Daten geladen für: produkt_n_stunde_19490101_20231231_02667.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01262\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/pressure/recent/stundenwerte_P0_01262_akt.zip\n",
      "Daten geladen für: produkt_p0_stunde_20230617_20241217_01262.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01262\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/sun/historical/stundenwerte_SD_01262_19920519_20231231_hist.zip\n",
      "Daten geladen für: produkt_sd_stunde_19920519_20231231_01262.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\02667\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01975\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/cloudiness/recent/stundenwerte_N_01975_akt.zip\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/cloudiness/recent/stundenwerte_N_02667_akt.zip\n",
      "Daten geladen für: produkt_n_stunde_20230617_20241217_01975.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01262\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01975\n",
      "Daten geladen für: produkt_n_stunde_20230617_20241217_02667.txt\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/sun/recent/stundenwerte_SD_01262_akt.zip\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\02667\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/pressure/historical/stundenwerte_P0_01975_19490101_20231231_hist.zip\n",
      "Daten geladen für: produkt_sd_stunde_20230617_20241217_01262.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01262\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/wind/historical/stundenwerte_FF_01262_19920519_20231231_hist.zip\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/pressure/historical/stundenwerte_P0_02667_19490101_20231231_hist.zip\n",
      "Daten geladen für: produkt_p0_stunde_19490101_20231231_01975.txt\n",
      "Daten geladen für: produkt_ff_stunde_19920519_20231231_01262.txt\n",
      "Daten geladen für: produkt_p0_stunde_19490101_20231231_02667.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01262\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/wind/recent/stundenwerte_FF_01262_akt.zip\n",
      "Daten geladen für: produkt_ff_stunde_20230617_20241217_01262.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01262\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/recent/stundenwerte_RR_01262_akt.zip\n",
      "Daten geladen für: produkt_rr_stunde_20230617_20241217_01262.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01262\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/historical/stundenwerte_RR_01262_19950901_20231231_hist.zip\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01975\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/pressure/recent/stundenwerte_P0_01975_akt.zip\n",
      "Daten geladen für: produkt_rr_stunde_19950901_20231231_01262.txt\n",
      "Daten geladen für: produkt_p0_stunde_20230617_20241217_01975.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01975\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/sun/historical/stundenwerte_SD_01975_19490101_20231231_hist.zip\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\02667\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/pressure/recent/stundenwerte_P0_02667_akt.zip\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01262\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01262\n",
      "Daten geladen für: produkt_p0_stunde_20230617_20241217_02667.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\02667\n",
      "Historisch bis 2015 gekürzt: clouds_01262_hist.txt\n",
      "Historisch bis 2015 gekürzt: precipitation_01262_hist.txt\n",
      "Historisch bis 2015 gekürzt: pressure_01262_hist.txt\n",
      "Historisch bis 2015 gekürzt: sun_01262_hist.txt\n",
      "Daten geladen für: produkt_sd_stunde_19490101_20231231_01975.txt\n",
      "Historisch bis 2015 gekürzt: temp_01262_hist.txt\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/sun/historical/stundenwerte_SD_02667_19610101_20231231_hist.zip\n",
      "Historisch bis 2015 gekürzt: wind_01262_hist.txt\n",
      "Start Remove Columns\n",
      "Spalten aus clouds_01262_combined.txt entfernt.\n",
      "Daten geladen für: produkt_sd_stunde_19610101_20231231_02667.txt\n",
      "Spalten aus clouds_01262_hist.txt entfernt.\n",
      "Spalten aus clouds_01262_recent.txt entfernt.\n",
      "Spalten aus pressure_01262_combined.txt entfernt.\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01975\n",
      "Spalten aus pressure_01262_hist.txt entfernt.\n",
      "Spalten aus pressure_01262_recent.txt entfernt.\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/sun/recent/stundenwerte_SD_01975_akt.zip\n",
      "Spalten aus sun_01262_combined.txt entfernt.\n",
      "Daten geladen für: produkt_sd_stunde_20230617_20241217_01975.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01975\n",
      "Spalten aus sun_01262_hist.txt entfernt.\n",
      "Spalten aus sun_01262_recent.txt entfernt.\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/wind/historical/stundenwerte_FF_01975_19500101_20231231_hist.zip\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\02667\n",
      "Spalten aus temp_01262_combined.txt entfernt.\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/sun/recent/stundenwerte_SD_02667_akt.zip\n",
      "Spalten aus temp_01262_hist.txt entfernt.\n",
      "Spalten aus temp_01262_recent.txt entfernt.\n",
      "Daten geladen für: produkt_sd_stunde_20230617_20241217_02667.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\02667\n",
      "Spalten aus wind_01262_combined.txt entfernt.\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/wind/historical/stundenwerte_FF_02667_19570701_20231231_hist.zip\n",
      "Spalten aus wind_01262_hist.txt entfernt.\n",
      "Spalten aus wind_01262_recent.txt entfernt.\n",
      "Daten geladen für: produkt_ff_stunde_19500101_20231231_01975.txt\n",
      "Spalten aus precipitation_01262_combined.txt entfernt.\n",
      "Spalten aus precipitation_01262_hist.txt entfernt.\n",
      "Spalten aus precipitation_01262_recent.txt entfernt.\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder\\01262\\clouds_01262_combined.txt\n",
      "Daten geladen für: produkt_ff_stunde_19570701_20231231_02667.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder\\01262\\precipitation_01262_combined.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder\\01262\\pressure_01262_combined.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder\\01262\\sun_01262_combined.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder\\01262\\temp_01262_combined.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder\\01262\\wind_01262_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder\\01262\\clouds_01262_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder\\01262\\precipitation_01262_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder\\01262\\pressure_01262_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder\\01262\\sun_01262_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder\\01262\\temp_01262_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder\\01262\\wind_01262_combined.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01975\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/wind/recent/stundenwerte_FF_01975_akt.zip\n",
      "Daten geladen für: produkt_ff_stunde_20230617_20241217_01975.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\02667\n",
      "Alle kombinierten Daten für Station 01262 gespeichert in: ./weather/stations\\01262\\01262_data_combined.csv\n",
      "Download abgeschlossen für Station 01262.\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01975\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/wind/recent/stundenwerte_FF_02667_akt.zip\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/recent/stundenwerte_RR_01975_akt.zip\n",
      "Daten geladen für: produkt_ff_stunde_20230617_20241217_02667.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\02667\n",
      "Daten geladen für: produkt_rr_stunde_20230617_20241217_01975.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01975\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/recent/stundenwerte_RR_02667_akt.zip\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/historical/stundenwerte_RR_01975_19950905_20231231_hist.zip\n",
      "Daten geladen für: produkt_rr_stunde_20230617_20241217_02667.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\02667\n",
      "Lade herunter: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/historical/stundenwerte_RR_02667_19950901_20231231_hist.zip\n",
      "Daten geladen für: produkt_rr_stunde_19950905_20231231_01975.txt\n",
      "Daten geladen für: produkt_rr_stunde_19950901_20231231_02667.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\01975\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\01975\n",
      "Historisch bis 2015 gekürzt: clouds_01975_hist.txt\n",
      "Wetterdaten gespeichert unter: ./weather/computing_folder\\02667\n",
      "Die Datei wurde erfolgreich gespeichert unter: \\\\nas-tk\\nas\\Daten\\Office\\Uni\\Informatik\\TimeSeries\\final-submission\\dataset\\scripts\\weather\\computing_folder\\02667\n",
      "Historisch bis 2015 gekürzt: precipitation_01975_hist.txt\n",
      "Historisch bis 2015 gekürzt: clouds_02667_hist.txt\n",
      "Historisch bis 2015 gekürzt: pressure_01975_hist.txt\n",
      "Historisch bis 2015 gekürzt: precipitation_02667_hist.txt\n",
      "Historisch bis 2015 gekürzt: sun_01975_hist.txt\n",
      "Historisch bis 2015 gekürzt: pressure_02667_hist.txt\n",
      "Historisch bis 2015 gekürzt: temp_01975_hist.txt\n",
      "Historisch bis 2015 gekürzt: sun_02667_hist.txt\n",
      "Historisch bis 2015 gekürzt: wind_01975_hist.txt\n",
      "Start Remove Columns\n",
      "Historisch bis 2015 gekürzt: temp_02667_hist.txt\n",
      "Spalten aus clouds_01975_combined.txt entfernt.\n",
      "Spalten aus clouds_01975_hist.txt entfernt.\n",
      "Historisch bis 2015 gekürzt: wind_02667_hist.txt\n",
      "Start Remove Columns\n",
      "Spalten aus clouds_01975_recent.txt entfernt.\n",
      "Spalten aus clouds_02667_combined.txt entfernt.\n",
      "Spalten aus pressure_01975_combined.txt entfernt.\n",
      "Spalten aus clouds_02667_hist.txt entfernt.\n",
      "Spalten aus clouds_02667_recent.txt entfernt.\n",
      "Spalten aus pressure_01975_hist.txt entfernt.\n",
      "Spalten aus pressure_01975_recent.txt entfernt.\n",
      "Spalten aus pressure_02667_combined.txt entfernt.\n",
      "Spalten aus sun_01975_combined.txt entfernt.\n",
      "Spalten aus sun_01975_hist.txt entfernt.\n",
      "Spalten aus pressure_02667_hist.txt entfernt.\n",
      "Spalten aus sun_01975_recent.txt entfernt.\n",
      "Spalten aus pressure_02667_recent.txt entfernt.\n",
      "Spalten aus sun_02667_combined.txt entfernt.\n",
      "Spalten aus temp_01975_combined.txt entfernt.\n",
      "Spalten aus sun_02667_hist.txt entfernt.\n",
      "Spalten aus sun_02667_recent.txt entfernt.\n",
      "Spalten aus temp_01975_hist.txt entfernt.\n",
      "Spalten aus temp_01975_recent.txt entfernt.\n",
      "Spalten aus temp_02667_combined.txt entfernt.\n",
      "Spalten aus wind_01975_combined.txt entfernt.\n",
      "Spalten aus temp_02667_hist.txt entfernt.\n",
      "Spalten aus wind_01975_hist.txt entfernt.\n",
      "Spalten aus temp_02667_recent.txt entfernt.\n",
      "Spalten aus wind_01975_recent.txt entfernt.\n",
      "Spalten aus precipitation_01975_combined.txt entfernt.\n",
      "Spalten aus wind_02667_combined.txt entfernt.\n",
      "Spalten aus precipitation_01975_hist.txt entfernt.\n",
      "Spalten aus precipitation_01975_recent.txt entfernt.\n",
      "Spalten aus wind_02667_hist.txt entfernt.\n",
      "Spalten aus wind_02667_recent.txt entfernt.\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder\\01975\\clouds_01975_combined.txt\n",
      "Spalten aus precipitation_02667_combined.txt entfernt.\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder\\01975\\precipitation_01975_combined.txt\n",
      "Spalten aus precipitation_02667_hist.txt entfernt.\n",
      "Spalten aus precipitation_02667_recent.txt entfernt.\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder\\02667\\clouds_02667_combined.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder\\01975\\pressure_01975_combined.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder\\02667\\precipitation_02667_combined.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder\\01975\\sun_01975_combined.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder\\02667\\wind_02667_combined.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder\\01975\\temp_01975_combined.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder\\02667\\sun_02667_combined.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder\\01975\\wind_01975_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder\\01975\\clouds_01975_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder\\01975\\precipitation_01975_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder\\01975\\pressure_01975_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder\\01975\\sun_01975_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder\\01975\\temp_01975_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder\\01975\\wind_01975_combined.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder\\02667\\temp_02667_combined.txt\n",
      "Kombinierte Datei gespeichert: ./weather/computing_folder\\02667\\pressure_02667_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder\\02667\\clouds_02667_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder\\02667\\precipitation_02667_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder\\02667\\pressure_02667_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder\\02667\\sun_02667_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder\\02667\\temp_02667_combined.txt\n",
      "Daten hinzugefügt von: ./weather/computing_folder\\02667\\wind_02667_combined.txt\n",
      "Alle kombinierten Daten für Station 01975 gespeichert in: ./weather/stations\\01975\\01975_data_combined.csv\n",
      "Download abgeschlossen für Station 01975.\n",
      "Alle kombinierten Daten für Station 02667 gespeichert in: ./weather/stations\\02667\\02667_data_combined.csv\n",
      "Download abgeschlossen für Station 02667.\n",
      "Ausführungszeit: 40.07482385635376 Sekunden\n",
      "Starte den Download für Station 10870...\n",
      "Die Wettervorhersage wurde in ./weather/computing_folder\\weather_forecast_Muenchen.json gespeichert.\n",
      "Die Wettervorhersage wurde in weather_forecast_Muenchen.csv konvertiert\n",
      "\n",
      "Starte den Download für Station 10147...\n",
      "Die Wettervorhersage wurde in ./weather/computing_folder\\weather_forecast_Hamburg.json gespeichert.\n",
      "Die Wettervorhersage wurde in weather_forecast_Hamburg.csv konvertiert\n",
      "\n",
      "Starte den Download für Station 10513...\n",
      "Die Wettervorhersage wurde in ./weather/computing_folder\\weather_forecast_KoelnBonn.json gespeichert.\n",
      "Die Wettervorhersage wurde in weather_forecast_KoelnBonn.csv konvertiert\n",
      "\n",
      "Starte den Spaltenentfernumg\n",
      "File: ['weather_forecast_Hamburg.csv', 'weather_forecast_KoelnBonn.csv', 'weather_forecast_Muenchen.csv']...\n",
      "Starte den Spaltenentfernumg für weather_forecast_Hamburg.csv...\n",
      "Spalten im DataFrame: ['date', 'T_temperature_C', 'T_temperature_standarddeviation_C', 'precipitationTotal_mm', 'sunshine_min', 'dewPoint2m', 'surfacePressure_hPa', 'humidity_Percent', 'isDay_bool']\n",
      "Spalten aus weather_forecast_Hamburg.csv entfernt.\n",
      "Starte den Spaltenentfernumg für weather_forecast_KoelnBonn.csv...\n",
      "Spalten im DataFrame: ['date', 'T_temperature_C', 'T_temperature_standarddeviation_C', 'precipitationTotal_mm', 'sunshine_min', 'dewPoint2m', 'surfacePressure_hPa', 'humidity_Percent', 'isDay_bool']\n",
      "Spalten aus weather_forecast_KoelnBonn.csv entfernt.\n",
      "Starte den Spaltenentfernumg für weather_forecast_Muenchen.csv...\n",
      "Spalten im DataFrame: ['date', 'T_temperature_C', 'T_temperature_standarddeviation_C', 'precipitationTotal_mm', 'sunshine_min', 'dewPoint2m', 'surfacePressure_hPa', 'humidity_Percent', 'isDay_bool']\n",
      "Spalten aus weather_forecast_Muenchen.csv entfernt.\n",
      "Ausführungszeit: 49.23252773284912 Sekunden\n",
      "Starte die Verknüfung aller Daten für 3 Stationen mit 3 Threads.\n",
      "Kombiniert: 01262 -> ./weather/stations_combined\\Muenchen_review.csv\n",
      "Dateien verknüpft aller Daten für Station ('01262', 'Muenchen').\n",
      "Kombiniert: 01975 -> ./weather/stations_combined\\Hamburg_review.csv\n",
      "Dateien verknüpft aller Daten für Station ('01975', 'Hamburg').\n",
      "Kombiniert: 02667 -> ./weather/stations_combined\\KoelnBonn_review.csv\n",
      "Dateien verknüpft aller Daten für Station ('02667', 'KoelnBonn').\n",
      "Ausführungszeit: 50.23874282836914 Sekunden\n",
      "Spalten umbennant für Hamburg_review\n",
      "Spalten umbennant für KoelnBonn_review\n",
      "Spalten umbennant für Muenchen_review\n",
      "Daten hinzugefügt von: ./weather/stations_combined\\Hamburg_review.csv\n",
      "Daten hinzugefügt von: ./weather/stations_combined\\KoelnBonn_review.csv\n",
      "Daten hinzugefügt von: ./weather/stations_combined\\Muenchen_review.csv\n",
      "Alle kombinierten Daten gespeichert in: ../data_collection\\weather.csv\n",
      "Spalten umbennant für weather_forecast_Hamburg\n",
      "Spalten umbennant für weather_forecast_KoelnBonn\n",
      "Spalten umbennant für weather_forecast_Muenchen\n",
      "Daten hinzugefügt von: ./weather/stations\\weather_forecast_Hamburg.csv\n",
      "Daten hinzugefügt von: ./weather/stations\\weather_forecast_KoelnBonn.csv\n",
      "Daten hinzugefügt von: ./weather/stations\\weather_forecast_Muenchen.csv\n",
      "Kombinierter Forecast gespeichert in: ../forecast\\weather_forecast.csv\n",
      "Ausführungszeit: 54.09875130653381 Sekunden\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#Erstelle die Ordner\n",
    "create_folder()\n",
    "\n",
    "#Starte Rückblick-Download\n",
    "download_weather_data_for_all_stations_review(station_ids_r)\n",
    "\n",
    "end = time.time()\n",
    "verstrichene_zeit = end - start\n",
    "print(f'Ausführungszeit: {verstrichene_zeit} Sekunden')\n",
    "\n",
    "#Starte Vorhersagen-Download\n",
    "download_weatherforecast_data_for_all_stations_forecast(station_ids_f, station_place)\n",
    "remove_columns_forecast()\n",
    "\n",
    "end = time.time()\n",
    "verstrichene_zeit = end - start\n",
    "print(f'Ausführungszeit: {verstrichene_zeit} Sekunden')\n",
    "\n",
    "#Kombiniere die historischen und vorhergesagten Daten\n",
    "start_combine_historic()\n",
    "end = time.time()\n",
    "verstrichene_zeit = end - start\n",
    "print(f'Ausführungszeit: {verstrichene_zeit} Sekunden')\n",
    "#Alle Stationen kombinieren\n",
    "combine_all_stations()\n",
    "combine_forecast()\n",
    "\n",
    "end = time.time()\n",
    "verstrichene_zeit = end - start\n",
    "print(f'Ausführungszeit: {verstrichene_zeit} Sekunden')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
